Accumulation of URLs about Unicode


================================================================================
Unicode standard
================================================================================

https://home.unicode.org/
https://www.unicode.org/                                        (same as home.unicode.org)
https://www.unicode.org/versions/
https://www.unicode.org/versions/latest/                        (latest version)
https://www.unicode.org/versions/enumeratedversions.html        (current and previous versions)

https://www.unicode.org/Public/                                 (datas for current and previous versions)

https://www.unicode.org/Public/MAPPINGS                         (ISO8859)
    These tables are considered to be authoritative mappings
    between the Unicode Standard and different parts of
    the ISO/IEC 8859 standard.


UNICODE COLLATION ALGORITHM
Unicode has an official string collation algorithm called UCA
http://unicode.org/reports/tr10/
https://unicode.org/reports/tr10/#S2.1.1
The Unicode Collation Algorithm takes an input Unicode string and a Collation Element Table,
containing mapping data for characters. It produces a sort key, which is an array of
unsigned 16-bit integers. Two or more sort keys so produced can then be binary-compared
to give the correct comparison between the strings for which they were generated.


08/06/2021
Default Unicode Collation Element Table (DUCET)
For the latest version, see:
http://www.unicode.org/Public/UCA/latest/allkeys.txt
---
UTS10-D1. Collation Weight: A non-negative integer used in the UCA to establish a means for systematic comparison of constructed sort keys.
UTS10-D2. Collation Element: An ordered list of collation weights.
UTS10-D3. Collation Level: The position of a collation weight in a collation element.


https://www.unicode.org/reports/tr14/
UNICODE LINE BREAKING ALGORITHM


https://unicode.org/reports/tr15/#Detecting_Normalization_Forms
UNICODE NORMALIZATION FORMS


http://www.unicode.org/reports/tr31/#Alternative_Identifier_Syntax


https://unicode.org/reports/tr36/#visual_spoofing
UNICODE SECURITY CONSIDERATIONS


https://www.unicode.org/reports/tr51/
Unicode  emoji


23/05/2021
https://www.unicode.org/notes/tn28/
UNICODEMATH, A NEARLY PLAIN-TEXT ENCODING OF MATHEMATIC
    𝑎𝑏𝑐
    𝑑

    𝑎 + 𝑐
    𝑑

    (𝑎 + 𝑏)𝑛 = ∑ (𝑛 𝑘) 𝑎𝑘𝑏𝑛−𝑘


https://unicode.org/notes/tn5/
Unicode Technical Note #5
CANONICAL EQUIVALENCE IN APPLICATIONS


https://icu.unicode.org/design/normalizing-to-shortest-form
Canonically Equivalent Shortest Form (CESF)
This is usually, but not always, the NFC form.


03/08/2022
https://discourse.julialang.org/t/unicode-15-0-beta-and-sorting-collation/83090
https://unicode.org/emoji/charts-15.0/emoji-ordering.html


https://www.unicode.org/notes/tn39/
BIDI BRACKETS FOR DUMMIES
    — So is the Arabic U+FD3E ﴾ ORNATE LEFT PARENTHESIS a bracket, too?
    No.
    — Why not?
    Because I said so.
    — How about quotation marks? They come in pairs and enclose stuff inside, separating it from all the stuff outside. Some of them, like that French thingie ‹ even look like brackets.
    No, they aren't "brackets", either.
    — Why not?
    Because I said so.
    — How about U+300C 「 LEFT CORNER BRACKET? Is that a "bracket"?
    Yes, that is a bracket.
    — But I thought that was a Japanese quotation mark. How come it is a bracket, but the other quotation marks aren't?
    Because I said so. Oh, and U+230A ⌊ LEFT FLOOR is a "bracket", too.
    — But that doesn't make any sense. What does "FLOOR" have to do with brackets?
    It's probably something to do with maths, but it's a "bracket", anyway. Besides, if you were a decent carpenter, you would know what a floor bracket was.
    — How about the left angle bracket "<"? Is that at least a "bracket"?
    No.
    — Why not?
    Because it's a LESS-THAN SIGN. And enough with all these silly "Why?" and "Why not?" questions.


================================================================================
Unicode general informations
================================================================================

https://en.wikipedia.org/wiki/UTF-8
https://en.wikipedia.org/wiki/UTF-16
https://en.wikipedia.org/wiki/UTF-32


http://xahlee.info/comp/unicode_index.html

http://xahlee.info/comp/unicode_invert_text.html
Inverted text: :ʇxǝʇ pǝʇɹǝʌuI

http://xahlee.info/comp/unicode_animals.html
T-REXX: 🦖


https://www.fontspace.com/unicode/analyzer
https://www.compart.com/en/unicode/


22/05/2021
https://onlineunicodetools.com/
Online Unicode tools is a collection of useful browser-based utilities for manipulating Unicode text.


28/05/2021
https://unicode.scarfboy.com/
Search tool
Provides plenty of information about Unicode characters
but no encoding UTF16

https://unicode-table.com/en/                   search by name
Provides the encoding UTF16


https://www.minaret.info/test/menu.msp
Minaret Unicode Tests
    Case Folding
    Character Type
    Collation
    Normalization
    Sorting
    Transliteration


https://www.gosecure.net/blog/2020/08/04/unicode-for-security-professionals/
Unicode for Security Professionals
by Philippe Arteau | Aug 4, 2020
jlf : this article covers many of the Unicode characteristics


https://github.com/bits/UTF-8-Unicode-Test-Documents
Every Unicode character / codepoint in files and a file generator


http://www.ltg.ed.ac.uk/~richard/utf-8.html
let convert utf8 to codepoint + symbolic name

https://blog.lunatech.com/posts/2009-02-03-what-every-web-developer-must-know-about-url-encoding

https://mothereff.in/utf-8
UTF-8 encoder/decoder


https://corp.unicode.org/pipermail/unicode/
The Unicode Archives
January 2, 2014 - current

https://www.unicode.org/mail-arch/unicode-ml/
March 21, 2001 - April 2, 2020

https://www.unicode.org/mail-arch/unicode-ml/Archives-Old/
October 11, 1994 - March 19, 2001

https://www.unicode.org/search/
Search Unicode.org


https://www.w3.org/TR/charmod/
Character Model for the World Wide Web 1.0: Fundamentals


https://www.codesections.com/blog/raku-unicode/
A deep dive into Raku's Unicode support
Grepping for "Unicode Character Database" brings us to unicode_db.c.
https://github.com/MoarVM/MoarVM/blob/master/src/strings/unicode_db.c


https://www.johndcook.com/blog/2021/11/01/number-sets-html/
Number sets in HTML and Unicode
    ℕ U+2115 &Nopf; &naturals;
    ℤ U+2124 &Zopf; &integers;
    ℚ U+211A &Qopf; &rationals;
    ℝ U+211D &Ropf; &reals;
    ℂ U+2102 &Copf; &complexes;
    ℍ U+210D &Hopf; &quaternions;


https://gregtatum.com/writing/2021/encoding-text-utf-32-utf-16-unicode/
https://gregtatum.com/writing/2021/encoding-text-utf-8-unicode/


https://lwn.net/Articles/667669/
Is the current Unicode design impractical?


https://www.sciencedirect.com/science/article/pii/S1742287613000595
Unicode search of dirty data.
This paper discusses problems arising in digital forensics with regard to Unicode,
character encodings, and search. It describes how multipattern search can handle
the different text encodings encountered in digital forensics and a number of issues
pertaining to proper handling of Unicode in search patterns. Finally, we demonstrate
the feasibility of the approach and discuss the integration of our developed search
engine, lightgrep, with the popular bulk_extractor tool.
---
There are UTF-16LE strings which contain completely different UTF-8 strings as prefixes.
For example the byte sequence which is “nonsense” in UTF-8 is 潮獮湥敳 in UTF-16LE (!)
    "nonsense"~c2x=                         -- '6E6F6E73656E7365'
    "nonsense"~text("utf16be")~c2x=         -- '6E6F 6E73 656E 7365'
    "nonsense"~text("utf16be")~c2u=         -- 'U+6E6F U+6E73 U+656E U+7365'
    "nonsense"~text("utf16be")~utf8=        -- T'湯湳敮獥'      Le potage
    "nonsense"~text("utf16le")~c2x=         -- '6E6F 6E73 656E 7365'
    "nonsense"~text("utf16le")~c2u=         -- 'U+6F6E U+736E U+6E65 U+6573'
    "nonsense"~text("utf16le")~utf8=        -- T'潮獮湥敳'      marée


https://github.com/simsong/bulk_extractor


http://t-a-w.blogspot.com/2008/12/funny-characters-in-unicode.html
SKULL AND CROSSBONES
SNOWMAN
POSTAL MARK FACE
APL FUNCTIONAL SYMBOL TILDE DIAERESIS
ARABIC LIGATURE UIGHUR KIRGHIZ YEH WITH HAMZA ABOVE WITH ALEF MAKSURA ISOLATED FORM
ARABIC LIGATURE SALLALLAHOU ALAYHE WASALLAM
THAI CHARACTER KHOMUT
GLAGOLITIC CAPITAL LETTER SPIDERY HA
VERY MUCH GREATER-THAN
NEITHER LESS-THAN NOR GREATER-THAN
HEAVY BLACK HEART
FLORAL HEART BULLET, REVERSED ROTATED
INTERROBANG
𝄞 (U+1D11E) MUSICAL SYMBOL G CLEF
𝕥 (U+1D565) MATHEMATICAL DOUBLE-STRUCK SMALL T
𝟶 (U+1D7F6) MATHEMATICAL MONOSPACE DIGIT ZERO
𠂊 (U+2008A) Han Character


================================================================================
U+ notation, Unicode escape sequence
================================================================================

29/05/2021
https://stackoverflow.com/questions/1273693/why-is-u-used-to-designate-a-unicode-code-point/8891355
The Python language defines the following string literals:
    u'xyz' to indicate a Unicode string, a sequence of Unicode characters
    '\uxxxx' to indicate a string with a unicode character denoted by four hex digits
    '\Uxxxxxxxx' to indicate a string with a unicode character denoted by eight hex digits
    \N{name}    Character named name in the Unicode database
    \uxxxx      Character with 16-bit hex value xxxx. Exactly four hex digits are required.
    \Uxxxxxxxx  Character with 32-bit hex value xxxxxxxx. Exactly eight hex digits are required.


https://www.perl.com/article/json-unicode-and-perl-oh-my-/
Its \uXXXX escapes support only characters within Unicode’s BMP;
to store emoji or other non-BMP characters you either have to encode to UTF-8 directly.
or indicate a UTF-16 surrogate pair in \uXXXX escapes.


https://corp.unicode.org/pipermail/unicode/2021-April/009410.html
Need reference to good ABNF for \uXXXX syntax


https://bit.ly/UnicodeEscapeSequences
Unicode Escape Sequences Across Various Languages and Platforms


================================================================================
Security
================================================================================

http://www.unicode.org/reports/tr39
UNICODE SECURITY MECHANISMS
http://www.unicode.org/Public/security/latest/confusables.txt


https://www.trojansource.codes/


https://api.mtr.pub/vhf/confusable_homoglyphs


================================================================================
Segmentation, Grapheme
================================================================================

29/05/2021
https://github.com/alvinlindstam/grapheme
https://pypi.org/project/grapheme/
Here too, he says that CR+LF is a grapheme...

Same here:
https://www.reddit.com/r/programming/comments/m274cg/til_rn_crlf_is_a_single_grapheme_cluster/
https://unicode.org/reports/tr29/#Table_Combining_Char_Sequences_and_Grapheme_Clusters


01/06/2021
https://halt.software/optimizing-unicodes-grapheme-cluster-break-algorithm/
They claim this improvement:
For the simple data set, this was 0.38 of utf8proc time.
For the complex data set, this was 0.56 of utf8proc time.


01/06/2021
https://docs.rs/unicode-segmentation/1.7.1/unicode_segmentation/
GraphemeCursor	Cursor-based segmenter for grapheme clusters.
GraphemeIndices	External iterator for grapheme clusters and byte offsets.
Graphemes	External iterator for a string's grapheme clusters.
USentenceBoundIndices	External iterator for sentence boundaries and byte offsets.
USentenceBounds	External iterator for a string's sentence boundaries.
UWordBoundIndices	External iterator for word boundaries and byte offsets.
UWordBounds	External iterator for a string's word boundaries.
UnicodeSentences	An iterator over the substrings of a string which, after splitting the string on sentence boundaries, contain any characters with the Alphabetic property, or with General_Category=Number.
UnicodeWords	An iterator over the substrings of a string which, after splitting the string on word boundaries, contain any characters with the Alphabetic property, or with General_Category=Number.


https://github.com/knighton/unicode
Minimalist Unicode normalization/segmentation library. Python and C++.
Abandonned, last commit 21/05/2015


07/06/2021
https://news.ycombinator.com/item?id=20914184
String lengths in Unicode
    Claude Roux
    We went through a lot of pain to get this right in Tamgu ( https://github.com/naver/tamgu ).
    In particular, emojis can be encoded across 5 or 6 Unicode characters.
    A "black thumb up" is encoded with 2 Unicode characters: the thumb glyph and its color.
    This comes at a cost. Every time you extract a sub-string from a string,
    you have to scan it first for its codepoints, then convert character positions
    into byte positions. One way to speed up stuff a bit, is to check if the string
    is in ASCII (see https://lemire.me/blog/2018/05/16/validating-utf-8-strings-u )
    and apply regular operator then.
    We implemented many techniques based on "intrinsics" instructions to speed up
    conversions and search in order to avoid scanning for codepoints.
    See https://github.com/naver/tamgu/blob/master/src/conversion.cxx for more information.
    https://github.com/naver/tamgu/wiki/4.-Speed-up-UTF8-string-processing-with-Intel's-%22intrinsics%22-instructions-(en)
jlf: they have specific support for Korean... Probably because the NAVER company is from Republic of Korea ?
08/06/2021
https://twitter.com/hashtag/tamgu?src=hashtag_click
https://twitter.com/hashtag/TAL?src=hashtag_click
#tamgu le #langage_de_programmation pour le Traitement Automatique des Langues (#TAL).


jlf 30/09/2021
I have a doubt about that:
Is 👩‍👨‍👩‍👧' really a grapheme?
When moving the cursor in BBEdit, I see a boundary between each character.
[later]
Ok, when moving the cursor in Visual Studio Code, it's really a unique grapheme, no way to put the cursor "inside".
And the display is aligned with what I see in Google Chrome : one WOMAN followed by a family, and no way to put the cursor between the WOMAN and the family.
---
https://www.unicode.org/review/pr-27.html       (old, talk about Unicode 4)
https://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries   (todo: review occurences of ZWJ)


29/10/2021
https://h3manth.com/posts/unicode-segmentation-in-javascript/
https://github.com/tc39/proposal-intl-segmenter


https://news.ycombinator.com/item?id=21690326
Tailored grapheme clusters
Grapheme clusters are locale-dependent, much like string collation is locale-dependent.
What Unicode gives you by default, the (extended) grapheme cluster, is as useful as
the DUCET (Default Unicode Collation Element Table); while you can live with them,
you would be unsatisfied. In fact there are tons of Unicode bugs that can't be corrected
due to the compatibility reason, and can only be fixed via tailored locale-dependent schemes.


================================================================================
Normalization, equivalence
================================================================================

26/11/2013
Text normalization in Go
https://blog.golang.org/normalization


27/11/2013
The string type is broken
https://mortoray.com/2013/11/27/the-string-type-is-broken/
In the comments
Objective-C’s NSString type does correctly upper-case baﬄe into BAFFLE.
(where the rectangle is a grapheme showing 2 small 'f')
Q: What about getting the first three characters of “baﬄe”? Is “baf” the correct answer?
A:  That’s a good question. I suspect “baf” is the correct answer, and I wonder if there is any library that does it.
    I suspect if you normalize it first (since the ffl would disappear I think).
A:  The ligarture disappears in NFK[CD] but not in NF[CD].
    Whether normalization to NFK[CD] is a good idea depends (as always) on the situation.
    For visual grapheme cluster counting, one would convert the entire text to NFKC.
    For getting teaser text from an article i would not a normalization step
    and let a ligature count as just one grapheme cluster even if it may resemble three of them logically.
    I assume, that articles are stored in NFC (the nondestructive normalization form with smallest memory footprint).
    The Unicode standard does not treat ligatures as containing more than one grapheme cluster for that normalization forms that permits them.
    So “eﬄab” (jlf: efflab) is the correct result of reversing “baﬄe” (jlf: baffle)
    and “baﬄe”[2] has to return “ﬄ” even when working on the grapheme cluster level!

    There may or may not be a need for another grapheme cluster definition that permits splitting of ligatures in NF[CD].
    A straight forward way to implement a reverse function adhering to that special definition would NFKC each Unicode grapheme cluster on the fly.
    When that results in multiple Unicode grapheme clusters, that are used – else the original is preserved (so that “ℕ” does not become “N”).
    The real problem is to find a good name for that special interpretation of a grapheme cluster…
Note :
    see also the comment of Tom Christiansen about casing.
    I don't copy-paste here, too long.


https://github.com/blackwinter/unicode
Unicode normalization library. (Mirror of Yoshida-san's code base to maintain the RubyGem.)
Abandonned, last commit 07/07/2016


https://github.com/sjorek/unicode-normalization
An enhanced facade to existing unicode-normalization implementations
Last commit 25/03/2018


https://docs.microsoft.com/en-us/windows/win32/intl/using-unicode-normalization-to-represent-strings
Using Unicode Normalization to Represent Strings


https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/normalize
String.prototype.normalize()
The normalize() method returns the Unicode Normalization Form of the string.


https://forums.swift.org/t/string-case-folding-and-normalization-apis/14663/3
For the comments


https://en.wikipedia.org/wiki/Unicode_equivalence
Unicode equivalence is the specification by the Unicode character encoding standard that some sequences
of code points represent essentially the same character. This feature was introduced in the standard
to allow compatibility with preexisting standard character sets, which often included similar or identical characters.


On Wed, Oct 28, 2020 at 9:54 AM Mark Davis ☕️ <mark@macchiato.com> wrote:
Re: [icu-support] Options for Immutable Collation?

        I think your search for 'middle ground' is fruitless.
            An NFKD ordering is not correct for any human language, and changes with each new Unicode version.
            And even the default Unicode collation ordering is wrong for many languages, because there is no order that simultaneously satisfies all (eg German ordering and Swedish ordering are incompatible).
        Your 'middle ground' would be correct for nobody, and yet be unstable across Unicode versions; or worse yet, fail for new characters.

        IMO, the best practice for a file system (or like systems) is to store in codepoint order. When called upon to present a sorted list of files to a user, the displaying program should sort that list according to the user's language preferences.

    You are right: for a deterministic/reproducible list sorting for a cross-platform filesystem API, anything more complex would be an implementation hazard.

    However, after reviewing both developer discussions and implementation of Unicode handling in 6+ filesystems, IDNA200X, PRECIS and getting roped into work on an IETF i18n filesystem best-practices RFC ... I've got some thoughts.  Thoughts that I will put into a new thread after I do some experimenting : ).

    Thank you all so much!!!
    -Zach Lym


08/06/2021
https://fr.wikipedia.org/wiki/Normalisation_Unicode
NFD     Les caractères sont décomposés par équivalence canonique et réordonnés
        canonical decomposition
NFC     Les caractères sont décomposés par équivalence canonique, réordonnés, et composés par équivalence canonique
        canonical decomposition followed by canonical composition
NFKD    Les caractères sont décomposés par équivalence canonique et de compatibilité, et sont réordonnés
        compatibility decomposition
NFKC    Les caractères sont décomposés par équivalence canonique et de compatibilité, sont réordonnés et sont composés par équivalence canonique
        compatibility decomposition followed by canonical composition
FCD     "Fast C or D" form; cf. UTN #5
FCC     "Fast C Contiguous"; cf. UTN #5


https://www.macchiato.com/unicode-intl-sw/nfc-faq
NFC FAQ
jlf: MUST READ!


09/06/2021
Rust
https://docs.rs/unicode-normalization
    Decompositions  External iterator for a string decomposition’s characters.
    Recompositions  External iterator for a string recomposition’s characters.
    Replacements    External iterator for replacements for a string’s characters.
    StreamSafe      UAX15-D4: This iterator keeps track of how many non-starters
                    there have been since the last starter in NFKD and will emit
                    a Combining Grapheme Joiner (U+034F) if the count exceeds 30.

    is_nfc                      Authoritatively check if a string is in NFC.
    is_nfc_quick                Quickly check if a string is in NFC, potentially returning IsNormalized::Maybe if further checks are necessary. In this case a check like s.chars().nfc().eq(s.chars()) should suffice.
    is_nfc_stream_safe          Authoritatively check if a string is Stream-Safe NFC.
    is_nfc_stream_safe_quick    Quickly check if a string is Stream-Safe NFC.
    is_nfd                      Authoritatively check if a string is in NFD.
    is_nfd_quick                Quickly check if a string is in NFD.
    is_nfd_stream_safe          Authoritatively check if a string is Stream-Safe NFD.
    is_nfd_stream_safe_quick    Quickly check if a string is Stream-Safe NFD.
    is_nfkc                     Authoritatively check if a string is in NFKC.
    is_nfkc_quick               Quickly check if a string is in NFKC.
    is_nfkd                     Authoritatively check if a string is in NFKD.
    is_nfkd_quick               Quickly check if a string is in NFKD.

    Enums
    IsNormalized	The QuickCheck algorithm can quickly determine if a text is or isn’t normalized without any allocations in many cases, but it has to be able to return Maybe when a full decomposition and recomposition is necessary.


08/06/2021
Pharo
https://medium.com/concerning-pharo/an-implementation-of-unicode-normalization-7c6719068f43


https://github.com/duerst/eprun
Efficient Pure Ruby Unicode Normalization (eprun)
According to julia/utf8proc, the interesting part is the tests.


https://corp.unicode.org/pipermail/unicode/2020-December/009150.html
Normalization Generics (NFx, NFKx, NFxy)


https://6guts.wordpress.com/2015/04/12/this-week-unicode-normalization-many-rts/


https://gregtatum.com/writing/2021/diacritical-marks/
DIACRITICAL MARKS IN UNICODE


https://news.ycombinator.com/item?id=29751641
Unicode Normalization Forms: When ö ≠ ö
https://blog.opencore.ch/posts/unicode-normalization-forms/


https://unicode-org.github.io/icu/userguide/transforms/normalization/
ICU Documentation
Normalization
Has a few comments about NFKC_Casefold
- NFKC_Casefold: ICU 4.4 supports the combination of NFKC, case folding and
  removing ignorable characters which was introduced with Unicode 5.2.
- Data Generation Tool


================================================================================
Character set
================================================================================

https://www.gnu.org/software/libc/manual/html_mono/libc.html#Character-Set-Handling


================================================================================
String matching - Lower vs Casefold
================================================================================

https://stackoverflow.com/questions/45745661/lower-vs-casefold-in-string-matching-and-converting-to-lowercase


https://www.w3.org/TR/charmod-norm/
Character Model for the World Wide Web: String Matching
MUST READ, PLENTY OF EXAMPLES FOR CORNER CASES

https://www.w3.org/TR/charmod-norm/#definitionCaseFolding
Very good explanation!
    A few characters have a case folding that map one Unicode code point to two or more code points.
    This set of case foldings are called the full case foldings.

    character ß U+00DF LATIN SMALL LETTER SHARP S
    - The full case folding and the lower case mapping of this character is to two ASCII letters 's'.
    - The upper case mapping is to "SS".
    Because some applications cannot allocate additional storage when performing a case fold operation,
    Unicode provides a simple case folding that maps a code point that would normally fold to more or
    fewer code points to use a single code point for comparison purposes instead.
    Unlike the full folding, this folding invariably alters the content (and potentially the meaning) of the text.
    Unicode simple is not appropriate for use on the Web.

    character ᾛ [U+1F9B GREEK CAPITAL LETTER ETA WITH DASIA AND VARIA AND PROSGEGRAMMENI]
    ᾛ	⇒	ἣι	full case fold: U+1F23 GREEK SMALL LETTER ETA WITH DASIA AND VARIA + U+03B9 GREEK SMALL LETTER IOTA
    ᾛ	⇒	ᾓ	simple case fold: U+1F93 GREEK SMALL LETTER ETA WITH DASIA AND VARIA AND YPOGEGRAMMENI

    Language Sensitivity
    Another aspect of case mapping and case folding is that it can be language sensitive.
    Unicode defines default case mappings and case foldings for each encoded character,
    but these are only defaults and are not appropriate in all cases. Some languages need
    case mapping to be tailored to meet specific linguistic needs. One example of this are
    Turkic languages written in the Latin script:
        Default Folding
        I	⇒	i	Default folding of letter I
        Turkic Language Folding
        I	⇒	ı	Turkic language folding of dotless (ASCII) letter I
        İ	⇒	i	Turkic language folding of dotted letter I

https://www.w3.org/TR/charmod-norm/#matchingAlgorithm
There are four choices for text normalization:
- Default.
  This normalization step has no effect on the text and, as a result, is sensitive
  to form differences involving both case and Unicode normalization.
- ASCII Case Fold.
  Comparison of text with the characters case folded in the ASCII (Basic Latin, U+0000 to U+007F) range.
- Unicode Canonical Case Fold.
  Comparison of text that is both case folded and has Unicode canonical normalization applied.
- Unicode Compatibility Case Fold.
  Comparison of text that is both case folded and has Unicode compatibility normalization applied.
  This normalization step is presented for completeness, but it is not generally appropriate for use on the Web.


https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html
Elasticsearch
Dealing with Human Language


https://stackoverflow.com/questions/319426/how-do-i-do-a-case-insensitive-string-comparison
Related to Python, but the comments are very general and worth reading.
---
Unicode Standard section 3.13 has two other definitions for caseless comparisons:
(D146, canonical) NFD(toCasefold(NFD(str))) on both sides and
(D147, compatibility) NFKD(toCasefold(NFKD(toCasefold(NFD(X))))) on both sides.
It states the inner NFD is solely to handle a certain Greek accent character.


https://boyter.org/posts/unicode-support-what-does-that-actually-mean/
https://news.ycombinator.com/item?id=23524400
ſecret == secret == Secret
ſatisfaction == satisfaction == ſatiſfaction == Satiſfaction == SatiSfaction === ſatiSfaction
Another good example to consider is the character Æ.
Under simple case folding rules the lower of Æ is ǣ.
However with full case folding rules this also matches ae.
Which one is correct? Well that depends on who you ask.


================================================================================
Locale
================================================================================

02/06/2021
https://www.php.net/manual/fr/function.setlocale.php
Warning
The locale information is maintained per process, not per thread.
If you are running PHP on a multithreaded server API , you may experience sudden changes
in locale settings while a script is running, though the script itself never called setlocale().
This happens due to other scripts running in different threads of the same process at the same time,
changing the process-wide locale using setlocale().
On Windows, locale information is maintained per thread as of PHP 7.0.5.
On Windows, setlocale(LC_ALL, '') sets the locale names from the system's regional/language settings (accessible via Control Panel).


https://www.gnu.org/software/libc/manual/html_mono/libc.html#Locales
Locales and Internationalization


================================================================================
CLDR Common Locale Data Repository
================================================================================

19/06/2021
https://github.com/twitter/twitter-cldr-rb
Ruby implementation of the ICU (International Components for Unicode) that uses
the Common Locale Data Repository to format dates, plurals, and more.


https://github.com/twitter/twitter-cldr-js
JavaScript implementation of the ICU (International Components for Unicode) that uses
the Common Locale Data Repository to format dates, plurals, and more. Based on twitter-cldr-rb.


================================================================================
Case mappings
================================================================================

https://unicode.org/faq/casemap_charprop.html


https://stackoverflow.com/questions/7360996/unicode-correct-title-case-in-java?noredirect=1&lq=1
Unicode-correct title case in Java


https://docs.rs/unicode-case-mapping/latest/unicode_case_mapping/
Example
assert_eq!(unicode_case_mapping::to_lowercase('İ'), ['i' as u32, 0x0307]);
assert_eq!(unicode_case_mapping::to_lowercase('ß'), ['ß' as u32, 0]);
assert_eq!(unicode_case_mapping::to_uppercase('ß'), ['S' as u32, 'S' as u32, 0]);
assert_eq!(unicode_case_mapping::to_titlecase('ß'), ['S' as u32, 's' as u32, 0]);
assert_eq!(unicode_case_mapping::to_titlecase('-'), [0; 3]);
assert_eq!(unicode_case_mapping::case_folded('I'), NonZeroU32::new('i' as u32));
assert_eq!(unicode_case_mapping::case_folded('ß'), None);
assert_eq!(unicode_case_mapping::case_folded('ẞ'), NonZeroU32::new('ß' as u32));


https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/titlecase.html
fun Char.titlecase(): String
    val chars = listOf('a', 'ǅ', 'ŉ', '+', 'ß')
    val titlecaseChar = chars.map { it.titlecaseChar() }
    val titlecase = chars.map { it.titlecase() }
    println(titlecaseChar) // [A, ǅ, ŉ, +, ß]
    println(titlecase) // [A, ǅ, ʼN, +, Ss]
fun Char.titlecase(locale: Locale): String
    val chars = listOf('a', 'ǅ', 'ŉ', '+', 'ß', 'i')
    val titlecase = chars.map { it.titlecase() }
    val turkishLocale = Locale.forLanguageTag("tr")
    val titlecaseTurkish = chars.map { it.titlecase(turkishLocale) }
    println(titlecase) // [A, ǅ, ʼN, +, Ss, I]
    println(titlecaseTurkish) // [A, ǅ, ʼN, +, Ss, İ]


================================================================================
Collation, sorting
================================================================================

01/06/2021
https://github.com/jgm/unicode-collation
https://hackage.haskell.org/package/unicode-collation
Haskell implementation of the Unicode Collation Algorithm


https://icu4c-demos.unicode.org/icu-bin/collation.html
ICU Collation Demo


https://www.enterprisedb.com/docs/epas/latest/epas_guide/03_database_administration/06_unicode_collation_algorithm/
Unicode Collation Algorithm


https://www.minaret.info/test/collate.msp
This page provides a means to convert a string of Unicode characters into a binary collation key using
the Java language version ("icu4j") of the IBM International Components for Unicode (ICU) library.
A collation key is the basis for sorting and comparing strings in a language-sensitive Unicode environment.
A collation key is built using a "locale" (a designation for a particular laguage or a variant) and a comparison level.
The levels supported here (Primary, Secondary, Tertiary, Quaternary and Identical) correspond to levels
"L1" through "Ln" as described in Unicode Technical Standard #10 - Unicode Collation Algorithm.
When comparing collation keys for two different strings, both keys must have been created using the same locale
and comparison level in order to be meaningful. The two keys are compared from left to right, byte for byte
until one of the bytes is not equal to the other. Whichever byte is numerically less than the other causes
the source string for that collation key to sort before the other string.


https://lemire.me/blog/2018/12/17/sorting-strings-properly-is-stupidly-hard/
It's the comments section which is interesting.


https://discourse.julialang.org/t/sorting-strings-by-unicode-collation-order/11195
Not supported


https://en.wikipedia.org/wiki/Natural_sort_order
Natural sort order is an ordering of strings in alphabetical order,
except that multi-digit numbers are ordered as a single character.
Natural sort order has been promoted as being more human-friendly ("natural")
than the machine-oriented pure alphabetical order.
For example, in alphabetical sorting "z11" would be sorted before "z2"
because "1" is sorted as smaller than "2",
while in natural sorting "z2" is sorted before "z11" because "2" is sorted as smaller than "11".
Alphabetical sorting:
    z11
    z2
Natural sorting:
    z2
    z11
Functionality to sort by natural sort order is built into many programming languages and libraries.


02/06/2021
https://www.postgresql.org/message-id/flat/BA6132ED-1F6B-4A0B-AC22-81278F5AB81E%40tripadvisor.com
The dangers of streaming across versions of glibc: A cautionary tale
SELECT 'Ｍ' > 'ஐ';
'FULLWIDTH LATIN CAPITAL LETTER M' (U+FF2D)
'TAMIL LETTER AI' (U+0B90)
Across different machines, running the same version of postgres, and in databases
with identical character encodings and collations ('en_US.UTF-8') that select will
return different results if the version of glibc is different.
master:src/backend/utils/adt/varlena.c:1494,1497  These are the lines where postgres
calls strcoll_l and strcoll, in order to sort strings in a locale aware manner.
The reality is that there are different versions of glibc out there in the wild,
and they do not sort consistently across versions/environments.


https://collations.info/concepts/
a site devoted to working with Collations, Unicode, Encodings, Code Pages, etc in Microsoft SQL Server.


================================================================================
Emoji
================================================================================

https://www.unicode.org/Public/emoji/15.0/emoji-test.txt


https://emojipedia.org/


http://xahlee.info/comp/unicode_emoji.html


29/05/2021
https://tonsky.me/blog/emoji/


================================================================================
Countries, flags
================================================================================

22/05/2021
https://en.wikipedia.org/wiki/Regional_indicator_symbol
Regional indicator symbol


https://en.wikipedia.org/wiki/ISO_3166-1
ISO 3166-1 (Codes for the representation of names of countries and their subdivisions)


https://observablehq.com/@jobleonard/which-unicode-flags-are-reversible


================================================================================
Evidence of partial or wrong support of Unicode
================================================================================

13/08/2013
We don’t need a string type
https://mortoray.com/2013/08/13/we-dont-need-a-string-type/


01/12/2013
Strings in Ruby are UTF-8 now… right?
http://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/


14/07/2017
Testing Ruby's Unicode Support
http://blog.honeybadger.io/ruby-s-unicode-support/


22/05/2021
Emoji.length == 2
https://news.ycombinator.com/item?id=13830177
Lot of comments, did not read all, to continue


22/05/2021
https://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/
Let's Stop Ascribing Meaning to Code Points


18/07/2021
https://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/
Breaking Our Latin-1 Assumptions


================================================================================
Optimization, SIMD
================================================================================

08/06/2021
https://lemire.me/blog/2018/05/16/validating-utf-8-strings-using-as-little-as-0-7-cycles-per-byte/


https://github.com/lemire/fastvalidate-utf-8
header-only library to validate utf-8 strings at high speeds (using SIMD instructions)


08/06/2021
https://github.com/simdjson/simdjson
simdjson : Parsing gigabytes of JSON per second
The simdjson library uses commonly available SIMD instructions and microparallel algorithms
to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++.
Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s


https://arxiv.org/abs/2010.03090
Validating UTF-8 In Less Than One Instruction Per Byte
John Keiser, Daniel Lemire
The majority of text is stored in UTF-8, which must be validated on ingestion.
We present the lookup algorithm, which outperforms UTF-8 validation routines used
in many libraries and languages by more than 10 times using commonly available SIMD instructions.
To ensure reproducibility, our work is freely available as open source software.


https://r-libre.teluq.ca/2178/
    Recherche et analyse de solutions performantes pour le traitement de fichiers JSON dans un langage de haut niveau [r-libre/2178]
Referenced from
    https://lemire.me/blog/
    Daniel Lemire's blog – Daniel Lemire is a computer science professor at the University of Quebec (TELUQ) in Montreal.
    His research is focused on software performance and data engineering. He is a techno-optimist.


https://github.com/simdutf/simdutf
https://news.ycombinator.com/item?id=32700315
Unicode routines (UTF8, UTF16, UTF32): billions of characters per second using SSE2, AVX2, NEON, AVX-512.


================================================================================
Variation sequence
================================================================================

22/05/2021
List of all code points that can display differently via a variation sequence
http://randomguy32.de/unicode/charts/standardized-variants/#emoji
Safari is better to display the characters.
Google Chrome and Opera have the same limitations: some characters are not supported (ex: section Phags-Pa).


================================================================================
Whitespaces, separators
================================================================================

22/05/2021
https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/
A section about wcwidth.
A section about spaces:
    There are actually two definitions of whitespace in Unicode.
    Unicode assigns every codepoint a category, and has three categories for
    what sounds like whitespace:
        “Separator, space”;
        “Separator, line”;
        “Separator, paragraph”.
    CR, LF, tab, and even vertical tab are all categorized as “Other, control”
    and not as separators.
    The only character in the “Separator, line” category is U+2028 LINE SEPARATOR,
    and the only character in “Separator, paragraph” is U+2029 PARAGRAPH SEPARATOR.
    Thankfully, all of these have the WSpace property.

    As an added wrinkle, the lone oddball character “⠀” renders like a space in most fonts.
    jlf: 2 cols x 3 lines of debossed dots.
    But it’s not whitespace, it’s not categorized as a separator, and it doesn’t have WSpace.
    It’s actually U+2800 BRAILLE PATTERN BLANK, the Braille character with none of the dots raised.
    (I say “most fonts” because I’ve occasionally seen it rendered as a 2×4 grid of open circles.)


================================================================================
Hyphenation
================================================================================

break words into syllables
    I need to break words into syllables:astronomical --> as - tro - nom - ic - al
    Is it possible to do this (in different languages) using ICU library?  (if no, may be you suggest other tools for it?)

    Andreas Heigl:
    While it looks like this is not something for ICU[1], there are libraries out there handling that - most of the time based on the thesis of Marc Liang.
    I've built an implementation for PHP[2] but there are a lot of others out there[3].

    [1] https://github.com/unicode-org/icu4x/issues/164#issuecomment-651410272
    [2] https://github.com/heiglandreas/Org_Heigl_Hyphenator
    [3] https://github.com/search?q=hyphenate&type=repositories
    https://tug.org/docs/liang/liang-thesis.pdf

================================================================================
Indic languages
================================================================================

This section illustrates that Unicode’s concepts like “extended grapheme cluster”
are meant to provide some low-level, general segmentation, and are not going
to be enough for ideal experience for end users.

https://stackoverflow.com/questions/6805311/combining-devanagari-characters
Combining Devanagari characters
"बिक्रम मेरो नाम हो"~text~graphemes==
    a GraphemeSupplier
     1  : T'बि'
     2  : T'क्'     <-- According the comments, these 2 graphemes should be only one: क्र
     3  : T'र'      <-- even ICU doesn't support that... it's a tailored grapheme cluster
     4  : T'म'
     5  : T' '
     6  : T'मे'
     7  : T'रो'
     8  : T' '
     9  : T'ना'
     10 : T'म'
     11 : T' '
     12 : T'हो'
"बिक्रम मेरो नाम हो"~text~characters==
    an Array (shape [18], 18 items)
     1  : ( "ब"   U+092C Lo 1 "DEVANAGARI LETTER BA" )
     2  : ( "ि"    U+093F Mc 0 "DEVANAGARI VOWEL SIGN I" )
     3  : ( "क"   U+0915 Lo 1 "DEVANAGARI LETTER KA" )
     4  : ( "्"    U+094D Mn 0 "DEVANAGARI SIGN VIRAMA" )           <-- influence segmentation
     5  : ( "र"   U+0930 Lo 1 "DEVANAGARI LETTER RA" )
     6  : ( "म"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     7  : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     8  : ( "म"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     9  : ( "े"    U+0947 Mn 0 "DEVANAGARI VOWEL SIGN E" )
     10 : ( "र"   U+0930 Lo 1 "DEVANAGARI LETTER RA" )
     11 : ( "ो"    U+094B Mc 0 "DEVANAGARI VOWEL SIGN O" )
     12 : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     13 : ( "न"   U+0928 Lo 1 "DEVANAGARI LETTER NA" )
     14 : ( "ा"    U+093E Mc 0 "DEVANAGARI VOWEL SIGN AA" )
     15 : ( "म"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     16 : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     17 : ( "ह"   U+0939 Lo 1 "DEVANAGARI LETTER HA" )
     18 : ( "ो"    U+094B Mc 0 "DEVANAGARI VOWEL SIGN O" )
In Devanagari, each grapheme cluster consists of an initial letter, optional
pairs of virama (vowel killer) and letter, and an optional vowel sign.
    virama = u'\N{DEVANAGARI SIGN VIRAMA}'
    cluster = u''
    last = None
    for c in s:
        cat = unicodedata.category(c)[0]
        if cat == 'M' or cat == 'L' and last == virama:
            cluster += c
        else:
            if cluster:
                yield cluster
            cluster = c
        last = c
    if cluster:
        yield cluster
---
Let's cover the grammar very quickly: The Devanagari Block.
As a developer, there are two character classes you'll want to concern yourself with:
Sign:
    This is a character that affects a previously-occurring character.
    Example, this character: ्. The light-colored circle indicates the location
    of the center of the character it is to be placed upon.
Letter / Vowel / Other:
    This is a character that may be affected by signs.
    Example, this character: क.
Combination result of ् and क: क्. But combinations can extend, so क् and षति will
actually become क्षति (in this case, we right-rotate the first character by 90 degrees,
modify some of the stylish elements, and attach it at the left side of the second character).


https://news.ycombinator.com/item?id=20058454
If I type anything like किमपि (“kimapi”) and hit backspace, it turns into किमप (“kimapa”).
That is, the following sequence of codepoints:
    ‎0915 DEVANAGARI LETTER KA
    ‎093F DEVANAGARI VOWEL SIGN I
    ‎092E DEVANAGARI LETTER MA
    ‎092A DEVANAGARI LETTER PA
    ‎093F DEVANAGARI VOWEL SIGN I
made of three grapheme clusters (containing 2, 1, and 2 codepoints respectively),
turns after a single backspace into the following sequence:
    ‎0915 DEVANAGARI LETTER KA
    ‎093F DEVANAGARI VOWEL SIGN I
    ‎092E DEVANAGARI LETTER MA
    ‎092A DEVANAGARI LETTER PA
This is what I expect/find intuitive, too, as a user.
Similarly अन्यच्च is made of 3 grapheme clusters but you hit backspace 7 times to delete it
(though there I'd slightly have preferred अन्यच्च→अन्यच्→अन्य→अन्→अ instead of
अन्यच्च→अन्यच्→अन्यच→अन्य→अन्→अन→अ that's seen, but one can live with this).


https://github.com/anoopkunchukuttan/indic_nlp_library
The goal of the Indic NLP Library is to build Python based libraries for common
text processing and Natural Language Processing in Indian languages.
The library provides the following functionalities:
    Text Normalization
    Script Information
    Word Tokenization and Detokenization
    Sentence Splitting
    Word Segmentation
    Syllabification
    Script Conversion
    Romanization
    Indicization
    Transliteration
    Translation


https://news.ycombinator.com/item?id=20056966
jlf: Devnagari seems to be an example where grapheme is not the right segmentation
What does "index" mean? (hindi) "इंडेक्स" का क्या अर्थ है?
Including the quote marks, spaces, and question mark, that's 18 characters.
as a native speaker, shouldn't they be considered 15 characters?
क्स, क्या and र्थ each form individual conjunct consonants.
Counting them as two would then beget the question as to why डे is not considered
two characters too, seeing as it is formed by combining ड and ए, much like क्स
is formed by combining क् and स.
...
Devnagari allows simple characters to form compound characters.
Regarding क्स and डे, the difference between them is that the former is a combination
of two consonants (pronounced "ks") while the latter is formed by a consonant and
a vowel ("de"). However, looking at the visual representation is wrong, since डा
(consonant+vowel) would also look like two characters.


https://slidetodoc.com/indic-text-segmentation-presented-by-swaran-lata-senior/
INDIC TEXT SEGMENTATION


https://github.com/w3c/iip/issues/34
the final rendered state of the text is what influences the segmentation,
rather than the sequence of code points used.


https://docs.microsoft.com/en-us/typography/

https://docs.microsoft.com/en-us/typography/script-development/tamil
Developing OpenType Fonts for Tamil Script
The first step is to analyze the input text and break it into syllable clusters.
Then apply font features and computes ligatures and combine marks.


https://docs.microsoft.com/en-us/typography/script-development/devanagari
Developing OpenType Fonts for Devanagari Script


================================================================================
Korean
================================================================================

22/05/2021
http://gernot-katzers-spice-pages.com/var/korean_hangul_unicode.html
The Korean Writing System


================================================================================
Japanese
================================================================================

https://heistak.github.io/your-code-displays-japanese-wrong/
https://news.ycombinator.com/item?id=29022906

https://www.johndcook.com/blog/2022/09/25/katakana-hiragana-unicode/
https://news.ycombinator.com/item?id=32987710


================================================================================
String Matching
================================================================================

https://www.w3.org/TR/charmod-norm/
String matching

Case folding is the process of making two texts which differ only in case identical for comparison purposes, that is, it is meant for the purpose of string matching.
This is distinct from case mapping, which is primarily meant for display purposes.
As with the default case mappings, Unicode defines default case fold mappings ("case folding") for each Unicode code point.


================================================================================
Fuzzy String Matching
================================================================================

29/05/2021
https://github.com/logannc/fuzzywuzzy-rs
Rust port of the Python fuzzywuzzy
https://github.com/seatgeek/fuzzywuzzy --> moved to https://github.com/seatgeek/thefuzz


================================================================================
Levenshtein distance and string similarity
================================================================================

https://github.com/ztane/python-Levenshtein/
The Levenshtein Python C extension module contains functions for fast computation of Levenshtein distance and string similarity


================================================================================
String comparison
================================================================================

31/05/2021
https://stackoverflow.com/questions/49662585/how-do-i-compare-a-unicode-string-that-has-different-bytes-but-the-same-value
A pair NFC considers different but a user might consider the same is 'µ' (MICRO SIGN) and 'μ' (GREEK SMALL LETTER MU).
NFKC will collapse these two.


http://www.unicode.org/reports/tr10/
Unicode® Technical Standard #10
UNICODE COLLATION ALGORITHM
Collation is the general term for the process and function of determining the sorting order of strings of characters.
Collation varies according to language and culture: Germans, French and Swedes sort the same characters differently.
It may also vary by specific application: even within the same language, dictionaries may sort differently than phonebooks or book indices.
For non-alphabetic scripts such as East Asian ideographs, collation can be either phonetic or based on the appearance of the character.
Collation can also be customized according to user preference, such as ignoring punctuation or not, putting uppercase before lowercase (or vice versa), and so on.


https://en.wikipedia.org/wiki/Unicode_equivalence
Short definition of NFD, NFC, NFKD, NFKC

    In this article, a short paragraph which confirms that it's important to keep
    the original string unchanged !
    Errors due to normalization differences
    When two applications share Unicode data, but normalize them differently, errors and data loss can result.
    In one specific instance, OS X normalized Unicode filenames sent from the Samba file- and printer-sharing software.
    Samba did not recognise the altered filenames as equivalent to the original, leading to data loss.[4][5]
    Resolving such an issue is non-trivial, as normalization is not losslessly invertible.
    http://sourceforge.net/p/netatalk/bugs/348/
    #348 volcharset:UTF8 doesn't work from Mac


http://unicode.org/faq/normalization.html
Mode detailled description of normalization


PHP
    http://php.net/manual/en/collator.compare.php
    Collator::compare -- collator_compare — Compare two Unicode strings
    Object oriented style
        public int Collator::compare ( string $str1 , string $str2 )
    Procedural style
        int collator_compare ( Collator $coll , string $str1 , string $str2 )


    http://php.net/manual/en/class.collator.php
    Provides string comparison capability with support for appropriate locale-sensitive sort orderings.


Swift
    https://developer.apple.com/library/prerelease/watchos/documentation/Swift/Conceptual/Swift_Programming_Language/StringsAndCharacters.html
        Two String values (or two Character values) are considered equal if their extended grapheme clusters are canonically equivalent.
        Extended grapheme clusters are canonically equivalent if they have the same linguistic meaning and appearance,
        even if they are composed from different Unicode scalars behind the scenes.

        .characters.count
        for character in dogString.characters
        for codeUnit in dogString.utf8
        for codeUnit in dogString.utf16
        for scalar in dogString.unicodeScalars

        Nothing about ordered comparison in Swift doc ?

    http://oleb.net/blog/2014/07/swift-strings/

        Ordering strings with the < and > operators uses the default Unicode collation algorithm.
        In the example below, "é" is smaller than i because the collation algorithm specifies
        that characters with combining marks follow right after their base character.
            "résumé" < "risotto" // -> true
        The String type does not (yet?) come with a method to specify the language to use for collation.
        You should continue to use
            -[NSString compare:options:range:locale:]
        or
            -[NSString localizedCompare:]
        if you need to sort strings that are shown to the user.

        In this example, specifying a locale that uses the German phonebook collation yields a different result than the default string ordering:
            let muffe = "Muffe"
            let müller = "Müller"
            muffe < müller // -> true

            // Comparison using an US English locale yields the same result
            let muffeRange = muffe.startIndex..<muffe.endIndex
            let en_US = NSLocale(localeIdentifier: "en_US")
            muffe.compare(müller, options: nil, range: muffeRange, locale: en_US) // -> .OrderedAscending

            // Germany phonebook ordering treats "ü" as "ue".
            // Thus, "Müller" < "Muffe"
            let de_DE_phonebook = NSLocale(localeIdentifier: "de_DE@collation=phonebook")
            muffe.compare(müller, options: nil, range: muffeRange, locale: de_DE_phonebook) // -> .OrderedDescending


Java
    https://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/
    How the JVM compares your strings using the craziest x86 instruction you've never heard of.
    ---
    A comment about this article:
    PCMPxSTRx is no longer faster than equivalent "simple" vector instruction sequences for straightforward comparisons
    (this had already been the case for a few years when that article was written, which is curious).
    It can be used productively (with some care) for some other operations like substring matching,
    but that's not as much of a heavy-hitter.
    There's a bunch of string stuff that will benefit from general vectorization, and which is absolutely on our roadmap to tackle,
    but using the PCMPxSTRx instructions specifically isn't a source of wins on the most important operations


C#
    https://docs.microsoft.com/en-us/dotnet/standard/base-types/comparing
    https://docs.microsoft.com/en-us/dotnet/core/extensions/performing-culture-insensitive-string-comparisons


================================================================================
Encodings
================================================================================

30/05/2021
https://datatracker.ietf.org/doc/html/rfc8259
The JavaScript Object Notation (JSON) Data Interchange Format
See this section about strings and encoding:
https://datatracker.ietf.org/doc/html/rfc8259#section-7


================================================================================
JSON
================================================================================

https://www.reddit.com/r/programming/comments/q5vmxc/parsing_json_is_a_minefield_2018/
https://seriot.ch/projects/parsing_json.html
Parsing JSON is a Minefield
Search for "unicode"


================================================================================
TOML serialization format
================================================================================

https://github.com/toml-lang/toml
Tom's Obvious, Minimal Language
TOML is a nice serialization format for human-maintained data structures.
It’s line-delimited and—of course!—allows comments, and any Unicode code point can be expressed in simple hexadecimal.
TOML is fairly new, and its specification is still in flux;


================================================================================
CBOR Concise Binary Representation
================================================================================

https://cbor.io/
RFC 8949 Concise Binary Object Representation
CBOR improves upon JSON’s efficiency and also allows for storage of binary strings.
Whereas JSON encoders must stringify numbers and escape all strings,
CBOR stores numbers “literally” and prefixes strings with their length,
which obviates the need to escape those strings.


https://www.rfc-editor.org/rfc/rfc8949.html
RFC 8949 Concise Binary Object Representation (CBOR)
In contrast to formats such as JSON, the Unicode characters in this type are never escaped.
Thus, a newline character (U+000A) is always represented in a string as the byte 0x0a,
and never as the bytes 0x5c6e (the characters "\" and "n")
nor as 0x5c7530303061 (the characters "\", "u", "0", "0", "0", and "a").


================================================================================
Binary encoding in Unicode
================================================================================

10/07/2021
https://qntm.org/unicodings
Efficiently encoding binary data in Unicode
in UTF-8, use Base64 or Base85
in UTF-16, use Base32768
in UTF-32, use Base65536


https://qntm.org/safe
What makes a Unicode code point safe?


https://github.com/qntm/safe-code-point
Ascertains whether a Unicode code point is 'safe' for the purposes of encoding binary data


https://github.com/qntm/base2048
Binary encoding optimised for Twitter
Originally, Twitter allowed Tweets to be at most 140 characters.
On 26 September 2017, Twitter allowed 280 characters.
Maximum Tweet length is indeed 280 Unicode code points.
Twitter divides Unicode into 4,352 "light" code points (U+0000 to U+10FF inclusive)
and 1,109,760 "heavy" code points (U+1100 to U+10FFFF inclusive).
Base2048 solely uses light characters, which means a new "long" Tweet can contain
at most 280 characters of Base2048. Base2048 is an 11-bit encoding, so those 280
characters encode 3080 bits i.e. 385 octets of data, significantly better than Base65536.


https://github.com/qntm/base65536
Unicode's answer to Base64
Base2048 renders Base65536 obsolete for its original intended purpose of sending
binary data through Twitter.
However, Base65536 remains the state of the art for sending binary data through
text-based systems which naively count Unicode code points, particularly those
using the fixed-width UTF-32 encoding.


================================================================================
Invalid format
================================================================================

22/07/2021
https://stackoverflow.com/questions/52131881/does-the-winapi-ever-validate-utf-16
Does the WinApi ever validate UTF-16?
Windows wide characters are arbitrary 16-bit numbers (formerly called "UCS-2",
before the Unicode Standard Consortium purged that notation). So you cannot
assume that it will be a valid UTF-16 sequence. (MultiByteToWideChar is a
notable exception that does return only UTF-16)


28/07/2021
https://invisible-island.net/xterm/bad-utf8/
Unicode replacement character in the Linux console.
This test text examines, how UTF-8 decoders handle various types of
corrupted or otherwise interesting UTF-8 sequences.
jlf : difficult to understand what is the conclusion...
What I notice in this review is :
Unicode 10.0.0's chapter 3 (June 2017): each of the ill-formed code units is separately replaced by U+FFFD.
That recommendation first appeared in Unicode 6's chapter 3 on conformance (February 2011).
However the comments about “best practice” were removed in Unicode 11.0.0 (June 2018).
The W3C WHATWG page entitled Encoding Standard started in January 2013.
    The constraints in the utf-8 decoder above match “Best Practices for Using
    U+FFFD” from the Unicode standard. No other behavior is permitted per the
    Encoding Standard (other algorithms that achieve the same result are
    obviously fine, even encouraged).
Although Unicode withdrew the recommendation more than two years ago, to date (August 2020) that is not yet corrected in the WHATWG page.


30/07/2021
https://hsivonen.fi/broken-utf-8/
---
The Unicode Technical Committee retracted the change in its meeting on August 3
2017, so the concern expressed below is now moot.
---
Not all byte sequences are valid UTF-8. When decoding potentially invalid UTF-8
input into a valid Unicode representation, something has to be done about invalid input.
The naïve answer is to ignore invalid input until finding valid input again (i.e.
finding the next byte that has a lead-byte value), but this is dangerous and
should never be done. The danger is that silently dropping bogus bytes might
make a string that didn’t look dangerous with the bogus bytes present become
valid active content. Most simply, <scr�ipt> (� standing in for a bogus byte)
could become <script> if the error is ignored. So it’s non-controversial that
every sequence of bogus bytes should result in at least one REPLACEMENT CHARACTER
and that the next lead-valued byte is the first byte that’s no longer part of
the invalid sequence.
But how many REPLACEMENT CHARACTERs should be generated for a sequence of
multiple bogus bytes?
jlf: the answer is not clear to me...


================================================================================
Mojibake
================================================================================

https://github.com/LuminosoInsight/python-ftfy
ftfy can fix mojibake (encoding mix-ups), by detecting patterns of characters
that were clearly meant to be UTF-8 but were decoded as something else


03/07/2021
Notebook in python-ftfy:
Services such as Slack and Discord don't use Unicode for their emoji.
They use ASCII strings like :green-heart: and turn them into images.
These won't help you test anything.
I recommend getting emoji for your test cases by copy-pasting them from emojipedia.org.
https://emojipedia.org/


================================================================================
Filenames
================================================================================

https://opensource.apple.com/source/subversion/subversion-52/subversion/notes/unicode-composition-for-filenames.auto.html
2 problems follow:
 1) We can't generally depend on the OS to give us back the
     exact filename we gave it
 2) The same filename may be encoded in different codepoints


================================================================================
WTF8
================================================================================

https://news.ycombinator.com/item?id=9611710
The WTF-8 encoding (simonsapin.github.io)
https://news.ycombinator.com/item?id=9613971
https://simonsapin.github.io/wtf-8/#acknowledgments
Thanks to Coralie Mercier for coining the name WTF-8.
---
The name is unserious but the project is very serious, its writer has responded
to a few comments and linked to a presentation of his on the subject[0].
It's an extension of UTF-8 used to bridge UTF-8 and UCS2-plus-surrogates:
while UTF8 is the modern encoding you have to interact with legacy systems,
for UNIX's bags of bytes you may be able to assume UTF8 (possibly ill-formed)
but a number of other legacy systems used UCS2 and added visible surrogates
(rather than proper UTF-16) afterwards.
Windows and NTFS, Java, UEFI, Javascript all work with UCS2-plus-surrogates.
Having to interact with those systems from a UTF8-encoded world is an issue
because they don't guarantee well-formed UTF-16, they might contain unpaired
surrogates which can't be decoded to a codepoint allowed in UTF-8 or UTF-32
(neither allows unpaired surrogates, for obvious reasons).
WTF8 extends UTF8 with unpaired surrogates (and unpaired surrogates only,
paired surrogates from valid UTF16 are decoded and re-encoded to a proper
UTF8-valid codepoint) which allows interaction with legacy UCS2 systems.
WTF8 exists solely as an internal encoding (in-memory representation),
but it's very useful there.
[0] http://exyr.org/2015/!!Con_WTF-8/slides.pdf

https://twitter.com/koalie/status/506821684687413248
Coralie Mercier
@koalie
I have a hunch we use "wtf-8" encoding.
Appreciate the irony of:
"ÃƒÆ’Ã‚Æ’ÃƒÂ¢Ã‚â‚¬Ã‚Å¡ÃƒÆ’Ã‚â€šÃƒâ€šÃ‚Â the future of publishing at W3C"


16/07/2021
Windows allows unpaired surrogates in filenames


https://github.com/golang/go/issues/32334
syscall: Windows filenames with unpaired surrogates are not handled correctly #32334


https://github.com/rust-lang/rust/issues/12056
path: Windows paths may contain non-utf8-representable sequences #12056
I don't know the precise details, but there exist portions of Windows in which
paths are UCS2 rather than UTF-16. I ignored it because I thought it wasn't going
to be an issue but at some point someone (and I wish I could remember who) showed
me some output that showed that they were actually getting a UCS2 path from some
Windows call and Path was unable to parse it.
---
JLF: this is the birth of WTF-8 in 2014.
The result is:
https://simonsapin.github.io/wtf-8/#16-bit-code-unit


====================================================================================
Indexation of UTF-8 strings
====================================================================================

https://nullprogram.com/blog/2019/05/29/


ObjectIcon
    http://objecticon.sourceforge.net/Unicode.html
    ucs (standing for Unicode character string) is a new builtin type, whose behaviour closely mirrors
    that of the conventional Icon string. It operates by providing a wrapper around a conventional
    conventional Icon string, which must be in utf-8 format. This has several advantages, and only one
    serious disadvantage, namely that a utf-8 string is not randomly accessible, in the sense that one
    cannot say where the representation for unicode character i begins. To alleviate this disadvantage,
    the ucs type maintains an index of offsets into the utf-8 string to make random access faster. The
    size of the index is only a few percent of the total allocation for the ucs object.
Jlf: I made a code review, but could not understand how they do that :-(


================================================================================
Rope
================================================================================

https://github.com/josephg/librope
Little C library for heavyweight utf-8 strings (rope).


https://news.ycombinator.com/item?id=8065608
Discussion about ropes, ideal of strings...


https://github.com/xi-editor/xi-editor/blob/e8065a3993b80af0aadbca0e50602125d60e4e38/doc/rope_science/rope_science_03.md


================================================================================
Encoding title
================================================================================


https://thephd.dev/the-c-c++-rust-string-text-encoding-api-landscape
OCTOBER 12, 2022
JeanHeyd Meneide
Project Editor for ISO/IEC JTC1 SC22 WG14 - Programming Languages, C.
The Wonderfully Terrible World of C and C++ Encoding APIs (with Some Rust)
Is he criticizing the work of Zach Laine? ( https://github.com/tzlaine/text )
"someone was doing something wrong on the internet and I couldn’t let that pass:"

Same person:
https://github.com/ThePhD
https://github.com/soasis

Any Encoding, Ever - ztd.text and Unicode for C++ - JUNE 30, 2021 : https://thephd.dev/any-encoding-ever-ztd-text-unicode-cpp

Starting a Basis - Shepherd's Oasis and Text - MAY 01, 2020: https://thephd.dev/basis-shepherds-oasis-text-encoding

https://ztdtext.readthedocs.io/en/latest/index.html
ztd.text
The premiere library for handling text in different encoding forms and reducing
transcoding bugs in your C++ software.
List of encodings: https://ztdtext.readthedocs.io/en/latest/encodings.html
List of Unicode encodings: https://ztdtext.readthedocs.io/en/latest/known%20unicode%20encodings.html
Design Goals and Philosophy: https://ztdtext.readthedocs.io/en/latest/design.html
---
jlf: don't know what to think about that...
related to https://github.com/soasis


https://github.com/soasis/text
JeanHeyd Meneide
This repository is an implementation of an up and coming proposal percolating
through SG16, P1629 - Standard Text Encoding
( https://thephd.dev/_vendor/future_cxx/papers/d1629.html )
---
https://github.com/soasis
Shepherd's Oasis
Software Services and Consulting.


================================================================================
ICU title
================================================================================

http://stackoverflow.com/questions/8253033/what-open-source-c-or-c-libraries-can-convert-arbitrary-utf-32-to-nfc
What open source C or C++ libraries can convert arbitrary UTF-32 to NFC?

std::string normalize(const std::string &unnormalized_utf8) {
    // FIXME: until ICU supports doing normalization over a UText
    // interface directly on our UTF-8, we'll use the insanely less
    // efficient approach of converting to UTF-16, normalizing, and
    // converting back to UTF-8.

    // Convert to UTF-16 string
    auto unnormalized_utf16 = icu::UnicodeString::fromUTF8(unnormalized_utf8);

    // Get a pointer to the global NFC normalizer
    UErrorCode icu_error = U_ZERO_ERROR;
    const auto *normalizer = icu::Normalizer2::getInstance(nullptr, "nfc", UNORM2_COMPOSE, icu_error);
    assert(U_SUCCESS(icu_error));

    // Normalize our string
    icu::UnicodeString normalized_utf16;
    normalizer->normalize(unnormalized_utf16, normalized_utf16, icu_error);
    assert(U_SUCCESS(icu_error));

    // Convert back to UTF-8
    std::string normalized_utf8;
    normalized_utf16.toUTF8String(normalized_utf8);

    return normalized_utf8;
}


https://icu4c-demos.unicode.org/icu-bin/icudemos
todo: review


https://icu4c-demos.unicode.org/icu-bin/scompare
ICU Unicode String Comparison
Interactive demo application

================================================================================
ICU bindings
================================================================================

02/06/2021
https://gitlab.pyicu.org/main/pyicu
Python extension wrapping the ICU C++ libraries.


02/06/2021
https://docs.microsoft.com/en-us/windows/win32/intl/international-components-for-unicode--icu-
In Windows 10 Creators Update, ICU was integrated into Windows, making the C APIs and data publicly accessible.
The version of ICU in Windows only exposes the C APIs.
It is impossible to ever expose the C++ APIs due to the lack of a stable ABI in C++.
Getting started
1) Your application needs to target Windows 10 Version 1703 (Creators Update) or higher.
2) Add in the header:
    #include <icu.h>
3) Link to:
    icu.lib
Example:
    void FormatDateTimeICU()
    {
        UErrorCode status = U_ZERO_ERROR;

        // Create a ICU date formatter, using only the 'short date' style format.
        UDateFormat* dateFormatter = udat_open(UDAT_NONE, UDAT_SHORT, nullptr, nullptr, -1, nullptr, 0, &status);

        if (U_FAILURE(status))
        {
            ErrorMessage(L"Failed to create date formatter.");
            return;
        }

        // Get the current date and time.
        UDate currentDateTime = ucal_getNow();

        int32_t stringSize = 0;

        // Determine how large the formatted string from ICU would be.
        stringSize = udat_format(dateFormatter, currentDateTime, nullptr, 0, nullptr, &status);

        if (status == U_BUFFER_OVERFLOW_ERROR)
        {
            status = U_ZERO_ERROR;
            // Allocate space for the formatted string.
            auto dateString = std::make_unique<UChar[]>(stringSize + 1);

            // Format the date time into the string.
            udat_format(dateFormatter, currentDateTime, dateString.get(), stringSize + 1, nullptr, &status);

            if (U_FAILURE(status))
            {
                ErrorMessage(L"Failed to format the date time.");
                return;
            }

            // Output the formatted date time.
            OutputMessage(dateString.get());
        }
        else
        {
            ErrorMessage(L"An error occured while trying to determine the size of the formatted date time.");
            return;
        }

        // We need to close the ICU date formatter.
        udat_close(dateFormatter);
    }


http://www.boost.org/doc/libs/1_58_0/libs/locale/doc/html/index.html
Boost.Locale creates the natural glue between the C++ locales framework, iostreams, and the powerful ICU library


http://blog.lukhnos.org/post/6441462604/using-os-xs-built-in-icu-library-in-your-own
Using OS X’s Built-in ICU Library in Your Own Project


================================================================================
ICU4X title
================================================================================

https://icu4x.unicode.org/

https://github.com/unicode-org/icu4x

http://blog.unicode.org/2022/09/announcing-icu4x-10.html
    SEPTEMBER 29, 2022
    Announcing ICU4X 1.0

    This week, after 2½ years of work by Google, Mozilla, Amazon, and community partners,
    the Unicode Consortium has published ICU4X 1.0, its first stable release.
        Lightweight:
        ICU4X is Unicode's first library to support static data slicing and dynamic data loading.
        Portable:
        ICU4X supports multiple programming languages out of the box. ICU4X can be used
        in the Rust programming language natively, with official wrappers in C++ via the
        foreign function interface (FFI) and JavaScript via WebAssembly.

    ICU4X does not seek to replace ICU4C or ICU4J; rather, it seeks to replace the large number
    of non-Unicode, often-unmaintained, often-incomplete i18n libraries that have been written
    to bring i18n to new programming languages and resource-constrained environments

    One of the most visible departures that ICU4X makes from ICU4C and ICU4J is an
    explicit data provider argument on most constructor functions.
    ICU4X team member Manish Goregaokar wrote a blog post series detailing how the zero-copy deserialization works under the covers.
        https://manishearth.github.io/blog/2022/08/03/zero-copy-1-not-a-yoking-matter/
        https://manishearth.github.io/blog/2022/08/03/zero-copy-2-zero-copy-all-the-things/
        https://manishearth.github.io/blog/2022/08/03/zero-copy-3-so-zero-its-dot-dot-dot-negative/
        (jlf: Related to ICU4X, but should I read that ? It's internal Rust stuff)


https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/cpp.md
Using ICU4X from C++


https://www.reddit.com/r/programming/comments/xrmine/the_unicode_consortium_announces_icu4x_10_its_new/
    The C and C++ APIs are header-only, you use them by linking to the icu_capi crate (more on this here).
    The C API is just not that idiomatic, so we don't advertise it as much.
    It exists more as a crutch for other languages to be able to call in, and it's optimized for cross language interop.
    That said, it has been pointed out to me that it's not that unidiomatic when you compare it with other large C libraries,
    so perhaps that's okay. We do have some tests that use it directly and it's .... fine to work with.
    Not an amazing experience, not terrible either.
    ---
    jlf: to investigate
    The C wrapper is probably better to use from Executor, because there is no hidden magic for memory management.
    The C++ wrapper is difficult to understand (at least to me, for the moment) because it's modern C++.


https://www.reddit.com/r/rust/comments/xrh7h6/announcing_icu4x_10_new_internationalization/
    icu_segmenter implements rule based segmentation, so you can actually customize
    the segmentation rules based on your needs by writing some toml and feeding it to datagen.
    The concept of a "character" or "word" has no single cross-linguistic meaning;
    it is not uncommon to need to tailor these algorithms by use case or even just
    the language being used. E.g. handling viramas in Indic scripts as a part of
    grapheme segmentation is a thing people might need, but may also not need,
    and UAX29 doesn't support that at the moment¹. CLDR contains a bunch of common
    tailorings for specific locales here, but as I mentioned folks may tailor further based on use case.

    Furthermore, icu_segmenter supports dictionary-based segmentation:
    for languages like Japanese and Thai where spaces are not typically used,
    you need a large dictionary to be able to segment them accurately
    (and again, it's language-specific).
    ICU4X's flexible data model means that you don't need to ship your application
    with this data and instead fetch it when it's actually necessary.
    We both support using dictionaries and an LSTM model depending on your code size/data size needs.


https://docs.google.com/document/d/1ojrOdIchyIHYbg2G9APX8j2p0XtmVLj0f9jPIbFYVUE/edit#heading=h.xy9pq2mk1ypz
ICU4X Segmenter Investigation


https://github.com/unicode-org/icu4x/issues/1397
Character names
jlf: Not yet supported by ICU4X, too bad... I need that for Executor.


https://github.com/unicode-org/icu4x/issues/545
Reconsider UTF-32 support


================================================================================
utfcpp
================================================================================

https://github.com/nemtrif/utfcpp/
referenced from https://corp.unicode.org/pipermail/unicode/2020-April/008582.html
Basic Unicode character/string support absent even in modern C++


================================================================================
Twitter text parsing
================================================================================

https://github.com/twitter/twitter-text
Twitter Text Libraries. This code is used at Twitter to tokenize and parse text
to meet the expectations for what can be used on the platform.

https://swiftpack.co/package/nysander/twitter-text
This is the Swift implementation of the twitter-text parsing library.
The library has methods to parse Tweets and calculate length, validity, parse @mentions, #hashtags, URLs, and more.


================================================================================
terminal / console
================================================================================

https://www.reddit.com/r/bash/comments/wfbf3w/determine_if_the_termconsole_supports_utf8/
Determine if the term/console supports UTF8?


================================================================================
Language comparison
================================================================================

https://blog.kdheepak.com/my-unicode-cheat-sheet
Vim, Python, Julia and Rust.


================================================================================
Regular expressions
================================================================================

https://www.regular-expressions.info/unicode.html
\X matches a grapheme


https://pypi.org/project/regex/
>>> a = "बिक्रम मेरो नाम हो"
>>> regex.findall(r'\X', a)
['बि', 'क्', 'र', 'म', ' ', 'मे', 'रो', ' ', 'ना', 'म', ' ', 'हो']
---
https://regex101.com/r/eD0eZ9/1
---
jlf: the results above are correct extended grapheme clusters, but tailored
grapheme clusters will group 'क्' 'र' in one cluster क्र


https://blog.burntsushi.net/ripgrep/
ripgrep is faster than {grep, ag, git grep, ucg, pt, sift}
search for "unicode" and read...


================================================================================
Test cases, test-cases, tests files
================================================================================

https://github.com/lemire/unicode_lipsum


================================================================================
youtube
================================================================================

https://www.youtube.com/playlist?list=PLMc927ywQmTNQrscw7yvaJbAbMJDIjeBh
Videos from Unicode's Overview of Internationalization and Unicode Projects


================================================================================
Ada lang
================================================================================

https://docs.adacore.com/live/wave/xmlada/html/xmlada_ug/unicode.html

http://www.dmitry-kazakov.de/ada/strings_edit.htm

UXStrings Ada Unicode Extended Strings
https://www.reddit.com/r/ada/comments/t4hpip/ann_uxstrings_package_available_uxs_20220226/
https://github.com/Blady-Com/UXStrings

================================================================================
Awk lang
================================================================================

Brian Kernighan adds Unicode support to Awk
https://github.com/onetrueawk/awk/commit/9ebe940cf3c652b0e373634d2aa4a00b8395b636
https://github.com/onetrueawk/awk/tree/unicode-support
https://news.ycombinator.com/item?id=32534173


https://github.com/AdaForge/Thematics/wiki/Unicode-and-String-manipulations
Unicode and String manipulations in UTF-8, UTF-16, ...


https://stackoverflow.com/questions/48829940/utf-8-on-windows-with-ada
UTF-8 on Windows with Ada


================================================================================
C++ lang, Boost
================================================================================

02/06/2021
http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1238r1.html
    SG16 initial Unicode direction and guidance for C++20 and beyond.
https://github.com/sg16-unicode/sg16
    SG16 is an ISO/IEC JTC1/SC22/WG21 C++ study group tasked with improving Unicode and text processing support within the C++ standard.


01/06/2021
Zach Laine
https://www.youtube.com/watch?v=944GjKxwMBo
https://tzlaine.github.io/text/doc/html/boost_text__proposed_/the_text_layer.html
The Text Layer
https://tzlaine.github.io/text/doc/html/
Chapter 1. Boost.Text (Proposed) - 2018
https://github.com/tzlaine/text
    last commit :
        master                          26/09/2020
        boost_serialization             24/10/2019
        coroutines                      25/08/2020
        experimental                    13/11/2019
        gh-pages                        04/09/2020
        optimization                    27/10/2019
        rope_free_fn_reimplementation   26/07/2020
No longer working on this project ?


14/06/2021
https://hsivonen.fi/non-unicode-in-cpp/
Same contents in sg16 mailing list + feedbacks
https://lists.isocpp.org/sg16/2019/04/0309.php


03/07/2021
https://news.ycombinator.com/item?id=27695412
Any Encoding, Ever – ztd.text and Unicode for C++


14/07/2021
https://hsivonen.fi/non-unicode-in-cpp/
It’s Time to Stop Adding New Features for Non-Unicode Execution Encodings in C++
The Microsoft Code Page 932 Issue


https://stackoverflow.com/questions/58878651/what-is-the-printf-formatting-character-for-char8-t/58895428#58895428.
What is the printf() formatting character for char8_t *?
jlf: todo read it? not sure yet if it's useful to read.
Referenced from https://corp.unicode.org/pipermail/unicode/2020-April/008579.html
Basic Unicode character/string support absent even in modern C++


================================================================================
DotNet, CoreFx
================================================================================

28/07/2021
https://github.com/dotnet/corefxlab/issues/2368
Scenarios and Design Philosophy - UTF-8 string support

    https://gist.github.com/GrabYourPitchforks/901684d0aa1d2440eb378d847cfc8607 (jlf: replaced by the following URL)
    https://github.com/dotnet/corefx/issues/34094 (go directly to next URL)
    https://github.com/dotnet/runtime/issues/28204
    Motivations and driving principles behind the Utf8Char proposal

    https://github.com/dotnet/runtime/issues/933
    The NuGet package generally follows the proposal in dotnet/corefxlab#2350, which
    is where most of the discussion has taken place. It's a bit aggravating that the
    discussion is split across so many different forums, I know. :(

        ceztko
        I noticed dotnet/corefxlab#2350 just got closed. Did the discussion moved
        somewhere else about more UTF8 first citizen support efforts?

        @ceztko The corefxlab repo was archived, so open issues were closed to
        support that effort. That thread also got so large that it was difficult
        to follow. @krwq is working on restructuring the conversation so that we
        can continue the discussion in a better forum.

        jlf
        Not clear where the discussion is continued...
        This URL just show some tags, one of them is "Future".
        https://github.com/orgs/dotnet/projects/7#card-33368432


    https://github.com/dotnet/corefxlab/issues/2350
    Utf8String design discussion - last edited 14-Sep-19
    Tons of comments, with this conclusion:
    The discussion in this issue is too long and github has troubles rendering it.
    I think we should close this issue and start a new one in dotnet/runtime.


================================================================================
Dafny lang
================================================================================

https://corp.unicode.org/pipermail/unicode/2021-May/009434.html
Dafny natively supports expressing statements about sets
and contract programming and a toy implementation turned out to be a fairly
rote translation of the Unicode spec.  Dafny is also transpilation focused,
so the primary interface must be highly functional and encoding neutral.


================================================================================
Factor lang
================================================================================

http://docs.factorcode.org/content/article-unicode.html

http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-1.html
JLF : bof...

http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-2.html

http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-3.html

http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-4.html
grapheme breaking

http://useless-factor.blogspot.fr/2007/08/r-597-rs-unicode-library-is-broken.html


http://useless-factor.blogspot.fr/2007/02/more-string-parsing.html
UTF-8/16 encoder/decoder

    I used a design pattern known as a sentinel, which helps me cross-cut pointcutting concerns
    by instantiating objects which encapsulate the state of the parser. I never mutate these,
    and the program is purely functional except for the use of make (which could trivially be
    changed into a less efficient map [ ] subset, sacrificing efficiency and some terseness
    but making it functional).

    TUPLE: new ;
    TUPLE: double val ;
    TUPLE: quad2 val ;
    TUPLE: quad3 val ;

    : bad-char CHAR: ? ;

    GENERIC: (utf16le) ( char state -- state )
    M: new (utf16le)
        drop <double> ;
    M: double (utf16le)
        over -3 shift BIN: 11011 = [
            over BIN: 100 bitand 0 =
            [ double-val swap BIN: 11 bitand 8 shift bitor <quad2> ]
            [ 2drop bad-char , <new> ] if
        ] [ double-val swap 8 shift bitor , <new> ] if ;
    M: quad2 (utf16le)
        quad2-val 10 shift bitor <quad3> ;
    M: quad3 (utf16le)
        over -2 shift BIN: 110111 = [
            swap BIN: 11 bitand 8 shift
            swap quad3-val bitor HEX: 10000 + , <new>
        ] [ 2drop bad-char , <new> ] if ;

    : utf16le ( state string -- state string )
        [ [ swap (utf16le) ] each ] { } make ;


================================================================================
Java lang
================================================================================

https://openjdk.org/jeps/400
JEP 400: UTF-8 by Default
A quick way to see the default charset of the current JDK is with the following command:
    java -XshowSettings:properties -version 2>&1 | grep file.encoding
As envisaged by the specification of Charset.defaultCharset(), the JDK will allow
the default charset to be configured to something other than UTF-8.
    java -Dfile.encoding=COMPAT
        the default charset will be the charset chosen by the algorithm in JDK 17 and earlier,
        based on the user's operating system, locale, and other factors.
        The value of file.encoding will be set to the name of that charset.
    java -Dfile.encoding=UTF-8
        the default charset will be UTF-8.
        This no-op value is defined in order to preserve the behavior of existing command lines.
    The treatment of values other than "COMPAT" and "UTF-8" are not specified.
    They are not supported, but if such a value worked in JDK 17 then it will likely continue to work in JDK 18.


https://www.baeldung.com/java-remove-accents-from-text
Remove Accents and Diacritics From a String in Java
- We will perform the compatibility decomposition represented as the Java enum NFKD.
  because it decomposes more ligatures than the canonical method (for example, ligature “ﬁ”).
- We will remove all characters matching the Unicode Mark category using the \p{M} regex expression.
Test:
    assertEquals("\\u0066 \\u0069", StringNormalizer.unicodeValueOfNormalizedString("ﬁ"));
    assertEquals("\\u0061 \\u0304", StringNormalizer.unicodeValueOfNormalizedString("ā"));
    assertEquals("\\u0069 \\u0308", StringNormalizer.unicodeValueOfNormalizedString("ï"));
    assertEquals("\\u006e \\u0301", StringNormalizer.unicodeValueOfNormalizedString("ń"));
Compare Strings Including Accents Using Collator.
Java provides four strength values for a Collator:
    PRIMARY: comparison omitting case and accents
    SECONDARY: comparison omitting case but including accents and diacritics
    TERTIARY: comparison including case and accents
    IDENTICAL: all differences are significant


https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/io/DataInput.html#modified-utf-8
Implementations of the DataInput and DataOutput interfaces represent Unicode strings in a format
that is a slight modification of UTF-8.
- Characters in the range '\u0001' to '\u007F' are represented by a single byte.
- The null character '\u0000' and characters in the range '\u0080' to '\u07FF' are represented by a pair of bytes.
- Characters in the range '\u0800' to '\uFFFF' are represented by three bytes.
The differences between this format and the standard UTF-8 format are the following:
- The null byte '\u0000' is encoded in 2-byte format rather than 1-byte,
  so that the encoded strings never have embedded nulls.
- Only the 1-byte, 2-byte, and 3-byte formats are used.
- Supplementary characters are represented in the form of surrogate pairs.


================================================================================
JavaScript lang
================================================================================

https://certitude.consulting/blog/en/invisible-backdoor/
THE INVISIBLE JAVASCRIPT BACKDOOR


================================================================================
Julia lang
================================================================================

Remember: search in issues with "utf8proc in:title,body"


https://docs.julialang.org/en/v1/manual/strings/
    You can input any Unicode character in single quotes using \u followed by up to
    four hexadecimal digits or \U followed by up to eight hexadecimal digits
    (the longest valid value only requires six):

    julia> '\u0'
    '\0': ASCII/Unicode U+0000 (category Cc: Other, control)

    julia> '\u78'
    'x': ASCII/Unicode U+0078 (category Ll: Letter, lowercase)

    julia> '\u2200'
    '∀': Unicode U+2200 (category Sm: Symbol, math)

    julia> '\U10ffff'
    '\U10ffff': Unicode U+10FFFF (category Cn: Other, not assigned)

    julia> s = "\u2200 x \u2203 y"
    "∀ x ∃ y"


https://juliapackages.com/p/strs
    This uses Swift-style \ escape sequences, such as \u{xxxx} for Unicode constants,
    instead of \uXXXX and \UXXXXXXXX, which have the advantage of not having to worry
    about some digit or letter A-F or a-f occurring after the last hex digit of the Unicode constant.

    It also means that $, a very common character for LaTeX strings or output of currencies,
     does not need to be in a string quoted as '$'

    It uses \(expr) for interpolation like Swift, instead of $name or $(expr), which
    also has the advantage of not having to worry about the next character in the
    string someday being allowed in a name.

    It allows for embedding Unicode characters using a variety of easy to remember
    names, instead of hex codes: \:emojiname: \<latexname> \N{unicodename} \&htmlname;
    Examples of this are:
    f"\<dagger> \&yen; \N{ACCOUNT OF} \:snake:", which returns the string: "† ¥ ℀ 🐍 "


https://discourse.julialang.org/t/stupid-question-on-unicode/27674/7
    Discussion about escape sequence


https://docs.julialang.org/en/v1/stdlib/Unicode/
Unicode.julia_chartransform(c::Union{Char,Integer})
Unicode.isassigned(c) -> Bool
isequal_normalized(s1::AbstractString, s2::AbstractString; casefold=false, stripmark=false, chartransform=identity)
Unicode.normalize(s::AbstractString; keywords...)
    boolean keywords options (which all default to false except for compose)
        - compose=false: do not perform canonical composition
        - decompose=true: do canonical decomposition instead of canonical composition (compose=true is ignored if present)
        - compat=true: compatibility equivalents are canonicalized
        - casefold=true: perform Unicode case folding, e.g. for case-insensitive string comparison
        - newline2lf=true, newline2ls=true, or newline2ps=true: convert various newline sequences (LF, CRLF, CR, NEL) into a linefeed (LF), line-separation (LS), or paragraph-separation (PS) character, respectively
        - stripmark=true: strip diacritical marks (e.g. accents)
        - stripignore=true: strip Unicode's "default ignorable" characters (e.g. the soft hyphen or the left-to-right marker)
        - stripcc=true: strip control characters; horizontal tabs and form feeds are converted to spaces; newlines are also converted to spaces unless a newline-conversion flag was specified
        - rejectna=true: throw an error if unassigned code points are found
        - stable=true: enforce Unicode versioning stability (never introduce characters missing from earlier Unicode versions)
Unicode.normalize(s::AbstractString, normalform::Symbol)
    normalform can be :NFC, :NFD, :NFKC, or :NFKD.


utf8proc doesn't support language-sensitive case-folding
Julia, which uses utf8proc, has decided to remain locale-independent.
See https://github.com/JuliaLang/julia/issues/7848


https://github.com/JuliaLang/julia/pull/42493
This PR adds a function isequal_normalized to the Unicode stdlib to check whether
two strings are canonically equivalent (optionally casefolding and/or stripping combining marks).


https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/13
    julia> '\ub5'
    'µ': Unicode U+00b5 (category Ll: Letter, lowercase)

    julia> '\uff'
    'ÿ': Unicode U+00ff (category Ll: Letter, lowercase)

    julia> Base.Unicode.uppercase("ÿ")[1]
    'Ÿ': Unicode U+0178 (category Lu: Letter, uppercase)

    julia> Base.Unicode.uppercase("µ")[1]
    'Μ': Unicode U+039c (category Lu: Letter, uppercase)


================================================================================
Kotlin lang
================================================================================

https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/
https://github.com/JetBrains/kotlin/tree/master/libraries/stdlib/jvm/src/kotlin/text


================================================================================
Lisp lang
================================================================================

14/09/2021
https://www.gnu.org/software/emacs/manual/html_node/elisp/Character-Properties.html
    name
    Corresponds to the Name Unicode property. The value is a string consisting of upper-case Latin letters A to Z, digits, spaces, and hyphen ‘-’ characters. For unassigned codepoints, the value is nil.

    general-category
    Corresponds to the General_Category Unicode property. The value is a symbol whose name is a 2-letter abbreviation of the character’s classification. For unassigned codepoints, the value is Cn.

    canonical-combining-class
    Corresponds to the Canonical_Combining_Class Unicode property. The value is an integer. For unassigned codepoints, the value is zero.

    bidi-class
    Corresponds to the Unicode Bidi_Class property. The value is a symbol whose name is the Unicode directional type of the character. Emacs uses this property when it reorders bidirectional text for display (see Bidirectional Display). For unassigned codepoints, the value depends on the code blocks to which the codepoint belongs: most unassigned codepoints get the value of L (strong L), but some get values of AL (Arabic letter) or R (strong R).

    decomposition
    Corresponds to the Unicode properties Decomposition_Type and Decomposition_Value. The value is a list, whose first element may be a symbol representing a compatibility formatting tag, such as small18; the other elements are characters that give the compatibility decomposition sequence of this character. For characters that don’t have decomposition sequences, and for unassigned codepoints, the value is a list with a single member, the character itself.

    decimal-digit-value
    Corresponds to the Unicode Numeric_Value property for characters whose Numeric_Type is ‘Decimal’. The value is an integer, or nil if the character has no decimal digit value. For unassigned codepoints, the value is nil, which means NaN, or “not a number”.

    digit-value
    Corresponds to the Unicode Numeric_Value property for characters whose Numeric_Type is ‘Digit’. The value is an integer. Examples of such characters include compatibility subscript and superscript digits, for which the value is the corresponding number. For characters that don’t have any numeric value, and for unassigned codepoints, the value is nil, which means NaN.

    numeric-value
    Corresponds to the Unicode Numeric_Value property for characters whose Numeric_Type is ‘Numeric’. The value of this property is a number. Examples of characters that have this property include fractions, subscripts, superscripts, Roman numerals, currency numerators, and encircled numbers. For example, the value of this property for the character U+2155 VULGAR FRACTION ONE FIFTH is 0.2. For characters that don’t have any numeric value, and for unassigned codepoints, the value is nil, which means NaN.

    mirrored
    Corresponds to the Unicode Bidi_Mirrored property. The value of this property is a symbol, either Y or N. For unassigned codepoints, the value is N.

    mirroring
    Corresponds to the Unicode Bidi_Mirroring_Glyph property. The value of this property is a character whose glyph represents the mirror image of the character’s glyph, or nil if there’s no defined mirroring glyph. All the characters whose mirrored property is N have nil as their mirroring property; however, some characters whose mirrored property is Y also have nil for mirroring, because no appropriate characters exist with mirrored glyphs. Emacs uses this property to display mirror images of characters when appropriate (see Bidirectional Display). For unassigned codepoints, the value is nil.

    paired-bracket
    Corresponds to the Unicode Bidi_Paired_Bracket property. The value of this property is the codepoint of a character’s paired bracket, or nil if the character is not a bracket character. This establishes a mapping between characters that are treated as bracket pairs by the Unicode Bidirectional Algorithm; Emacs uses this property when it decides how to reorder for display parentheses, braces, and other similar characters (see Bidirectional Display).

    bracket-type
    Corresponds to the Unicode Bidi_Paired_Bracket_Type property. For characters whose paired-bracket property is non-nil, the value of this property is a symbol, either o (for opening bracket characters) or c (for closing bracket characters). For characters whose paired-bracket property is nil, the value is the symbol n (None). Like paired-bracket, this property is used for bidirectional display.

    old-name
    Corresponds to the Unicode Unicode_1_Name property. The value is a string. For unassigned codepoints, and characters that have no value for this property, the value is nil.

    iso-10646-comment
    Corresponds to the Unicode ISO_Comment property. The value is either a string or nil. For unassigned codepoints, the value is nil.

    uppercase
    Corresponds to the Unicode Simple_Uppercase_Mapping property. The value of this property is a single character. For unassigned codepoints, the value is nil, which means the character itself.

    lowercase
    Corresponds to the Unicode Simple_Lowercase_Mapping property. The value of this property is a single character. For unassigned codepoints, the value is nil, which means the character itself.

    titlecase
    Corresponds to the Unicode Simple_Titlecase_Mapping property. Title case is a special form of a character used when the first character of a word needs to be capitalized. The value of this property is a single character. For unassigned codepoints, the value is nil, which means the character itself.

    special-uppercase
    Corresponds to Unicode language- and context-independent special upper-casing rules. The value of this property is a string (which may be empty). For example mapping for U+00DF LATIN SMALL LETTER SHARP S is "SS". For characters with no special mapping, the value is nil which means uppercase property needs to be consulted instead.

    special-lowercase
    Corresponds to Unicode language- and context-independent special lower-casing rules. The value of this property is a string (which may be empty). For example mapping for U+0130 LATIN CAPITAL LETTER I WITH DOT ABOVE the value is "i\u0307" (i.e. 2-character string consisting of LATIN SMALL LETTER I followed by U+0307 COMBINING DOT ABOVE). For characters with no special mapping, the value is nil which means lowercase property needs to be consulted instead.

    special-titlecase
    Corresponds to Unicode unconditional special title-casing rules. The value of this property is a string (which may be empty). For example mapping for U+FB01 LATIN SMALL LIGATURE FI the value is "Fi". For characters with no special mapping, the value is nil which means titlecase property needs to be consulted instead.


================================================================================
Mathematica lang
================================================================================

https://www.youtube.com/watch?v=yiwLBvirm7A
Live CEOing Ep 426: Language Design in Wolfram Language [Unicode Characters & WFR Suggestions]
At the begining, there are a few minutes about character properties.


================================================================================
MOAR-VM RAKU lang
================================================================================

29/05/2021
http://moarvm.com/releases.html
    2017.07
        Greatly reduce the cases when string concatenation needs renormalization
        Use normalize_should_break to decide if concat needs normalization
        Rename should_break to MVM_unicode_normalize_should_break
        Fix memory leak in MVM_nfg_is_concat_stable
        If both last_a and first_b during concat are non-0 CCC, re-NFG
    --> maybe to review : the last sentence seems to be an optimization of concatenation.
    2017.02
        Implement support for synthetic graphemes in MVM_unicode_string_compare
        Implement configurable collation_mode for MVM_unicode_string_compare
    2017.01
        Add a new unicmp_s op, which compares using the Unicode Collation Algorithm
        Add support for Grapheme_Cluster_Break=Prepend from Unicode 9.0
        Add a script to download the latest version of all of the Unicode data
    --> should review this script
    2015.11
        NFG now uses Unicode Grapheme Cluster algorithm; "\r\n" is now one grapheme
    --> ??? [later] ah, I had a bug! Was not analyzing an UTF-8 ASCII string... Now fixed:
        "0A0D"x~text~description= -- UTF-8 ASCII ( 2 graphemes, 2 codepoints, 2 bytes )
        "0D0A"x~text~description= -- UTF-8 ASCII ( 1 grapheme, 2 codepoints, 2 bytes )


29/05/2021
https://news.ycombinator.com/item?id=26591373
String length functions for single emoji characters evaluate to greater than 1
--> to check : MOAR VM really concatenate a 8bit string with a 32bit string using a string concatenation object ?

    You could do it the way Raku does. It's implementation defined. (Rakudo on MoarVM)
    The way MoarVM does it is that it does NFG, which is sort of like NFC except that it stores grapheme clusters as if they were negative codepoints.

    If a string is ASCII it uses an 8bit storage format, otherwise it uses a 32bit one.
    It also creates a tree of immutable string objects.
    If you do a substring operation it creates a substring object that points at an existing string object.
    If you combine two strings it creates a string concatenation object. Which is useful for combining an 8bit string with a 32bit one.
    All of that is completely opaque at the Raku level of course.

        my $str = "\c[FACE PALM, EMOJI MODIFIER FITZPATRICK TYPE-3, ZWJ, MALE SIGN, VARIATION SELECTOR-16]";

        say $str.chars;        # 1
        say $str.codes;        # 5
        say $str.encode('utf16').elems; # 7
        say $str.encode('utf16').bytes; # 14
        say $str.encode.elems; # 17
        say $str.encode.bytes; # 17
        say $str.codes * 4;    # 20
        #(utf32 encode/decode isn't implemented in MoarVM yet)

        say for $str.uninames;
        # FACE PALM
        # EMOJI MODIFIER FITZPATRICK TYPE-3
        # ZERO WIDTH JOINER
        # MALE SIGN
        # VARIATION SELECTOR-16
    The reason we have utf8-c8 encode/decode is because filenames, usernames, and passwords are not actually Unicode.
    (I have 4 files all named rèsumè in the same folder on my computer.)
    utf8-c8 uses the same synthetic codepoint system as grapheme clusters.


https://andrewshitov.com/2018/10/31/unicode-in-perl-6/
Unicode in Raku


https://docs.raku.org/language/unicode
Raku applies normalization by default to all input and output except for file names,
which are read and written as UTF8-C8
UTF-8 Clean-8 is an encoder/decoder that primarily works as the UTF-8 one. However,
upon encountering a byte sequence that will either not decode as valid UTF-8, or
that would not round-trip due to normalization, it will use NFG synthetics to
keep track of the original bytes involved. This means that encoding back to UTF-8 Clean-8
will be able to recreate the bytes as they originally existed.


https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc
Strings in MoarVM
Strands
    Strands are a type of MVMString which instead of being a flat string with contiguous data,
    actually contains references to other strings. Strands are created during concatenation
    or substring operations. When two flat strings are concatenated together, a Strand with
    references to both string a and string b is created. If string a and string b were strands
    themselves, the references of string a and references of string b are copied one after another
    into the Strand.
Synthetic’s
    Synthetics are graphemes which contain multiple codepoints. In MoarVM these are stored
    and accessed using a trie, while the actual data itself stores the base character seprately
    and then the combiners are stored in an array.
    Currently the maximum number of combiners in a synthetic is 1024.
    MoarVM will throw an exception if you attempt to create a grapheme with more than 1024 codepoints in it.
Normalization
    MoarVM normalizes into NFG form all input text.
NFG
    Normalization Form Grapheme. Similar to NFC except graphemes which contain multiple codepoints
    are stored in Synthetic graphemes.

================================================================================
Oracle
================================================================================

https://docs.oracle.com/database/121/NLSPG/ch5lingsort.htm#NLSPG288
Database Globalization Support Guide
5 Linguistic Sorting and Matching
Complex! Did not read in details, maybe I should...


https://docs.oracle.com/database/121/NLSPG/ch6unicode.htm#NLSPG323
Database Globalization Support Guide
6 Supporting Multilingual Databases with Unicode


https://docs.oracle.com/database/121/NLSPG/ch7progrunicode.htm#NLSPG346
Database Globalization Support Guide
7 Programming with Unicode


================================================================================
Perl lang (Perl 6 has been renamed to Raku)
================================================================================

https://swigunicode.wordpress.com/2021/10/18/example-post-3/
    SWIG and Perl: Unicode C Library
    Part 1. Small Intro to SWIG

    https://swigunicode.wordpress.com/2021/10/22/part-2-c-header-file/
    Part 2. C Header File

    https://swigunicode.wordpress.com/2021/10/24/part-3-c-source-file/
    Part 3. C Source File

    https://swigunicode.wordpress.com/2021/10/25/part-4-perl-source-file/
    Part 4. Perl Source File

    https://swigunicode.wordpress.com/2021/10/26/part-5-build-and-run-scripts/
    Part 5. Build and Run Scripts

    https://swigunicode.wordpress.com/2021/10/27/part-6-swig-interface-file/
    Part 6. SWIG Interface File


https://lwn.net/Articles/667684/
An article about NFG.
Unless one specifies otherwise, Perl 6 normalizes a text string to NFC when it's not NFG.


================================================================================
PHP lang
================================================================================

https://github.com/nicolas-grekas/Patchwork-UTF8
Extensive, portable and performant handling of UTF-8 and grapheme clusters for PHP


================================================================================
Powershell lang
================================================================================

https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window
Using UTF-8 Encoding (CHCP 65001) in Command Prompt / Windows Powershell (Windows 10)
Describes how o set the system locale (language for non-Unicode programs) to UTF-8.
Optional reading: Why the Windows PowerShell ISE is a poor choice


https://stackoverflow.com/questions/49476326/displaying-unicode-in-powershell/49481797#49481797
Displaying Unicode in Powershell


================================================================================
Python lang
================================================================================

10/08/2021
List of Python PEPS related to string.
https://www.python.org/dev/peps/
    Other Informational PEPs
        I	257	Docstring Conventions	Goodger, GvR
        I	287	reStructuredText Docstring Format	Goodger

    Accepted PEPs (accepted; may not be implemented yet)
        SA	597	Add optional EncodingWarning	Naoki
        SA	616	String methods to remove prefixes and suffixes	Sweeney
        SA	623	Remove wstr from Unicode	Naoki

    Open PEPs (under consideration)
        S	558	Defined semantics for locals()	Coghlan

    Finished PEPs (done, with a stable interface)
        SF	100	Python Unicode Integration	Lemburg
        SF	260	Simplify xrange()	GvR
        SF	261	Support for "wide" Unicode characters	Prescod
        SF	263	Defining Python Source Code Encodings	Lemburg, von Löwis
        SF	277	Unicode file name support for Windows NT	Hodgson
        SF	278	Universal Newline Support	Jansen
        SF	292	Simpler String Substitutions	Warsaw
        SF	331	Locale-Independent Float/String Conversions	Reis
        SF	393	Flexible String Representation	v. Löwis
        SF	414	Explicit Unicode Literal for Python 3.3	Ronacher, Coghlan
        SF	498	Literal String Interpolation	Smith
        SF	515	Underscores in Numeric Literals	Brandl, Storchaka
        SF	528	Change Windows console encoding to UTF-8	Dower
        SF	529	Change Windows filesystem encoding to UTF-8	Dower
        SF	538	Coercing the legacy C locale to a UTF-8 based locale	Coghlan
        SF	540	Add a new UTF-8 Mode	Stinner
        SF	624	Remove Py_UNICODE encoder APIs	Naoki
        SF	3101	Advanced String Formatting	Talin
        SF	3112	Bytes literals in Python 3000	Orendorff
        SF	3120	Using UTF-8 as the default source encoding	von Löwis
        SF	3127	Integer Literal Support and Syntax	Maupin
        SF	3131	Supporting Non-ASCII Identifiers	von Löwis
        SF	3137	Immutable Bytes and Mutable Buffer	GvR
        SF	3138	String representation in Python 3000	Ishimoto

    Deferred PEPs (postponed pending further research or updates)
        SD	501	General purpose string interpolation	Coghlan
        SD	536	Final Grammar for Literal String Interpolation	Angerer

    Abandoned, Withdrawn, and Rejected PEPs
        SS	215	String Interpolation	Yee
        IR	216	Docstring Format	Zadka
        SR	224	Attribute Docstrings	Lemburg
        SR	256	Docstring Processing System Framework	Goodger
        SR	295	Interpretation of multiline string constants	Koltsov
        SR	332	Byte vectors and String/Unicode Unification	Montanaro
        SR	349	Allow str() to return unicode strings	Schemenauer
        IR	502	String Interpolation - Extended Discussion	Miller
        SR	3126	Remove Implicit String Concatenation	Jewett, Hettinger


15/07/2021
review
https://docs.python.org/3/howto/unicode.html

    Escape sequences in string literals
        "\N{GREEK CAPITAL LETTER DELTA}"        # Using the character name  '\u0394'
        "\u0394"                                # Using a 16-bit hex value  '\u0394'
        "\U00000394"                            # Using a 32-bit hex value  '\u0394'

    One can create a string using the decode() method of bytes.
    This method takes an encoding argument, such as UTF-8, and optionally an errors argument.
    The errors argument specifies the response when the input string can’t be converted
    according to the encoding’s rules. Legal values for this argument are
        'strict'            (raise a UnicodeDecodeError exception),
        'replace'           (use U+FFFD, REPLACEMENT CHARACTER),
        'ignore'            (just leave the character out of the Unicode result),
        'backslashreplace'  (inserts a \xNN escape sequence).
    Examples:
        b'\x80abc'.decode("utf-8", "strict")                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0
        b'\x80abc'.decode("utf-8", "replace")               # '\ufffdabc'
        b'\x80abc'.decode("utf-8", "backslashreplace")      # '\\x80abc'
        b'\x80abc'.decode("utf-8", "ignore")                # 'abc'

    Encodings are specified as strings containing the encoding’s name.
    Python comes with roughly 100 different encodings:
        https://docs.python.org/3/library/codecs.html#standard-encodings

    One-character Unicode strings can also be created with the chr() built-in function,
    which takes integers and returns a Unicode string of length 1 that contains the corresponding code point:
        chr(57344)      # '\ue000'
    The reverse operation is the built-in ord() function that takes a one-character Unicode string and returns the code point value:
        ord('\ue000')   # 57344

    The opposite method of bytes.decode() is str.encode(), which returns a bytes representation of the Unicode string, encoded in the requested encoding.
    The errors parameter is the same as the parameter of the decode() method but supports a few more possible handlers.
        'strict'            (raise a UnicodeDecodeError exception),
        'replace'           inserts a question mark instead of the unencodable character,
        'ignore'            (just leave the character out of the Unicode result),
        'backslashreplace'  (inserts a \uNNNN escape sequence)
        'xmlcharrefreplace' (inserts an XML character reference),
        'namereplace'       (inserts a \N{...} escape sequence).

    Unicode code points can be written using the \u escape sequence, which is
    followed by four hex digits giving the code point. The \U escape sequence
    is similar, but expects eight hex digits, not four
        >>> s = "a\xac\u1234\u20ac\U00008000"
        ... #     ^^^^ two-digit hex escape
        ... #         ^^^^^^ four-digit Unicode escape
        ... #                     ^^^^^^^^^^ eight-digit Unicode escape
        >>> [ord(c) for c in s]
        [97, 172, 4660, 8364, 32768]

    Python supports writing source code in UTF-8 by default, but you can use almost any encoding if you declare the encoding being used. This is done by including a special comment as either the first or second line of the source file:
        #!/usr/bin/env python
        # -*- coding: latin-1 -*-
        u = 'abcdé'
    https://www.python.org/dev/peps/pep-0263/
    PEP 263 -- Defining Python Source Code Encodings

    Comparing Strings
    The casefold() string method converts a string to a case-insensitive
    form following an algorithm described by the Unicode Standard. This
    algorithm has special handling for characters such as the German letter ‘ß’
    (code point U+00DF), which becomes the pair of lowercase letters ‘ss’.
        >>> street = 'Gürzenichstraße'
        >>> street.casefold()
        'gürzenichstrasse'
    The unicodedata module’s normalize() function converts strings to one of
    several normal forms: ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.
        def compare_strs(s1, s2):
            def NFD(s):
                return unicodedata.normalize('NFD', s)
            return NFD(s1) == NFD(s2)
    The Unicode Standard also specifies how to do caseless comparisons:
        def compare_caseless(s1, s2):
            def NFD(s):
                return unicodedata.normalize('NFD', s)
            return NFD(NFD(s1).casefold()) == NFD(NFD(s2).casefold())
    Why is NFD() invoked twice? Because there are a few characters that make
    casefold() return a non-normalized string, so the result needs to be
    normalized again. See section 3.13 of the Unicode Standard

    https://docs.python.org/3/library/unicodedata.html
        unicodedata.lookup(name)
            Look up character by name.
            If a character with the given name is found, return the corresponding character.
            If not found, KeyError is raised.
            Changed in version 3.3: Support for name aliases 1 and named sequences 2 has been added.
        unicodedata.name(chr[, default])
            Returns the name assigned to the character chr as a string.
        unicodedata.decimal(chr[, default])
            Returns the decimal value assigned to the character chr as integer.
        unicodedata.digit(chr[, default])
            Returns the digit value assigned to the character chr as integer.
        unicodedata.numeric(chr[, default])
            Returns the numeric value assigned to the character chr as float.
        unicodedata.category(chr)
            Returns the general category assigned to the character chr as string.
        unicodedata.bidirectional(chr)
            Returns the bidirectional class assigned to the character chr as string.
        unicodedata.combining(chr)
            Returns the canonical combining class assigned to the character chr as integer.
            Returns 0 if no combining class is defined.
        unicodedata.east_asian_width(chr)
            Returns the east asian width assigned to the character chr as string.
        unicodedata.mirrored(chr)
            Returns the mirrored property assigned to the character chr as integer.
            Returns 1 if the character has been identified as a “mirrored” character in bidirectional text, 0 otherwise.
        unicodedata.decomposition(chr)
            Returns the character decomposition mapping assigned to the character chr as string.
            An empty string is returned in case no such mapping is defined.
        unicodedata.normalize(form, unistr)
            Return the normal form form for the Unicode string unistr.
            Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.
        unicodedata.is_normalized(form, unistr)
            Return whether the Unicode string unistr is in the normal form form.
            Valid values for form are ‘NFC’, ‘NFKC’, ‘NFD’, and ‘NFKD’.
        unicodedata.unidata_version
            The version of the Unicode database used in this module.
        unicodedata.ucd_3_2_0
            This is an object that has the same methods as the entire module,
            but uses the Unicode database version 3.2 instead

    https://www.python.org/dev/peps/pep-0393/
    PEP 393 -- Flexible String Representation
        When creating new strings, it was common in Python to start of with a
        heuristical buffer size, and then grow or shrink if the heuristics failed.
        With this PEP, this is now less practical, as you need not only a heuristics
        for the length of the string, but also for the maximum character.

        In order to avoid heuristics, you need to make two passes over the input:
        once to determine the output length, and the maximum character; then
        allocate the target string with PyUnicode_New and iterate over the input
        a second time to produce the final output. While this may sound expensive,
        it could actually be cheaper than having to copy the result again as in
        the following approach.

        If you take the heuristical route, avoid allocating a string meant to be
        resized, as resizing strings won't work for their canonical representation.
        Instead, allocate a separate buffer to collect the characters, and then
        construct a unicode object from that using PyUnicode_FromKindAndData.
        One option is to use Py_UCS4 as the buffer element, assuming for the worst
        case in character ordinals. This will allow for pointer arithmetics, but
         may require a lot of memory. Alternatively, start with a 1-byte buffer,
         and increase the element size as you encounter larger characters.
         In any case, PyUnicode_FromKindAndData will scan over the buffer to
         verify the maximum character.


15/07/2021
https://docs.python.org/3/library/codecs.html
Codec registry and base classes
Most standard codecs are text encodings, which encode text to bytes, but there
are also codecs provided that encode text to text, and bytes to bytes.
errors string argument:
    strict
    ignore
    replace
    xmlcharrefreplace
    backslashreplace
    namereplace
    surrogateescape
    surrogatepass


15/07/2021
https://discourse.julialang.org/t/a-python-rant-about-types/43294/22
A Python rant about types
jlf: the main discussion is about invalid string data.
Stefan Karpinski describes the Julia strings:
    1. You can read and write any data, valid or not.
    2. It is interpreted as UTF-8 where possible and as invalid characters otherwise.
    3. You can simply check if strings or chars are valid UTF-8 or not.
    4. You can work with individual characters easily, even invalid ones.
    5. You can losslessly read and write any string data, valid or not, as strings or chars.
    6. You only get an error when you try to ask for the code point of an invalid char.
    Most Julia code that works with strings is automatically robust with respect to
    invalid UTF-8 data. Only code that needs to look at the code points of individual
    characters will fail on invalid data; in order to do that robustly, you simply
    need to check if the character is valid before taking its code point and handle
    that appropriately.
jlf: I think that all the Julia methods working at character level will raise an error,
not just when looking at the code point.
jlf: Stefan Karpinski explains why Python design is problematic.
Python 3 has to be able to represent any input string in terms of code points.
Needing to turn every string into a fixed-width sequence of code points puts them
in a tough position with respect to invalid strings where there is simply no
corresponding sequence of code points.


17/07/2021
https://groups.google.com/g/python-ideas/c/wStIS1_NVJQ
Fix default encodings on Windows
jlf: did not read in details, too long, too many feedbacks.
Maybe some comments are interesting, so I save this URL.


https://djangocas.dev/blog/python-unicode-string-lowercase-casefold-caseless-match/
Interesting infos about caseless matching


https://gist.github.com/dpk/8325992
PyICU cheat sheet


================================================================================
R lang
================================================================================

https://stringi.gagolewski.com/index.html
stringi: Fast and Portable Character String Processing in R
stringi (pronounced “stringy”, IPA [strinɡi]) is THE R package for very fast, portable,
correct, consistent, and convenient string/text processing in any locale or character encoding.
Thanks to ICU, stringi fully supports a wide range of Unicode standards.
Paper (PDF): https://www.jstatsoft.org/index.php/jss/article/view/v103i02/4324

https://github.com/gagolews/stringi
Fast and Portable Character String Processing in R (with the Unicode ICU)


================================================================================
Rexx lang
================================================================================

11/08/2021
http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEIN
Reads in a numeric value from a binary (ie, non-text) file.
value = VALUEIN(stream, position, length, options)
    Args
        stream is the name of the stream.
        It can include the full path to the stream (ie, any drive and directory names).
        If omitted, the default is to read from STDIN.

        position specifies at what character position (within the stream) to start
        reading from, where 1 means to start reading at the very first character
        in the stream. If omitted, the default is to resume reading at where a
        previous call to CHARIN() or VALUEIN() left off (ie, where you current
        read character position is).

        length is a 1 to read in the next binary byte (ie, 8-bit value), a 2 to
        read in the next binary short (ie, 16-bit value), or a 4 to read in the
        next binary long (ie, 32-bit value). If length is omitted, VALUEIN() defaults to reading a byte.

        options can be any of the following:
            M	The value is stored (in the stream) in Motorola (big endian) byte order,
                rather than Intel (little endian) byte order.
                The effects only long and short values.
            H	Read in the value as hexadecimal (rather than the default of base 10,
                or decimal, which is the base that REXX uses to express numbers).
                The value can later be converted with X2D().
            B	Read in the value as binary (base 2).
            -	The value is signed (as opposed to unsigned).
            V	stream is the actual data string from which to extract a value.
                You can now replace calls to SUBSTR and C2D with a single, faster call to VALUEIN.
            If omitted, options defaults to none of the above.
    Returns
        The value, if successful.
        If an error, an empty string is returned (unless the NOTREADY condition
        is trapped via CALL method. Then, a '0' is returned).

http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEOUT
Write out numeric values to a binary (ie, non-text) file (ie, in non-text format).
result = VALUEOUT(stream, values, position, size, options)
    Args
        stream is the name of the stream.
        It can include the full path to the stream (ie, any drive and directory names).
        If omitted, the default is to write to STDOUT (typically, display the data in the console window).

        position specifies at what character position (within the stream) to start writing the data,
        where 1 means to start writing at the very first character in the stream.
        If omitted, the default is to resume writing at where a previous call to
        CHAROUT() or VALUEOUT() left off (or where the "write character pointer" was set via STREAM's SEEK).

        values are the numeric values (ie, data) to write out.
        Each value is separated by one space.

        size is a 1 if each value is to be written as a byte (ie, 8-bit value),
        2 if each value is to be written as a short (16-bit value),
        or 4 if each value is to be written as a long (32-bit value). If omitted, size defaults to 1.

        options can be any of the following:
            M	Write out the values in Motorola (big endian) byte order,
                rather than Intel (little endian) byte order. The effects only long and short values.
            H	The values you supplied are specified in hexadecimal.
            B	The values you supplied are specified in binary (base 2).
            V	stream is the name of a variable, and the data will be overlaid
                onto that variable's value. You can now replace calls to D2C and
                OVERLAY with a single, faster call to VALUEOUT, especially when
                a variable has a large amount of non-text data.
            If omitted, options defaults to none of the above.
    Returns
        0 if the string was written out successfully.
        If an error, VALUEOUT() returns non-zero.


http://www.dg77.net/tekno/manuel/rexxendian.htm
Test de l’endianité
    /* Verifie l'endianité / check endiannity          */
    /* Pour traitement d'information encodees en UTF-8 */
    /* Adapter si on utilise un autre encodage         */
    CALL CONV8_16 ' '
    IF c2x(sortie) = '2000' THEN DO
        endian = 'LE' /* little endian  */
        blanx = '2000'
        END
    ELSE DO
        endian = 'BE' /* big endian  */
        blanx = '0020'
        END
    return endian blanx
    /* ********************************************************************** */
    /*           Conversion UTF-8 -> UNICODE                                  */
    CONV8_16:
    parse arg entree
    sortie = ''
    ZONESORTIE.='NUL'; ZONESORTIE.0=0
    err = systounicode(entree, 'UTF8', , ZONESORTIE.)
    if err == 0 then sortie = ZONESORTIE.!TEXT
      else say 'probleme car., code ' err
    return


http://www.dg77.net/tekno/xhtml/codage.htm
Le codage des caractères
To read, some infos about the code pages could be useful.


Regina doc
    EXPORT(address, [string], [length] [,pad]) - (AREXX)
        Copies data from the (optional) string into a previously-allocated memory area, which must be
        specified as a 4-byte address. The length parameter specifies the maximum number of characters to
        be copied; the default is the length of the string. If the specified length is longer than the string, the
        remaining area is filled with the pad character or nulls('00'x). The returned value is the number
        of characters copied.
        Caution is advised in using this function. Any area of memory can be overwritten,possibly
        causing a system crash.
        See also STORAGE() and IMPORT().
        Note that the address specified is subject to a machine's endianess.
        EXPORT('0004 0000'x,'The answer') '10'

    IMPORT(address [,length]) - (AREXX)
        Creates a string by copying data from the specified 4-byte address. If the length parameter is not
        supplied,the copy terminates when a null byte is found.
        See also EXPORT()
        Note that the address specified is subject to a machine's endianess.
        IMPORT('0004 0000'x,10) 'The answer' /* maybe */


================================================================================
Rust lang
================================================================================

Seen in a comment here :  https://bugs.swift.org/browse/SR-7602

    For reference, I think [Rust's model]( https://doc.rust-lang.org/std/string/struct.String.html ) is pretty good:

    `from_utf8` produces an error explaining why the code units were invalid
    `from_utf8_lossy` replaces encoding errors with U+FFFD
    `from_utf8_unchecked` which takes the bytes, but if there's an encoding error, then memory safety has been violated

    I'm not entirely sure if accepting invalid bytes requires voiding memory safety (assuming bounds checking always happens), but it is totally a security hazard if used improperly.
    We may want to be very cautious about if/how we expose it.

    I think that trying to do read-time validation is dubious for UTF-16, and totally bananas for UTF-8.


17/07/2021
https://www.generacodice.com/en/articolo/120763/Unicode+Support+in+Various+Programming+Languages
jlf: I learned something: OsStr/OsString
    Rust's strings (std::String and &str) are always valid UTF-8, and do not use null
    terminators, and as a result can not be indexed as an array, like they can be in C/C++, etc.
    They can be sliced somewhat like Go using .get since 1.20, with the caveat that
    it will fail if you try slicing the middle of a code point.

    Rust also has OsStr/OsString for interacting with the Host OS.
    It's byte array on Unix (containing any sequence of bytes).
    On windows it's WTF-8 (A super-set of UTF-8 that handles the improperly
    formed Unicode strings that are allowed in Windows and Javascript),
    &str and String can be freely converted to OsStr or OsString, but require
    checks to covert the other way. Either by Failing on invalid unicode, or
    replacing with the Unicode replacement char. (There is also Path/PathBuf,
    which are just wrappers around OsStr/OsString).

    There is also the CStr and CString types, which represent Null terminated C
    strings, like OsStr on Unix they can contain arbitrary bytes.

    Rust doesn't directly support UTF-16. But can convert OsStr to UCS-2 on windows.


22/07/2021
https://lib.rs/crates/
STFU-8: Sorta Text Format in UTF-8
STFU-8 is a hacky text encoding/decoding protocol for data that might be not
quite UTF-8 but is still mostly UTF-8.
Its primary purpose is to be able to allow a human to visualize and edit "data"
that is mostly (or fully) visible UTF-8 text. It encodes all non visible or non
UTF-8 compliant bytes as longform text (i.e. ESC becomes the full string r"\x1B").
It can also encode/decode ill-formed UTF-16.


28/07/2021
https://fasterthanli.me/articles/working-with-strings-in-rust


07/11/2021
https://blog.rust-lang.org/2021/11/01/cve-2021-42574.html
security concern affecting source code containing "bidirectional override" Unicode codepoints


10/03/2022
https://rust-lang.github.io/rfcs/2457-non-ascii-idents.html
Allow non-ASCII letters (such as accented characters, Cyrillic, Greek, Kanji, etc.) in Rust identifiers.


10/09/2022
https://blog.burntsushi.net/bstr/
A byte string library for Rust
Invalid UTF-8 doesn’t actually prevent one from applying Unicode-aware algorithms on the parts
of the string that are valid UTF-8. The parts that are invalid UTF-8 are simply ignored.


15/10/2022
https://crates.io/crates/finl_unicode
Library for handling Unicode functionality for finl (categories and grapheme segmentation)
There are these comments in https://news.ycombinator.com/item?id=32700315
    All with two-step tables instead of range- and binary search?
    Yes. The two-step tables are really not that expensive and they enable features not possible with range and binary search, like identifying the category of a character cheaply.


https://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/
"baﬄe".upcase == "BAFFLE"


================================================================================
Swift lang
================================================================================

03/08/2021
https://swiftdoc.org/v5.1/type/string/
Auto-generated documentation for Swift.
A Unicode string value that is a collection of characters.


15/07/2017
String Processing For Swift 4
https://github.com/apple/swift/blob/master/docs/StringManifesto.md


https://swift.org/blog/utf8-string/
Swift 5 switches the preferred encoding of strings from UTF-16 to UTF-8 while preserving efficient Objective-C-interoperability.


https://bugs.swift.org/browse/SR-7602
UTF8 should be (one of) the fastest String encoding(s)


https://github.com/apple/swift/blob/7e68e8f4a3cb1173e909dc22a3490c05e43fa592/stdlib/public/core/StringObject.swift
swift/stdlib/public/core/StringObject.swift


milseman Michael Ilseman added a comment - 5 Nov 2018 3:44 PM
    It's now the fastest encoding.
    https://forums.swift.org/t/string-s-abi-and-utf-8/17676/1
    https://github.com/apple/swift/pull/20315


13/06/2021
https://github.com/apple/swift-evolution/blob/master/proposals/0211-unicode-scalar-properties.md
Add Unicode Properties to Unicode.Scalar
    Issues Linking with ICU
    The Swift standard library uses the system's ICU libraries to implement its Unicode support.
    A third-party developer may expect that they could also link their application directly to the system ICU
    to access the functionality that they need, but this proves problematic on both Apple and Linux platforms.
    Apple
        On Apple operating systems, libicucore.dylib is built with function renaming disabled
        (function names lack the _NN version number suffix). This makes it fairly straightforward to import the C APIs
        and call them from Swift without worrying about which version the operating system is using.
        Unfortunately, libicucore.dylib is considered to be private API for submissions to the App Store,
        so applications doing this will be rejected. Instead, users must built their own copy of ICU from source
        and link that into their applications. This is significant overhead.
    Linux
        On Linux, system ICU libraries are built with function renaming enabled (the default),
        so function names have the _NN version number suffix. Function renaming makes it more difficult
        to use these APIs from Swift; even though the C header files contain #defines that map function names
        like u_foo_59 to u_foo, these #defines are not imported into Swift—only the suffixed function names are available.
        This means that Swift bindings would be fixed to a specific version of the library without some other intermediary layer.
        Again, this is significant overhead.
    extension Unicode.Scalar.Properties {
      public var isAlphabetic: Bool { get }    // Alphabetic
      public var isASCIIHexDigit: Bool { get }    // ASCII_Hex_Digit
      public var isBidiControl: Bool { get }    // Bidi_Control
      public var isBidiMirrored: Bool { get }    // Bidi_Mirrored
      public var isDash: Bool { get }    // Dash
      public var isDefaultIgnorableCodePoint: Bool { get }    // Default_Ignorable_Code_Point
      public var isDeprecated: Bool { get }    // Deprecated
      public var isDiacritic: Bool { get }    // Diacritic
      public var isExtender: Bool { get }    // Extender
      public var isFullCompositionExclusion: Bool { get }    // Full_Composition_Exclusion
      public var isGraphemeBase: Bool { get }    // Grapheme_Base
      public var isGraphemeExtend: Bool { get }    // Grapheme_Extend
      public var isHexDigit: Bool { get }    // Hex_Digit
      public var isIDContinue: Bool { get }    // ID_Continue
      public var isIDStart: Bool { get }    // ID_Start
      public var isIdeographic: Bool { get }    // Ideographic
      public var isIDSBinaryOperator: Bool { get }    // IDS_Binary_Operator
      public var isIDSTrinaryOperator: Bool { get }    // IDS_Trinary_Operator
      public var isJoinControl: Bool { get }    // Join_Control
      public var isLogicalOrderException: Bool { get }    // Logical_Order_Exception
      public var isLowercase: Bool { get }    // Lowercase
      public var isMath: Bool { get }    // Math
      public var isNoncharacterCodePoint: Bool { get }    // Noncharacter_Code_Point
      public var isQuotationMark: Bool { get }    // Quotation_Mark
      public var isRadical: Bool { get }    // Radical
      public var isSoftDotted: Bool { get }    // Soft_Dotted
      public var isTerminalPunctuation: Bool { get }    // Terminal_Punctuation
      public var isUnifiedIdeograph: Bool { get }    // Unified_Ideograph
      public var isUppercase: Bool { get }    // Uppercase
      public var isWhitespace: Bool { get }    // Whitespace
      public var isXIDContinue: Bool { get }    // XID_Continue
      public var isXIDStart: Bool { get }    // XID_Start
      public var isCaseSensitive: Bool { get }    // Case_Sensitive
      public var isSentenceTerminal: Bool { get }    // Sentence_Terminal (S_Term)
      public var isVariationSelector: Bool { get }    // Variation_Selector
      public var isNFDInert: Bool { get }    // NFD_Inert
      public var isNFKDInert: Bool { get }    // NFKD_Inert
      public var isNFCInert: Bool { get }    // NFC_Inert
      public var isNFKCInert: Bool { get }    // NFKC_Inert
      public var isSegmentStarter: Bool { get }    // Segment_Starter
      public var isPatternSyntax: Bool { get }    // Pattern_Syntax
      public var isPatternWhitespace: Bool { get }    // Pattern_White_Space
      public var isCased: Bool { get }    // Cased
      public var isCaseIgnorable: Bool { get }    // Case_Ignorable
      public var changesWhenLowercased: Bool { get }    // Changes_When_Lowercased
      public var changesWhenUppercased: Bool { get }    // Changes_When_Uppercased
      public var changesWhenTitlecased: Bool { get }    // Changes_When_Titlecased
      public var changesWhenCaseFolded: Bool { get }    // Changes_When_Casefolded
      public var changesWhenCaseMapped: Bool { get }    // Changes_When_Casemapped
      public var changesWhenNFKCCaseFolded: Bool { get }    // Changes_When_NFKC_Casefolded
      public var isEmoji: Bool { get }    // Emoji
      public var isEmojiPresentation: Bool { get }    // Emoji_Presentation
      public var isEmojiModifier: Bool { get }    // Emoji_Modifier
      public var isEmojiModifierBase: Bool { get }    // Emoji_Modifier_Base
    }
    extension Unicode.Scalar.Properties {

      // Implemented in terms of ICU's `u_isdefined`.
      public var isDefined: Bool { get }
    }
    Case Mappings
    The properties below provide full case mappings for scalars. Since a handful of mappings result in multiple scalars (e.g., "ß" uppercases to "SS"), these properties are String-valued, not Unicode.Scalar.
    extension Unicode.Scalar.Properties {

      public var lowercaseMapping: String { get }  // u_strToLower
      public var titlecaseMapping: String { get }  // u_strToTitle
      public var uppercaseMapping: String { get }  // u_strToUpper
    }
Identification and Classification
    extension Unicode.Scalar.Properties {

      /// Corresponds to the `Age` Unicode property, when a code point was first
      /// defined.
      public var age: Unicode.Version? { get }

      /// Corresponds to the `Name` Unicode property.
      public var name: String? { get }

      /// Corresponds to the `Name_Alias` Unicode property.
      public var nameAlias: String? { get }

      /// Corresponds to the `General_Category` Unicode property.
      public var generalCategory: Unicode.GeneralCategory { get }

      /// Corresponds to the `Canonical_Combining_Class` Unicode property.
      public var canonicalCombiningClass: Unicode.CanonicalCombiningClass { get }
    }

    extension Unicode {

      /// Represents the version of Unicode in which a scalar was introduced.
      public typealias Version = (major: Int, minor: Int)

      /// General categories returned by
      /// `Unicode.Scalar.Properties.generalCategory`. Listed along with their
      /// two-letter code.
      public enum GeneralCategory {
        case uppercaseLetter  // Lu
        case lowercaseLetter  // Ll
        case titlecaseLetter  // Lt
        case modifierLetter  // Lm
        case otherLetter  // Lo

        case nonspacingMark  // Mn
        case spacingMark  // Mc
        case enclosingMark  // Me

        case decimalNumber  // Nd
        case letterlikeNumber  // Nl
        case otherNumber  // No

        case connectorPunctuation  //Pc
        case dashPunctuation  // Pd
        case openPunctuation  // Ps
        case closePunctuation  // Pe
        case initialPunctuation  // Pi
        case finalPunctuation  // Pf
        case otherPunctuation  // Po

        case mathSymbol  // Sm
        case currencySymbol  // Sc
        case modifierSymbol  // Sk
        case otherSymbol  // So

        case spaceSeparator  // Zs
        case lineSeparator  // Zl
        case paragraphSeparator  // Zp

        case control  // Cc
        case format  // Cf
        case surrogate  // Cs
        case privateUse  // Co
        case unassigned  // Cn
      }

      public struct CanonicalCombiningClass:
        Comparable, Hashable, RawRepresentable
      {
        public static let notReordered = CanonicalCombiningClass(rawValue: 0)
        public static let overlay = CanonicalCombiningClass(rawValue: 1)
        public static let nukta = CanonicalCombiningClass(rawValue: 7)
        public static let kanaVoicing = CanonicalCombiningClass(rawValue: 8)
        public static let virama = CanonicalCombiningClass(rawValue: 9)
        public static let attachedBelowLeft = CanonicalCombiningClass(rawValue: 200)
        public static let attachedBelow = CanonicalCombiningClass(rawValue: 202)
        public static let attachedAbove = CanonicalCombiningClass(rawValue: 214)
        public static let attachedAboveRight = CanonicalCombiningClass(rawValue: 216)
        public static let belowLeft = CanonicalCombiningClass(rawValue: 218)
        public static let below = CanonicalCombiningClass(rawValue: 220)
        public static let belowRight = CanonicalCombiningClass(rawValue: 222)
        public static let left = CanonicalCombiningClass(rawValue: 224)
        public static let right = CanonicalCombiningClass(rawValue: 226)
        public static let aboveLeft = CanonicalCombiningClass(rawValue: 228)
        public static let above = CanonicalCombiningClass(rawValue: 230)
        public static let aboveRight = CanonicalCombiningClass(rawValue: 232)
        public static let doubleBelow = CanonicalCombiningClass(rawValue: 233)
        public static let doubleAbove = CanonicalCombiningClass(rawValue: 234)
        public static let iotaSubscript = CanonicalCombiningClass(rawValue: 240)

        public let rawValue: UInt8

        public init(rawValue: UInt8)
      }
    }
    Numerics
    Many Unicode scalars have associated numeric values.
    These are not only the common digits zero through nine, but also vulgar fractions
    and various other linguistic characters and ideographs that have an innate numeric value.
    These properties are exposed below. They can be useful for determining whether segments
    of text contain numbers or non-numeric data, and can also help in the design of algorithms
    to determine the values of such numbers.
    extension Unicode.Scalar.Properties {

      /// Corresponds to the `Numeric_Type` Unicode property.
      public var numericType: Unicode.NumericType?

      /// Corresponds to the `Numeric_Value` Unicode property.
      public var numericValue: Double?
    }

    extension Unicode {

      public enum NumericType {
        case decimal
        case digit
        case numeric
      }
    }


14/06/2021
https://lists.isocpp.org/sg16/2018/08/0113.php
Feedback from swift team

    https://lists.isocpp.org/sg16/2018/08/0121.php
    Swift strings now sort with NFC (currently UTF-16 code unit order, but likely changed to Unicode scalar value order).
    We didn't find FCC significantly more compelling in practice. Since NFC is far more frequent in the wild
    (why waste space if you don't have to), strings are likely to already be in NFC.
    We have fast-paths to detect on-the-fly normal sections of strings (e.g. all ASCII, all < U+0300, NFC_QC=yes, etc.).
    We lazily normalize portions of string during comparison when needed.
    Q: Swift strings support comparison via normalization. Has use of canonical string equality been a performance issue?
       Or been a source of surprise to programmers?
    A: This was a big performance issue on Linux, where we used to do UCA+DUCET based comparisons.
       We switch to lexicographical order of NFC-normalized UTF-16 code units (future: scalar values),
       and saw a very significant speed up there. The remaining performance work revolves around checking
       and tracking whether a string is known to already be in a normal form, so we can just memcmp.
    Q: I'm curious why this was a larger performance issue for Linux than for (presumably) macOS and/or iOS.
    A: There were two main factors.
       The first is that on Darwin platforms, CFString had an implementation that we used instead of UCA+DUCET which was faster.
       The second is that Darwin platforms are typically up-to-date and have very recent versions of ICU.
       On Linux, we still support Ubuntu LTS 14.04 which has a version of ICU which predates Swift and didn't have any fast-paths for ASCII or mostly-ASCII text.
       Switching to our own implementation based on NFC gave us many X improvement over CFString, which in turn was many X faster than UCA+DUCET (especially on older versions of ICU).
    Q: How firmly is the Swift string implementation tied to ICU?
       If the C++ standard library were to add suitable Unicode support, what would motivate reimplementing Swift strings on top of it?
    A: Swift's tie to ICU is less firm than it used to be
       If the C++ standard library provided these operations, sufficiently up-to-date with Unicode version and comparable or better to ICU in performance,
       we would be willing to switch. A big pain in interacting with ICU is their limited support for UTF-8.
       Some users who would like to use a lighter-weight Swift and are unhappy at having to link against ICU, as it's fairly large, and it can complicate security audits.


https://forums.swift.org/t/pitch-unicode-for-string-processing/56907/6
[Pitch] Unicode for String Processing


https://github.com/apple/swift-evolution/blob/main/proposals/0350-regex-type-overview.md
jlf: surprising intro!
Swift strings provide an obsessively Unicode-forward model of programming with strings.
String processing with Collection's algorithms is woefully inadequate for many day-to-day
tasks compared to other popular programming and scripting languages.
We propose addressing this basic shortcoming through an effort we are calling regex.


https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/ProposalOverview.md
Regex Proposals
todo: read String processing algorithms https://forums.swift.org/t/pitch-regex-powered-string-processing-algorithms/55969
todo: read Unicode for String Processing https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/UnicodeForStringProcessing.md


================================================================================
Zig lang, Ziglyph
================================================================================

04/07/2021
https://github.com/jecolon/ziglyph
Unicode text processing for the Zig programming language.

https://devlog.hexops.com/2021/unicode-data-file-compression/
achieving 40-70% reduction over gzip alone

https://github.com/jecolon/ziglyph/issues/3
More size-optimal grapheme cluster sorting
