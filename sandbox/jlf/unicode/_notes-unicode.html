<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN"
                      "http://www.w3.org/TR/html4/loose.dtd">
<html>
<head>
    <title>_notes-unicode.txt</title>
</head>
<body>
<pre>
<hr><h1 id="Accumulation_of_URLs_about_Unicode">Accumulation of URLs about Unicode</h1><hr>
Contents:
    <a href="#Unicode_standard">Unicode standard</a>
    <a href="#Unicode_general_informations">Unicode general informations</a>
    <a href="#U__notation__Unicode_escape_sequence">U+ notation, Unicode escape sequence</a>
    <a href="#Security_title">Security title</a>
    <a href="#Segmentation__Grapheme">Segmentation, Grapheme</a>
    <a href="#Normalization__equivalence">Normalization, equivalence</a>
    <a href="#Character_set">Character set</a>
    <a href="#String_matching___Lower_vs_Casefold">String matching - Lower vs Casefold</a>
    <a href="#String_matching___Collation">String matching - Collation</a>
    <a href="#Locale">Locale</a>
    <a href="#CLDR_Common_Locale_Data_Repository">CLDR Common Locale Data Repository</a>
    <a href="#Case_mappings">Case mappings</a>
    <a href="#Collation__sorting">Collation, sorting</a>
    <a href="#BIDI_title">BIDI title</a>
    <a href="#Emoji">Emoji</a>
    <a href="#Countries__flags">Countries, flags</a>
    <a href="#Evidence_of_partial_or_wrong_support_of_Unicode">Evidence of partial or wrong support of Unicode</a>
    <a href="#Optimization__SIMD">Optimization, SIMD</a>
    <a href="#Variation_sequence">Variation sequence</a>
    <a href="#Whitespaces__separators">Whitespaces, separators</a>
    <a href="#Hyphenation">Hyphenation</a>
    <a href="#DNS_title__Domain_Name_title__Domain_Name_System_title">DNS title, Domain Name title, Domain Name System title</a>
    <a href="#All_languages">All languages</a>
    <a href="#Classical_languages">Classical languages</a>
    <a href="#Arabic_language">Arabic language</a>
    <a href="#Indic_languages">Indic languages</a>
    <a href="#CJK">CJK</a>
    <a href="#Korean">Korean</a>
    <a href="#Japanese">Japanese</a>
    <a href="#Polish">Polish</a>
    <a href="#IME___Input_Method_Editor">IME - Input Method Editor</a>
    <a href="#Text_editing">Text editing</a>
    <a href="#Text_rendering__Text_shaping_library">Text rendering, Text shaping library</a>
    <a href="#String_Matching">String Matching</a>
    <a href="#Fuzzy_String_Matching">Fuzzy String Matching</a>
    <a href="#Levenshtein_distance_and_string_similarity">Levenshtein distance and string similarity</a>
    <a href="#String_comparison">String comparison</a>
    <a href="#JSON">JSON</a>
    <a href="#TOML_serialization_format">TOML serialization format</a>
    <a href="#CBOR_Concise_Binary_Representation">CBOR Concise Binary Representation</a>
    <a href="#Binary_encoding_in_Unicode">Binary encoding in Unicode</a>
    <a href="#Invalid_format">Invalid format</a>
    <a href="#Mojibake">Mojibake</a>
    <a href="#Filenames">Filenames</a>
    <a href="#WTF8">WTF8</a>
    <a href="#Codepoint_grapheme_indexation">Codepoint/grapheme indexation</a>
    <a href="#Rope">Rope</a>
    <a href="#Encoding_title">Encoding title</a>
    <a href="#ICU_title">ICU title</a>
    <a href="#ICU_demos">ICU demos</a>
    <a href="#ICU_bindings">ICU bindings</a>
    <a href="#ICU4X_title">ICU4X title</a>
    <a href="#utf8proc_title">utf8proc title</a>
    <a href="#Twitter_text_parsing">Twitter text parsing</a>
    <a href="#terminal___console___cmd">terminal / console / cmd</a>
    <a href="#QT_Title">QT Title</a>
    <a href="#IBM_OS">IBM OS</a>
    <a href="#IBM_RPG_Lang">IBM RPG Lang</a>
    <a href="#IBM_z_OS">IBM z/OS</a>
    <a href="#macOS_OS">macOS OS</a>
    <a href="#Windows_OS">Windows OS</a>
    <a href="#Language_comparison">Language comparison</a>
    <a href="#Regular_expressions">Regular expressions</a>
    <a href="#Test_cases__test_cases__tests_files">Test cases, test-cases, tests files</a>
    <a href="#font_bold__italic__strikethrough__underline__backwards__upside_down">font bold, italic, strikethrough, underline, backwards, upside down</a>
    <a href="#youtube">youtube</a>
    <a href="#xxx_lang">xxx lang</a>
    <a href="#Ada_lang">Ada lang</a>
    <a href="#Awk_lang">Awk lang</a>
    <a href="#C___lang__cpp_lang__Boost">C++ lang, cpp lang, Boost</a>
    <a href="#cRexx_lang">cRexx lang</a>
    <a href="#DotNet__CoreFx">DotNet, CoreFx</a>
    <a href="#Dafny_lang">Dafny lang</a>
    <a href="#Dart_lang">Dart lang</a>
    <a href="#Elixir_lang">Elixir lang</a>
    <a href="#Factor_lang">Factor lang</a>
    <a href="#Fortran_lang">Fortran lang</a>
    <a href="#GO_lang">GO lang</a>
    <a href="#jRuby_lang">jRuby lang</a>
    <a href="#Java_lang">Java lang</a>
    <a href="#JavaScript_lang">JavaScript lang</a>
    <a href="#Julia_lang">Julia lang</a>
    <a href="#Kotlin_lang">Kotlin lang</a>
    <a href="#Lisp_lang">Lisp lang</a>
    <a href="#Mathematica_lang">Mathematica lang</a>
    <a href="#netrexx_lang">netrexx lang</a>
    <a href="#Oracle">Oracle</a>
    <a href="#Perl_lang__Perl_6_has_been_renamed_to_Raku_">Perl lang (Perl 6 has been renamed to Raku)</a>
    <a href="#PHP_lang">PHP lang</a>
    <a href="#Python_lang">Python lang</a>
    <a href="#R_lang">R lang</a>
    <a href="#RAKU_lang_Rakudo_lang__Perl6__Perl_6__MOAR_VM_">RAKU lang Rakudo lang (Perl6, Perl 6, MOAR-VM)</a>
    <a href="#Rexx_lang">Rexx lang</a>
    <a href="#Ruby_lang">Ruby lang</a>
    <a href="#Rust_lang">Rust lang</a>
    <a href="#Saxon_lang">Saxon lang</a>
    <a href="#SQL_lang">SQL lang</a>
    <a href="#Swift_lang">Swift lang</a>
    <a href="#Typst_lang">Typst lang</a>
    <a href="#XPath_lang">XPath lang</a>
    <a href="#Zig_lang__Ziglyph">Zig lang, Ziglyph</a>
    <a href="#Knock__knock_">Knock, knock.</a>


<hr><h2 id="Unicode_standard">Unicode standard</h2><hr>
Remember
Don't know why, but the Unicode consortium has 2 different URLs:
    <a target="_blank" rel="noopener noreferrer" href="https://unicode.org/">https://unicode.org/</a>
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/">https://www.unicode.org/</a>
To avoid doubling URLs, I use the 2nd form.


<a target="_blank" rel="noopener noreferrer" href="https://home.unicode.org/">https://home.unicode.org/</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/">https://www.unicode.org/</a>                                        (same as home.unicode.org)
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/versions/">https://www.unicode.org/versions/</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/versions/latest/">https://www.unicode.org/versions/latest/</a>                        (latest version)
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/versions/enumeratedversions.html">https://www.unicode.org/versions/enumeratedversions.html</a>        (current and previous versions)

<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/">https://www.unicode.org/Public/</a>                                 (datas for current and previous versions)


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/ucd/">https://www.unicode.org/ucd/</a>
UCD = Unicode Character Database


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/MAPPINGS">https://www.unicode.org/Public/MAPPINGS</a>                         (ISO8859)
    These tables are considered to be authoritative mappings
    between the Unicode Standard and different parts of
    the ISO/IEC 8859 standard.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/specifications.html">https://www.unicode.org/faq/specifications.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/">https://www.unicode.org/reports/</a>
Unicode¬Æ Technical Reports
    A Unicode Standard Annex (UAX) forms an integral part of the Unicode Standard,
    but is published as a separate document. The Unicode Standard may require
    conformance to normative content in a Unicode Standard Annex, if so specified
    in the Conformance chapter of that version of the Unicode Standard.

    A Unicode Technical Standard (UTS) is an independent specification.
    Conformance to the Unicode Standard does not imply conformance to any UTS.

    A Unicode Technical Report (UTR) contains informative material.
    Conformance to the Unicode Standard does not imply conformance to any UTR.
    Other specifications, however, are free to make normative references to a UTR.


Unicode Standard Annex (UAX)

    UAX #9, The Unicode Bidirectional Algorithm
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr9/">https://www.unicode.org/reports/tr9/</a>

    UAX #11, East Asian Width
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr11/">https://www.unicode.org/reports/tr11/</a>

    UAX #14, Unicode Line Breaking Algorithm
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr14/">https://www.unicode.org/reports/tr14/</a>

    UAX #15, Unicode Normalization Forms
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr15/">https://www.unicode.org/reports/tr15/</a>

    UAX #24, Unicode Script Property
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr24/">https://www.unicode.org/reports/tr24/</a>

    UAX #29, Unicode Text Segmentation
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/">https://www.unicode.org/reports/tr29/</a>

    UAX #31, Unicode Identifier and Pattern Syntax
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr31/">https://www.unicode.org/reports/tr31/</a>

    UAX #34, Unicode Named Character Sequences
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr34/">https://www.unicode.org/reports/tr34/</a>

    UAX #38, Unicode Han Database (Unihan)
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr38/">https://www.unicode.org/reports/tr38/</a>

    UAX #41, Common References for Unicode Standard Annexes
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr41/">https://www.unicode.org/reports/tr41/</a>

    UAX #42, Unicode Character Database in XML
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr42/">https://www.unicode.org/reports/tr42/</a>

    UAX #44, Unicode Character Database
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr44/">https://www.unicode.org/reports/tr44/</a>

    UAX #45, U-Source Ideographs
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr45/">https://www.unicode.org/reports/tr45/</a>

    UAX #50, Unicode Vertical Text Layout
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr50/">https://www.unicode.org/reports/tr50/</a>


Unicode Technical Standard  (UTS)

    UTS #22, UNICODE CHARACTER MAPPING MARKUP LANGUAGE (CharMapML)
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr22/">https://www.unicode.org/reports/tr22/</a>
    This document specifies an XML format for the interchange of mapping data
    for character encodings, and describes some of the issues connected with the
    use of character conversion.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/glossary">https://www.unicode.org/glossary</a>
Code Point.
    (1) Any value in the Unicode codespace; that is, the range of integers from 0 to 10FFFF.
        Not all code points are assigned to encoded characters. See code point type.
    (2) A value, or position, for a character, in any coded character set.
Code Unit.
    The minimal bit combination that can represent a unit of encoded text for processing or interchange.
    The Unicode Standard uses 8-bit code units in the UTF-8 encoding form,
    16-bit code units in the UTF-16 encoding form,
    and 32-bit code units in the UTF-32 encoding form.
Unicode Scalar Value.
    Any Unicode code point except high-surrogate and low-surrogate code points.
    In other words, the ranges of integers 0 to D7FF and E000 to 10FFFF inclusive.


UNICODE COLLATION ALGORITHM
Unicode has an official string collation algorithm called UCA
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/">https://www.unicode.org/reports/tr10/</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/#S2.1.1">https://www.unicode.org/reports/tr10/#S2.1.1</a>
The Unicode Collation Algorithm takes an input Unicode string and a Collation Element Table,
containing mapping data for characters. It produces a sort key, which is an array of
unsigned 16-bit integers. Two or more sort keys so produced can then be binary-compared
to give the correct comparison between the strings for which they were generated.


08/06/2021
Default Unicode Collation Element Table (DUCET)
For the latest version, see:
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/UCA/latest/allkeys.txt">https://www.unicode.org/Public/UCA/latest/allkeys.txt</a>
---
UTS10-D1. Collation Weight: A non-negative integer used in the UCA to establish
          a means for systematic comparison of constructed sort keys.
UTS10-D2. Collation Element: An ordered list of collation weights.
UTS10-D3. Collation Level: The position of a collation weight in a collation element.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr15/#Detecting_Normalization_Forms">https://www.unicode.org/reports/tr15/#Detecting_Normalization_Forms</a>
UNICODE NORMALIZATION FORMS


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr31/#Alternative_Identifier_Syntax">https://www.unicode.org/reports/tr31/#Alternative_Identifier_Syntax</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr31/">https://www.unicode.org/reports/tr31/</a>
UNICODE IDENTIFIER AND PATTERN SYNTAX
jlf: there is ONE (just ONE) occurence of NFKC_CF:
    Comparison and matching should be done after converting to NFKC_CF format.
    Thus #M√∂tleyCr√ºe should match #M√ñTLEYCR√úE and other variants.
---
In the UnicodeStandard PDF:
- The mapping NFKC_Casefold (short alias NFKC_CF) is specified in the data
  file DerivedNormalizationProps.txt in the Unicode Character Database.
- The derived binary property Changes_When_NFKC_Casefolded is also listed
  in the data file DerivedNormalizationProps.txt in the Unicode Character Database.
  Conformance 156 3.13 Default Case Algorithms
For more information on the use of NFKC_Casefold and caseless matching for identifiers,
see Unicode Standard Annex #31, ‚ÄúUnicode Identifier and Pattern Syntax


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr51/">https://www.unicode.org/reports/tr51/</a>
Unicode  emoji


23/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/notes/tn28/">https://www.unicode.org/notes/tn28/</a>
UNICODEMATH, A NEARLY PLAIN-TEXT ENCODING OF MATHEMATIC
    ùëéùëèùëê
    ùëë

    ùëé + ùëê
    ùëë

    (ùëé + ùëè)ùëõ = ‚àë (ùëõ ùëò) ùëéùëòùëèùëõ‚àíùëò


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/notes/tn5/">https://www.unicode.org/notes/tn5/</a>
Unicode Technical Note #5
CANONICAL EQUIVALENCE IN APPLICATIONS


<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org/design/normalizing-to-shortest-form">https://icu.unicode.org/design/normalizing-to-shortest-form</a>
Canonically Equivalent Shortest Form (CESF)
This is usually, but not always, the NFC form.


Conformance
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/conformance">https://github.com/unicode-org/conformance</a>
This repository provides tools and procedures for verifying that an
implementation is working correctly according to the data-based specifications.
The tests are implemented on several platforms including NodeJS (JavaScript),
ICU4X (RUST), ICU4C, etc.
Data Driven Test was initiated in 2022 at Google.
The first release of the package was delivered in October, 2022.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/main.html">https://www.unicode.org/main.html</a>
Unicode¬Æ Technical Site
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/">https://www.unicode.org/faq/</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/char_combmark.html">https://www.unicode.org/faq/char_combmark.html</a>
Characters and Combining Marks


<hr><h2 id="Unicode_general_informations">Unicode general informations</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://codepoints.net/">https://codepoints.net/</a>
Very detailled decription of each character
Source of the WEB site:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Codepoints/codepoints.net">https://github.com/Codepoints/codepoints.net</a>


<a target="_blank" rel="noopener noreferrer" href="https://util.unicode.org/UnicodeJsps/">https://util.unicode.org/UnicodeJsps/</a>
Lot of informations about a character.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/UTF-8">https://en.wikipedia.org/wiki/UTF-8</a>
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/UTF-16">https://en.wikipedia.org/wiki/UTF-16</a>
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/UTF-32">https://en.wikipedia.org/wiki/UTF-32</a>


<a target="_blank" rel="noopener noreferrer" href="http://xahlee.info/comp/unicode_index.html">http://xahlee.info/comp/unicode_index.html</a>

<a target="_blank" rel="noopener noreferrer" href="http://xahlee.info/comp/unicode_invert_text.html">http://xahlee.info/comp/unicode_invert_text.html</a>
Inverted text: : áx«ù á p«ù á…π«ù åuI

<a target="_blank" rel="noopener noreferrer" href="http://xahlee.info/comp/unicode_animals.html">http://xahlee.info/comp/unicode_animals.html</a>
T-REXX: ü¶ñ


<a target="_blank" rel="noopener noreferrer" href="https://www.fontspace.com/unicode/analyzer">https://www.fontspace.com/unicode/analyzer</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.compart.com/en/unicode/">https://www.compart.com/en/unicode/</a>


22/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://onlineunicodetools.com/">https://onlineunicodetools.com/</a>
Online Unicode tools is a collection of useful browser-based utilities for manipulating Unicode text.


28/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://unicode.scarfboy.com/">https://unicode.scarfboy.com/</a>
Search tool
Provides plenty of information about Unicode characters
but no encoding UTF16

<a target="_blank" rel="noopener noreferrer" href="https://unicode-table.com/en/">https://unicode-table.com/en/</a>                   search by name
Provides the encoding UTF16


<a target="_blank" rel="noopener noreferrer" href="https://www.minaret.info/test/menu.msp">https://www.minaret.info/test/menu.msp</a>
Minaret Unicode Tests
    Case Folding
    Character Type
    Collation
    Normalization
    Sorting
    Transliteration


<a target="_blank" rel="noopener noreferrer" href="https://www.gosecure.net/blog/2020/08/04/unicode-for-security-professionals/">https://www.gosecure.net/blog/2020/08/04/unicode-for-security-professionals/</a>
Unicode for Security Professionals
by Philippe Arteau | Aug 4, 2020
jlf : this article covers many of the Unicode characteristics


<a target="_blank" rel="noopener noreferrer" href="https://github.com/bits/UTF-8-Unicode-Test-Documents">https://github.com/bits/UTF-8-Unicode-Test-Documents</a>
Every Unicode character / codepoint in files and a file generator


<a target="_blank" rel="noopener noreferrer" href="http://www.ltg.ed.ac.uk/~richard/utf-8.html">http://www.ltg.ed.ac.uk/~richard/utf-8.html</a>
let convert utf8 to codepoint + symbolic name

<a target="_blank" rel="noopener noreferrer" href="https://blog.lunatech.com/posts/2009-02-03-what-every-web-developer-must-know-about-url-encoding">https://blog.lunatech.com/posts/2009-02-03-what-every-web-developer-must-know-about-url-encoding</a>

<a target="_blank" rel="noopener noreferrer" href="https://mothereff.in/utf-8">https://mothereff.in/utf-8</a>
UTF-8 encoder/decoder


<a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/">https://corp.unicode.org/pipermail/unicode/</a>
The Unicode Archives
January 2, 2014 - current

<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/mail-arch/unicode-ml/">https://www.unicode.org/mail-arch/unicode-ml/</a>
March 21, 2001 - April 2, 2020

<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/mail-arch/unicode-ml/Archives-Old/">https://www.unicode.org/mail-arch/unicode-ml/Archives-Old/</a>
October 11, 1994 - March 19, 2001

<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/search/">https://www.unicode.org/search/</a>
Search Unicode.org


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod/">https://www.w3.org/TR/charmod/</a>
Character Model for the World Wide Web 1.0: Fundamentals


<a target="_blank" rel="noopener noreferrer" href="https://www.johndcook.com/blog/2021/11/01/number-sets-html/">https://www.johndcook.com/blog/2021/11/01/number-sets-html/</a>
Number sets in HTML and Unicode
    ‚Ñï U+2115 &amp;Nopf; &amp;naturals;
    ‚Ñ§ U+2124 &amp;Zopf; &amp;integers;
    ‚Ñö U+211A &amp;Qopf; &amp;rationals;
    ‚Ñù U+211D &amp;Ropf; &amp;reals;
    ‚ÑÇ U+2102 &amp;Copf; &amp;complexes;
    ‚Ñç U+210D &amp;Hopf; &amp;quaternions;


<a target="_blank" rel="noopener noreferrer" href="https://gregtatum.com/writing/2021/encoding-text-utf-32-utf-16-unicode/">https://gregtatum.com/writing/2021/encoding-text-utf-32-utf-16-unicode/</a>
<a target="_blank" rel="noopener noreferrer" href="https://gregtatum.com/writing/2021/encoding-text-utf-8-unicode/">https://gregtatum.com/writing/2021/encoding-text-utf-8-unicode/</a>


<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/667669/">https://lwn.net/Articles/667669/</a>
Is the current Unicode design impractical?
jlf: this link is also in the section Raku Lang because it's about Perl6.
jlf: worth reading.


<a target="_blank" rel="noopener noreferrer" href="https://www.sciencedirect.com/science/article/pii/S1742287613000595">https://www.sciencedirect.com/science/article/pii/S1742287613000595</a>
Unicode search of dirty data.
This paper discusses problems arising in digital forensics with regard to Unicode,
character encodings, and search. It describes how multipattern search can handle
the different text encodings encountered in digital forensics and a number of issues
pertaining to proper handling of Unicode in search patterns. Finally, we demonstrate
the feasibility of the approach and discuss the integration of our developed search
engine, lightgrep, with the popular bulk_extractor tool.
---
There are UTF-16LE strings which contain completely different UTF-8 strings as prefixes.
For example the byte sequence which is ‚Äúnonsense‚Äù in UTF-8 is ÊΩÆÁçÆÊπ•Êï≥ in UTF-16LE (!)
    "nonsense"~c2x=                         -- '6E6F6E73656E7365'
    "nonsense"~text("utf16be")~c2x=         -- '6E6F 6E73 656E 7365'
    "nonsense"~text("utf16be")~c2u=         -- 'U+6E6F U+6E73 U+656E U+7365'
    "nonsense"~text("utf16be")~utf8=        -- T'ÊπØÊπ≥ÊïÆÁç•'      Le potage
    "nonsense"~text("utf16le")~c2x=         -- '6E6F 6E73 656E 7365'
    "nonsense"~text("utf16le")~c2u=         -- 'U+6F6E U+736E U+6E65 U+6573'
    "nonsense"~text("utf16le")~utf8=        -- T'ÊΩÆÁçÆÊπ•Êï≥'      mar√©e


<a target="_blank" rel="noopener noreferrer" href="https://github.com/simsong/bulk_extractor">https://github.com/simsong/bulk_extractor</a>


<a target="_blank" rel="noopener noreferrer" href="http://t-a-w.blogspot.com/2008/12/funny-characters-in-unicode.html">http://t-a-w.blogspot.com/2008/12/funny-characters-in-unicode.html</a>
SKULL AND CROSSBONES
SNOWMAN
POSTAL MARK FACE
APL FUNCTIONAL SYMBOL TILDE DIAERESIS
ARABIC LIGATURE UIGHUR KIRGHIZ YEH WITH HAMZA ABOVE WITH ALEF MAKSURA ISOLATED FORM
ARABIC LIGATURE SALLALLAHOU ALAYHE WASALLAM
THAI CHARACTER KHOMUT
GLAGOLITIC CAPITAL LETTER SPIDERY HA
VERY MUCH GREATER-THAN
NEITHER LESS-THAN NOR GREATER-THAN
HEAVY BLACK HEART
FLORAL HEART BULLET, REVERSED ROTATED
INTERROBANG
ùÑû (U+1D11E) MUSICAL SYMBOL G CLEF
ùï• (U+1D565) MATHEMATICAL DOUBLE-STRUCK SMALL T
ùü∂ (U+1D7F6) MATHEMATICAL MONOSPACE DIGIT ZERO
†Çä (U+2008A) Han Character


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/udhr/">https://www.unicode.org/udhr/</a>
UDHR in Unicode
The goal of the UDHR in Unicode project is to demonstrate the use of Unicode
for a wide variety of languages, using the Universal Declaration of Human Rights
(UDHR) as a representative text.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/jagracey/Awesome-Unicode">https://github.com/jagracey/Awesome-Unicode</a>
Awesome Unicode


<a target="_blank" rel="noopener noreferrer" href="https://cldr.unicode.org/index/charts">https://cldr.unicode.org/index/charts</a>
CLDR Charts
By-Type Chart: Numbers:Symbols
Question
    I am using the following code excerpt to format numbers:
        LocalizedNumberFormatter lnFmt = NumberFormatter.withLocale(Locale.US).unit(MeasureUnit.CELSIUS).unitWidth(NumberFormatter.UnitWidth.SHORT);
        System.out.println(lnFmt.format(-10).toString());
    In the resulting string, minus sign is represented as 0x2d (ASCII HYPHEN-MINUS).
    Shouldn't it be U+2212 (Unicode MINUS SIGN)?
Answer
    You can see the minus sign symbol being used for each locale here:
    <a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/cldr-staging/charts/latest/by_type/numbers.symbols.html#2f08b5ebf85e1e8b">https://unicode-org.github.io/cldr-staging/charts/latest/by_type/numbers.symbols.html#2f08b5ebf85e1e8b</a>
    U+2212 is used in: ¬∑fa¬∑ ¬∑ps¬∑ ¬∑uz_Arab¬∑ ¬∑eo¬∑ ¬∑et¬∑ ¬∑eu¬∑ ¬∑fi¬∑ ¬∑fo¬∑ ¬∑gsw¬∑ ¬∑hr¬∑ ¬∑kl¬∑ ¬∑ksh¬∑ ¬∑lt¬∑ ¬∑nn¬∑ ¬∑no¬∑ ¬∑rm¬∑ ¬∑se¬∑ ¬∑sl¬∑ ¬∑sv¬∑
Question
    Where this list of locales was taken from?
    I am particulary interested in ‚Äòru‚Äô: why U+2212 is not used for it?


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/10143836/why-is-there-no-utf-24">https://stackoverflow.com/questions/10143836/why-is-there-no-utf-24</a>
Why is there no UTF-24? [duplicate]
Well, the truth is : UTF-24 was suggested in 2007 :
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/mail-arch/unicode-ml/y2007-m01/0057.html">https://www.unicode.org/mail-arch/unicode-ml/y2007-m01/0057.html</a>

Possible Duplicate:
Why UTF-32 exists whereas only 21 bits are necessary to encode every character?
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/6339756/why-utf-32-exists-whereas-only-21-bits-are-necessary-to-encode-every-character">https://stackoverflow.com/questions/6339756/why-utf-32-exists-whereas-only-21-bits-are-necessary-to-encode-every-character</a>


<a target="_blank" rel="noopener noreferrer" href="https://unicodebook.readthedocs.io/">https://unicodebook.readthedocs.io/</a>
Book "Programming with Unicode"
2010-2011, Victor Stinner
jlf: only one occurrence of the word "grapheme".
Maybe at that time, it was not obvious that it would become an important concept.


<a target="_blank" rel="noopener noreferrer" href="https://mcilloni.ovh/2023/07/23/unicode-is-hard/">https://mcilloni.ovh/2023/07/23/unicode-is-hard/</a>
Unicode is harder than you think
23 Jul 2023
---
jlf: good overview, with some ICU samples.


<a target="_blank" rel="noopener noreferrer" href="https://www.kermitproject.org/utf8.html">https://www.kermitproject.org/utf8.html</a>
UTF-8 SAMPLER
Last update: Sun Mar 12 14:21:05 2023


<a target="_blank" rel="noopener noreferrer" href="http://www.inter-locale.com/whitepaper/learn/learn-to-test.html">http://www.inter-locale.com/whitepaper/learn/learn-to-test.html</a>
International Testing Basics
Testing non-English and non-ASCII (and/or Unicode) support in a product requires
tests and test plans that exercise the edge cases in the software.


<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=gd5uJ7Nlvvo">https://www.youtube.com/watch?v=gd5uJ7Nlvvo</a>
Plain Text - Dylan Beattie - NDC Copenhagen 2022
---
jlf: many comments say it's a good talk
did not watch
todo: watch


<a target="_blank" rel="noopener noreferrer" href="https://www.smashingmagazine.com/2012/06/all-about-unicode-utf8-character-sets/">https://www.smashingmagazine.com/2012/06/all-about-unicode-utf8-character-sets/</a>
Unicode, UTF8 &amp; Character Sets: The Ultimate Guide
jlf: maybe to read


<a target="_blank" rel="noopener noreferrer" href="https://tonsky.me/blog/unicode/">https://tonsky.me/blog/unicode/</a>
The Absolute Minimum Every Software Developer Must Know About Unicode in 2023 (Still No Excuses!)
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37735801">https://news.ycombinator.com/item?id=37735801</a>
What every software developer must know about Unicode in 2023
jlf: nothing new in this article, just reusing infos from other sites.
jlf: did not read all the comments


<hr><h2 id="U__notation__Unicode_escape_sequence">U+ notation, Unicode escape sequence</h2><hr>
29/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/1273693/why-is-u-used-to-designate-a-unicode-code-point/8891355">https://stackoverflow.com/questions/1273693/why-is-u-used-to-designate-a-unicode-code-point/8891355</a>
The Python language defines the following string literals:
    u'xyz' to indicate a Unicode string, a sequence of Unicode characters
    '\uxxxx' to indicate a string with a unicode character denoted by four hex digits
    '\Uxxxxxxxx' to indicate a string with a unicode character denoted by eight hex digits
    \N{name}    Character named name in the Unicode database
    \uxxxx      Character with 16-bit hex value xxxx. Exactly four hex digits are required.
    \Uxxxxxxxx  Character with 32-bit hex value xxxxxxxx. Exactly eight hex digits are required.


<a target="_blank" rel="noopener noreferrer" href="https://www.perl.com/article/json-unicode-and-perl-oh-my-/">https://www.perl.com/article/json-unicode-and-perl-oh-my-/</a>
Its \uXXXX escapes support only characters within Unicode‚Äôs BMP;
to store emoji or other non-BMP characters you either have to encode to UTF-8 directly.
or indicate a UTF-16 surrogate pair in \uXXXX escapes.


<a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/2021-April/009410.html">https://corp.unicode.org/pipermail/unicode/2021-April/009410.html</a>
Need reference to good ABNF for \uXXXX syntax


<a target="_blank" rel="noopener noreferrer" href="https://bit.ly/UnicodeEscapeSequences">https://bit.ly/UnicodeEscapeSequences</a>
Unicode Escape Sequences Across Various Languages and Platforms


<hr><h2 id="Security_title">Security title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr39">https://www.unicode.org/reports/tr39</a>
UNICODE SECURITY MECHANISMS
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/security/latest/confusables.txt">https://www.unicode.org/Public/security/latest/confusables.txt</a>


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Homoglyph">https://en.wikipedia.org/wiki/Homoglyph</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.trojansource.codes/">https://www.trojansource.codes/</a>


<a target="_blank" rel="noopener noreferrer" href="https://api.mtr.pub/vhf/confusable_homoglyphs">https://api.mtr.pub/vhf/confusable_homoglyphs</a>


<a target="_blank" rel="noopener noreferrer" href="https://util.unicode.org/UnicodeJsps/confusables.jsp">https://util.unicode.org/UnicodeJsps/confusables.jsp</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod-norm/#normalizationLimitations">https://www.w3.org/TR/charmod-norm/#normalizationLimitations</a>
Confusable characters:
"Œ°–†P"~text~characters==
    an Array (shape [3], 3 items)
     1 : ( "Œ°"   U+03A1 Lu 1 "GREEK CAPITAL LETTER RHO" )
     2 : ( "–†"   U+0420 Lu 1 "CYRILLIC CAPITAL LETTER ER" )
     3 : ( "P"   U+0050 Lu 1 "LATIN CAPITAL LETTER P" )
These confusable characters are not impacted by the lump option:
"Œ°–†P"~text~nfc(lump:)~characters    -- same result


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr36/#visual_spoofing">https://www.unicode.org/reports/tr36/#visual_spoofing</a>
UNICODE SECURITY CONSIDERATIONS


<a target="_blank" rel="noopener noreferrer" href="http://www.unicode.org/reports/tr55/">http://www.unicode.org/reports/tr55/</a>
Draft Unicode¬Æ Technical Standard #55
UNICODE SOURCE CODE HANDLING
---
While the normative material for computer language specifications is part of the
Unicode Standard, in Unicode Standard Annex #31, Unicode Identifiers and Syntax
[UAX31], the algorithms specific to the display of source code or to higher-level
diagnostics are specified in this document.
Note: While, for the sake of brevity, many of the examples in this document make
use of non-ASCII identifiers, most of the issues described here apply even if
non-ASCII characters are confined to strings and comments.
---
3.1.1 Normalization and Case
Case-insensitive languages should meet requirement UAX31-R4 with normalization
form KC, and requirement UAX31-R5 with full case folding. They should ignore
default ignorable code points in comparison. Conformance with these requirements
and ignoring of default ignorable code points may be achieved by comparing
identifiers after applying the transformation toNFKC_Casefold.
Note: Full case folding is preferable to simple case folding, as it better
matches expectations of case-insensitive equivalence.
The choice between Normalization Form C and Normalization Form KC should match
expectations of identifier equivalence for the language.
In a case-sensitive language, identifiers are the same if and only if they look
the same, so Normalization Form C (canonical equivalence) is appropriate, as
canonical equivalent sequences should display the same way.
In a case-insensitive language, the equivalence relation between identifiers is
based on a more abstract sense of character identity; for instance, e and E are
treated as the same letter. Normalization Form KC (compatibility equivalence) is
an equivalence between characters that share such an abstract identity.
Example: In a case-insensitive language, SO and so are the same identifier; if
that language uses Normalization Form KC, the identifiers so and ùñòùñî are likewise
identical.


Unicode 15.1
[icu-design] ICU 74 API proposal: bidiSkeleton and LTR- and RTL-confusabilities
The Source Code Working Group, a limited-duration working group under the
Properties &amp; Algorithms Group of the Unicode Technical Committee, has added a
new bidi-aware concept of confusability to UTS #39 in Unicode Version 15.1;
until publication see the proposed update,
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr39/tr39-27.html#Confusable_Detection.">https://www.unicode.org/reports/tr39/tr39-27.html#Confusable_Detection.</a>
The new UTS #55, Unicode Source Code Handling, to be published simultaneously
with Unicode Version 15.1, recommends the use of this new kind of confusability:
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr55/tr55-2.html#Confusable-Detection.">https://www.unicode.org/reports/tr55/tr55-2.html#Confusable-Detection.</a>


<a target="_blank" rel="noopener noreferrer" href="https://semanticdiff.com/blog/pull-request-unicode-tricks/">https://semanticdiff.com/blog/pull-request-unicode-tricks/</a>
Unicode tricks in pull requests: Do review tools warn us?


<hr><h2 id="Segmentation__Grapheme">Segmentation, Grapheme</h2><hr>
29/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/alvinlindstam/grapheme">https://github.com/alvinlindstam/grapheme</a>
<a target="_blank" rel="noopener noreferrer" href="https://pypi.org/project/grapheme/">https://pypi.org/project/grapheme/</a>
Here too, he says that CR+LF is a grapheme...

Same here:
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/programming/comments/m274cg/til_rn_crlf_is_a_single_grapheme_cluster/">https://www.reddit.com/r/programming/comments/m274cg/til_rn_crlf_is_a_single_grapheme_cluster/</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/#Table_Combining_Char_Sequences_and_Grapheme_Clusters">https://www.unicode.org/reports/tr29/#Table_Combining_Char_Sequences_and_Grapheme_Clusters</a>


01/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://halt.software/optimizing-unicodes-grapheme-cluster-break-algorithm/">https://halt.software/optimizing-unicodes-grapheme-cluster-break-algorithm/</a>
They claim this improvement:
For the simple data set, this was 0.38 of utf8proc time.
For the complex data set, this was 0.56 of utf8proc time.


01/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/unicode-segmentation/1.7.1/unicode_segmentation/">https://docs.rs/unicode-segmentation/1.7.1/unicode_segmentation/</a>
GraphemeCursor	Cursor-based segmenter for grapheme clusters.
GraphemeIndices	External iterator for grapheme clusters and byte offsets.
Graphemes	External iterator for a string's grapheme clusters.
USentenceBoundIndices	External iterator for sentence boundaries and byte offsets.
USentenceBounds	External iterator for a string's sentence boundaries.
UWordBoundIndices	External iterator for word boundaries and byte offsets.
UWordBounds	External iterator for a string's word boundaries.
UnicodeSentences	An iterator over the substrings of a string which, after splitting
    the string on sentence boundaries, contain any characters with the Alphabetic
    property, or with General_Category=Number.
UnicodeWords	An iterator over the substrings of a string which, after splitting
    the string on word boundaries, contain any characters with the Alphabetic
    property, or with General_Category=Number.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/knighton/unicode">https://github.com/knighton/unicode</a>
Minimalist Unicode normalization/segmentation library. Python and C++.
Abandonned, last commit 21/05/2015


<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/string-length/">https://hsivonen.fi/string-length/</a>
First published: 2019-09-08
It‚Äôs Not Wrong that "ü§¶üèº‚Äç‚ôÇÔ∏è".length == 7
But It‚Äôs Better that "ü§¶üèº‚Äç‚ôÇÔ∏è".len() == 17 and Rather Useless that len("ü§¶üèº‚Äç‚ôÇÔ∏è") == 5
But I Want the Length to Be 1!
jlf:
    "ü§¶üèº‚Äç‚ôÇÔ∏è"~text~length=  -- 1
    "ü§¶üèº‚Äç‚ôÇÔ∏è"~text~characters==
    an Array (shape [5], 5 items)
     1 : ( "ü§¶"  U+1F926 So 2 "FACE PALM" )
     2 : ( "üèº"  U+1F3FC Sk 2 "EMOJI MODIFIER FITZPATRICK TYPE-3" )
     3 : ( "‚Äç"    U+200D Cf 0 "ZERO WIDTH JOINER", "ZWJ" )
     4 : ( "‚ôÇ"   U+2642 So 1 "MALE SIGN" )
     5 : ( "Ô∏è"    U+FE0F Mn 0 "VARIATION SELECTOR-16", "VS16" )


07/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=20914184">https://news.ycombinator.com/item?id=20914184</a>
String lengths in Unicode
    Claude Roux
    We went through a lot of pain to get this right in Tamgu ( <a target="_blank" rel="noopener noreferrer" href="https://github.com/naver/tamgu">https://github.com/naver/tamgu</a> ).
    In particular, emojis can be encoded across 5 or 6 Unicode characters.
    A "black thumb up" is encoded with 2 Unicode characters: the thumb glyph and its color.
    This comes at a cost. Every time you extract a sub-string from a string,
    you have to scan it first for its codepoints, then convert character positions
    into byte positions. One way to speed up stuff a bit, is to check if the string
    is in ASCII (see <a target="_blank" rel="noopener noreferrer" href="https://lemire.me/blog/2018/05/16/validating-utf-8-strings-u">https://lemire.me/blog/2018/05/16/validating-utf-8-strings-u</a> )
    and apply regular operator then.
    We implemented many techniques based on "intrinsics" instructions to speed up
    conversions and search in order to avoid scanning for codepoints.
    See <a target="_blank" rel="noopener noreferrer" href="https://github.com/naver/tamgu/blob/master/src/conversion.cxx">https://github.com/naver/tamgu/blob/master/src/conversion.cxx</a> for more information.
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/naver/tamgu/wiki/4.-Speed-up-UTF8-string-processing-with-Intel's-%22intrinsics%22-instructions-(en)">https://github.com/naver/tamgu/wiki/4.-Speed-up-UTF8-string-processing-with-Intel's-%22intrinsics%22-instructions-(en)</a>
jlf: they have specific support for Korean... Probably because the NAVER company is from Republic of Korea ?
08/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://twitter.com/hashtag/tamgu?src=hashtag_click">https://twitter.com/hashtag/tamgu?src=hashtag_click</a>
<a target="_blank" rel="noopener noreferrer" href="https://twitter.com/hashtag/TAL?src=hashtag_click">https://twitter.com/hashtag/TAL?src=hashtag_click</a>
#tamgu le #langage_de_programmation pour le Traitement Automatique des Langues (#TAL).


jlf 30/09/2021
I have a doubt about that:
Is üë©‚Äçüë®‚Äçüë©‚Äçüëß' really a grapheme?
When moving the cursor in BBEdit, I see a boundary between each character.
[later]
Ok, when moving the cursor in Visual Studio Code, it's really a unique grapheme, no way to put the cursor "inside".
And the display is aligned with what I see in Google Chrome :
one WOMAN followed by a family, and no way to put the cursor between the WOMAN and the family.
---
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/review/pr-27.html">https://www.unicode.org/review/pr-27.html</a>       (old, talk about Unicode 4)
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries">https://www.unicode.org/reports/tr29/#Grapheme_Cluster_Boundaries</a>   (todo: review occurences of ZWJ)


29/10/2021
<a target="_blank" rel="noopener noreferrer" href="https://h3manth.com/posts/unicode-segmentation-in-javascript/">https://h3manth.com/posts/unicode-segmentation-in-javascript/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/tc39/proposal-intl-segmenter">https://github.com/tc39/proposal-intl-segmenter</a>


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=21690326">https://news.ycombinator.com/item?id=21690326</a>
Tailored grapheme clusters
Grapheme clusters are locale-dependent, much like string collation is locale-dependent.
What Unicode gives you by default, the (extended) grapheme cluster, is as useful as
the DUCET (Default Unicode Collation Element Table); while you can live with them,
you would be unsatisfied. In fact there are tons of Unicode bugs that can't be corrected
due to the compatibility reason, and can only be fixed via tailored locale-dependent schemes.
---
Hangul normalization and collation is broken in Unicode, albeit for slightly different reasons.
The Unicode Collation Algorithm explictly devotes two sections related to Hangul; the first section, for "trailing weights" [1], is recommended for the detailed explanation.
The Unicode Text Segmentation standard [2] explicitly mentions that Indic aksaras [3]
require the tailoring to grapheme clusters. Depending on the view, you can also consider
orthographic digraphs as examples (Dutch "ij" is sometimes considered a single character for example).


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/27331819/whats-the-difference-between-a-character-a-code-point-a-glyph-and-a-grapheme">https://stackoverflow.com/questions/27331819/whats-the-difference-between-a-character-a-code-point-a-glyph-and-a-grapheme</a>
What's the difference between a character, a code point, a glyph and a grapheme?
jlf: not very good...


<a target="_blank" rel="noopener noreferrer" href="https://github.com/clipperhouse/words">https://github.com/clipperhouse/words</a>
words is a command which splits strings into individual words, as defined by Unicode.
It accepts text from stdin, and writes one word (token) per line to stdout.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/#Random_Access">https://www.unicode.org/reports/tr29/#Random_Access</a>
    jlf: Executor uses indexers for random access (ako breadcrumbs).

    Random access introduces a further complication. When iterating through a string
    from beginning to end, a regular expression or state machine works well. From
    each boundary to find the next boundary is very fast. By constructing a state
    table for the reverse direction from the same specification of the rules,
    reverse iteration is possible.

    However, suppose that the user wants to iterate starting at a random point in
    the text, or detect whether a random point in the text is a boundary. If the
    starting point does not provide enough context to allow the correct set of rules
    to be applied, then one could fail to find a valid boundary point. For example,
    suppose a user clicked after the first space after the question mark in
    ‚ÄúAre‚ê£you‚ê£there?‚ê£ ‚ê£No,‚ê£I‚Äôm‚ê£not‚Äù. On a forward iteration searching for a sentence
    boundary, one would fail to find the boundary before the ‚ÄúN‚Äù, because the ‚Äú?‚Äù
    had not been seen yet.

    A second set of rules to determine a ‚Äúsafe‚Äù starting point provides a solution.
    Iterate backward with this second set of rules until a safe starting point is
    located, then iterate forward from there. Iterate forward to find boundaries
    that were located between the safe point and the starting point; discard these.
    The desired boundary is the first one that is not less than the starting point.
    The safe rules must be designed so that they function correctly no matter what
    the starting point is, so they have to be conservative in terms of finding
    boundaries, and only find those boundaries that can be determined by a small
    context (a few neighboring characters).

    This process would represent a significant performance cost if it had to be
    performed on every search. However, this functionality can be wrapped up in an
    iterator object, which preserves the information regarding whether it currently
    is at a valid boundary point. Only if it is reset to an arbitrary location in
    the text is this extra backup processing performed. The iterator may even cache
    local values that it has already traversed.


Unicode 15.1
New rule GB9c for grapheme segmentation.
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/">https://www.unicode.org/reports/tr29/</a>
---
No longer available:
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr29/proposed.html">https://www.unicode.org/reports/tr29/proposed.html</a>
---
jlf: saw this review note
"the new rule GB9c has been implemented in CLDR and ICU as a profile for some years"
What is a profile?
---
This specification defines default mechanisms; more sophisticated implementations
can and should tailor them for particular locales or environments and, for the
purpose of claiming conformance, document the tailoring in the form of a profile.
...
Note that a profile can both add and remove boundary positions, compared to the
results specified by UAX29-C1-1, UAX29-C2-1, or UAX29-C3-1.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/lstm_word_segmentation">https://github.com/unicode-org/lstm_word_segmentation</a>
Python code for training an LSTM model for word segmentation in Thai, Burmese,
and similar languages.


<hr><h2 id="Normalization__equivalence">Normalization, equivalence</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/normalization.html">https://www.unicode.org/faq/normalization.html</a>
Normalization FAQ


<a target="_blank" rel="noopener noreferrer" href="https://www.macchiato.com/unicode-intl-sw/nfc-faq">https://www.macchiato.com/unicode-intl-sw/nfc-faq</a>
NFC FAQ
jlf: MUST READ!


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr15">https://www.unicode.org/reports/tr15</a>
UNICODE NORMALIZATION FORMS


26/11/2013
Text normalization in Go
<a target="_blank" rel="noopener noreferrer" href="https://blog.golang.org/normalization">https://blog.golang.org/normalization</a>


27/11/2013
The string type is broken
<a target="_blank" rel="noopener noreferrer" href="https://mortoray.com/2013/11/27/the-string-type-is-broken/">https://mortoray.com/2013/11/27/the-string-type-is-broken/</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=6807524">https://news.ycombinator.com/item?id=6807524</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/programming/comments/1rkdip/the_string_type_is_broken/">https://www.reddit.com/r/programming/comments/1rkdip/the_string_type_is_broken/</a>
In the comments
Objective-C‚Äôs NSString type does correctly upper-case baÔ¨Ñe into BAFFLE.
(where the rectangle is a grapheme showing 2 small 'f')
Q: What about getting the first three characters of ‚ÄúbaÔ¨Ñe‚Äù? Is ‚Äúbaf‚Äù the correct answer?
A:  That‚Äôs a good question. I suspect ‚Äúbaf‚Äù is the correct answer, and I wonder if there is any library that does it.
    I suspect if you normalize it first (since the ffl would disappear I think).
A:  The ligarture disappears in NFK[CD] but not in NF[CD].
    Whether normalization to NFK[CD] is a good idea depends (as always) on the situation.
    For visual grapheme cluster counting, one would convert the entire text to NFKC.
    For getting teaser text from an article i would not a normalization step
    and let a ligature count as just one grapheme cluster even if it may resemble three of them logically.
    I assume, that articles are stored in NFC (the nondestructive normalization form with smallest memory footprint).
    The Unicode standard does not treat ligatures as containing more than one grapheme cluster for that normalization forms that permits them.
    So ‚ÄúeÔ¨Ñab‚Äù (jlf: efflab) is the correct result of reversing ‚ÄúbaÔ¨Ñe‚Äù (jlf: baffle)
    and ‚ÄúbaÔ¨Ñe‚Äù[2] has to return ‚ÄúÔ¨Ñ‚Äù even when working on the grapheme cluster level!

    There may or may not be a need for another grapheme cluster definition that permits splitting of ligatures in NF[CD].
    A straight forward way to implement a reverse function adhering to that special definition would NFKC each Unicode grapheme cluster on the fly.
    When that results in multiple Unicode grapheme clusters, that are used ‚Äì else the original is preserved (so that ‚Äú‚Ñï‚Äù does not become ‚ÄúN‚Äù).
    The real problem is to find a good name for that special interpretation of a grapheme cluster‚Ä¶
Note :
    see also the comment of Tom Christiansen about casing.
    I don't copy-paste here, too long.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/blackwinter/unicode">https://github.com/blackwinter/unicode</a>
Unicode normalization library. (Mirror of Yoshida-san's code base to maintain the RubyGem.)
Abandonned, last commit 07/07/2016


<a target="_blank" rel="noopener noreferrer" href="https://github.com/sjorek/unicode-normalization">https://github.com/sjorek/unicode-normalization</a>
An enhanced facade to existing unicode-normalization implementations
Last commit 25/03/2018


<a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/windows/win32/intl/using-unicode-normalization-to-represent-strings">https://docs.microsoft.com/en-us/windows/win32/intl/using-unicode-normalization-to-represent-strings</a>
Using Unicode Normalization to Represent Strings


<a target="_blank" rel="noopener noreferrer" href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/normalize">https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/String/normalize</a>
String.prototype.normalize()
The normalize() method returns the Unicode Normalization Form of the string.


<a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/string-case-folding-and-normalization-apis/14663/3">https://forums.swift.org/t/string-case-folding-and-normalization-apis/14663/3</a>
For the comments


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Unicode_equivalence">https://en.wikipedia.org/wiki/Unicode_equivalence</a>
Unicode equivalence is the specification by the Unicode character encoding standard that some sequences
of code points represent essentially the same character. This feature was introduced in the standard
to allow compatibility with preexisting standard character sets, which often included similar or identical characters.


On Wed, Oct 28, 2020 at 9:54 AM Mark Davis ‚òïÔ∏è &lt;mark@macchiato.com&gt; wrote:
Re: [icu-support] Options for Immutable Collation?

        I think your search for 'middle ground' is fruitless.
            An NFKD ordering is not correct for any human language, and changes with each new Unicode version.
            And even the default Unicode collation ordering is wrong for many languages, because there is no order that simultaneously satisfies all (eg German ordering and Swedish ordering are incompatible).
        Your 'middle ground' would be correct for nobody, and yet be unstable across Unicode versions; or worse yet, fail for new characters.

        IMO, the best practice for a file system (or like systems) is to store in codepoint order. When called upon to present a sorted list of files to a user, the displaying program should sort that list according to the user's language preferences.

    You are right: for a deterministic/reproducible list sorting for a cross-platform filesystem API, anything more complex would be an implementation hazard.

    However, after reviewing both developer discussions and implementation of Unicode handling in 6+ filesystems, IDNA200X, PRECIS and getting roped into work on an IETF i18n filesystem best-practices RFC ... I've got some thoughts.  Thoughts that I will put into a new thread after I do some experimenting : ).

    Thank you all so much!!!
    -Zach Lym


08/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://fr.wikipedia.org/wiki/Normalisation_Unicode">https://fr.wikipedia.org/wiki/Normalisation_Unicode</a>
NFD     Les caract√®res sont d√©compos√©s par √©quivalence canonique et r√©ordonn√©s
        canonical decomposition
NFC     Les caract√®res sont d√©compos√©s par √©quivalence canonique, r√©ordonn√©s, et compos√©s par √©quivalence canonique
        canonical decomposition followed by canonical composition
NFKD    Les caract√®res sont d√©compos√©s par √©quivalence canonique et de compatibilit√©, et sont r√©ordonn√©s
        compatibility decomposition
NFKC    Les caract√®res sont d√©compos√©s par √©quivalence canonique et de compatibilit√©, sont r√©ordonn√©s et sont compos√©s par √©quivalence canonique
        compatibility decomposition followed by canonical composition
FCD     "Fast C or D" form; cf. UTN #5
FCC     "Fast C Contiguous"; cf. UTN #5


09/06/2021
Rust
<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/unicode-normalization">https://docs.rs/unicode-normalization</a>
    Decompositions  External iterator for a string decomposition‚Äôs characters.
    Recompositions  External iterator for a string recomposition‚Äôs characters.
    Replacements    External iterator for replacements for a string‚Äôs characters.
    StreamSafe      UAX15-D4: This iterator keeps track of how many non-starters
                    there have been since the last starter in NFKD and will emit
                    a Combining Grapheme Joiner (U+034F) if the count exceeds 30.

    is_nfc                      Authoritatively check if a string is in NFC.
    is_nfc_quick                Quickly check if a string is in NFC, potentially returning IsNormalized::Maybe if further checks are necessary. In this case a check like s.chars().nfc().eq(s.chars()) should suffice.
    is_nfc_stream_safe          Authoritatively check if a string is Stream-Safe NFC.
    is_nfc_stream_safe_quick    Quickly check if a string is Stream-Safe NFC.
    is_nfd                      Authoritatively check if a string is in NFD.
    is_nfd_quick                Quickly check if a string is in NFD.
    is_nfd_stream_safe          Authoritatively check if a string is Stream-Safe NFD.
    is_nfd_stream_safe_quick    Quickly check if a string is Stream-Safe NFD.
    is_nfkc                     Authoritatively check if a string is in NFKC.
    is_nfkc_quick               Quickly check if a string is in NFKC.
    is_nfkd                     Authoritatively check if a string is in NFKD.
    is_nfkd_quick               Quickly check if a string is in NFKD.

    Enums
    IsNormalized	The QuickCheck algorithm can quickly determine if a text is
                    or isn‚Äôt normalized without any allocations in many cases,
                    but it has to be able to return Maybe when a full decomposition
                    and recomposition is necessary.


08/06/2021
Pharo
<a target="_blank" rel="noopener noreferrer" href="https://medium.com/concerning-pharo/an-implementation-of-unicode-normalization-7c6719068f43">https://medium.com/concerning-pharo/an-implementation-of-unicode-normalization-7c6719068f43</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/duerst/eprun">https://github.com/duerst/eprun</a>
Efficient Pure Ruby Unicode Normalization (eprun)
According to julia/utf8proc, the interesting part is the tests.


<a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/2020-December/009150.html">https://corp.unicode.org/pipermail/unicode/2020-December/009150.html</a>
Normalization Generics (NFx, NFKx, NFxy)


<a target="_blank" rel="noopener noreferrer" href="https://6guts.wordpress.com/2015/04/12/this-week-unicode-normalization-many-rts/">https://6guts.wordpress.com/2015/04/12/this-week-unicode-normalization-many-rts/</a>


<a target="_blank" rel="noopener noreferrer" href="https://gregtatum.com/writing/2021/diacritical-marks/">https://gregtatum.com/writing/2021/diacritical-marks/</a>
DIACRITICAL MARKS IN UNICODE


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=29751641">https://news.ycombinator.com/item?id=29751641</a>
Unicode Normalization Forms: When √∂ ‚â† √∂
<a target="_blank" rel="noopener noreferrer" href="https://blog.opencore.ch/posts/unicode-normalization-forms/">https://blog.opencore.ch/posts/unicode-normalization-forms/</a>


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/transforms/normalization/">https://unicode-org.github.io/icu/userguide/transforms/normalization/</a>
ICU Documentation
Normalization
Has a few comments about NFKC_Casefold
- NFKC_Casefold: ICU 4.4 supports the combination of NFKC, case folding and
  removing ignorable characters which was introduced with Unicode 5.2.
- Data Generation Tool


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/56995429/will-normalizing-a-string-give-the-same-result-as-normalizing-the-individual-gra">https://stackoverflow.com/questions/56995429/will-normalizing-a-string-give-the-same-result-as-normalizing-the-individual-gra</a>
Will normalizing a string give the same result as normalizing the individual grapheme clusters?
---
No, that generally is not true. The Unicode Standard warns against the assumption that concatenating
normalised strings produces another normalised string. From UAX #15:
    In using normalization functions, it is important to realize that none of the Normalization Forms
    are closed under string concatenation. That is, even if two strings X and Y are normalized,
    their string concatenation X+Y is not guaranteed to be normalized.


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/7171377/separating-unicode-ligature-characters">https://stackoverflow.com/questions/7171377/separating-unicode-ligature-characters</a>
NFKD is no panacea: there are plenty of ligatures and other notionally combined
forms it just does not work on at all. For example, it will not manage to decompose
√ü or ·∫û to SS (even those there is a casefold thither!), nor √Ü to AE or √¶ to ae,
nor ≈í to OE or ≈ì to oe. It is also useless for turning √∞ or ƒë into d or √∏ into o.
For all those things, you need the UCA (Unicode Collation Algorithm), not NFKD.
NFD/NFKD also both have the annoying property of destroying singletons, if this
matters to you.
---
my understanding is that those decompositions you mention should not be done.
They are not simply ligatures in the typographical sense, but real separate
characters that are used differently! √ü can be decomposed to ss if necessary
(for example if you can only store ASCII), but they are not equivalent. The ff
Ligature, on the other hand is only a typographical ligature.


<hr><h2 id="Character_set">Character set</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.gnu.org/software/libc/manual/html_mono/libc.html#Character-Set-Handling">https://www.gnu.org/software/libc/manual/html_mono/libc.html#Character-Set-Handling</a>


<hr><h2 id="String_matching___Lower_vs_Casefold">String matching - Lower vs Casefold</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/45745661/lower-vs-casefold-in-string-matching-and-converting-to-lowercase">https://stackoverflow.com/questions/45745661/lower-vs-casefold-in-string-matching-and-converting-to-lowercase</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod-norm/">https://www.w3.org/TR/charmod-norm/</a>
Character Model for the World Wide Web: String Matching
MUST READ, PLENTY OF EXAMPLES FOR CORNER CASES


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod-norm/#definitionCaseFolding">https://www.w3.org/TR/charmod-norm/#definitionCaseFolding</a>
Very good explanation!
    A few characters have a case folding that map one Unicode code point to two or more code points.
    This set of case foldings are called the full case foldings.

    character √ü U+00DF LATIN SMALL LETTER SHARP S
    - The full case folding and the lower case mapping of this character is to two ASCII letters 's'.
    - The upper case mapping is to "SS".
    Because some applications cannot allocate additional storage when performing a case fold operation,
    Unicode provides a simple case folding that maps a code point that would normally fold to more or
    fewer code points to use a single code point for comparison purposes instead.
    Unlike the full folding, this folding invariably alters the content (and potentially the meaning) of the text.
    Unicode simple is not appropriate for use on the Web.

    character ·æõ [U+1F9B GREEK CAPITAL LETTER ETA WITH DASIA AND VARIA AND PROSGEGRAMMENI]
    ·æõ	‚áí	·º£Œπ	full case fold: U+1F23 GREEK SMALL LETTER ETA WITH DASIA AND VARIA + U+03B9 GREEK SMALL LETTER IOTA
    ·æõ	‚áí	·æì	simple case fold: U+1F93 GREEK SMALL LETTER ETA WITH DASIA AND VARIA AND YPOGEGRAMMENI

    Language Sensitivity
    Another aspect of case mapping and case folding is that it can be language sensitive.
    Unicode defines default case mappings and case foldings for each encoded character,
    but these are only defaults and are not appropriate in all cases. Some languages need
    case mapping to be tailored to meet specific linguistic needs. One example of this are
    Turkic languages written in the Latin script:
        Default Folding
        I	‚áí	i	Default folding of letter I
        Turkic Language Folding
        I	‚áí	ƒ±	Turkic language folding of dotless (ASCII) letter I
        ƒ∞	‚áí	i	Turkic language folding of dotted letter I


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod-norm/#matchingAlgorithm">https://www.w3.org/TR/charmod-norm/#matchingAlgorithm</a>
There are four choices for text normalization:
- Default.
  This normalization step has no effect on the text and, as a result, is sensitive
  to form differences involving both case and Unicode normalization.
- ASCII Case Fold.
  Comparison of text with the characters case folded in the ASCII (Basic Latin, U+0000 to U+007F) range.
- Unicode Canonical Case Fold.
  Comparison of text that is both case folded and has Unicode canonical normalization applied.
- Unicode Compatibility Case Fold.
  Comparison of text that is both case folded and has Unicode compatibility normalization applied.
  This normalization step is presented for completeness, but it is not generally appropriate for use on the Web.


<a target="_blank" rel="noopener noreferrer" href="https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html">https://www.elastic.co/guide/en/elasticsearch/guide/current/languages.html</a>
Elasticsearch
Dealing with Human Language


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/319426/how-do-i-do-a-case-insensitive-string-comparison">https://stackoverflow.com/questions/319426/how-do-i-do-a-case-insensitive-string-comparison</a>
Related to Python, but the comments are very general and worth reading.
---
Unicode Standard section 3.13 has two other definitions for caseless comparisons:
(D146, canonical) NFD(toCasefold(NFD(str))) on both sides and
(D147, compatibility) NFKD(toCasefold(NFKD(toCasefold(NFD(X))))) on both sides.
It states the inner NFD is solely to handle a certain Greek accent character.


<a target="_blank" rel="noopener noreferrer" href="https://boyter.org/posts/unicode-support-what-does-that-actually-mean/">https://boyter.org/posts/unicode-support-what-does-that-actually-mean/</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=23524400">https://news.ycombinator.com/item?id=23524400</a>
≈øecret == secret == Secret
≈øatisfaction == satisfaction == ≈øati≈øfaction == Sati≈øfaction == SatiSfaction === ≈øatiSfaction
Another good example to consider is the character √Ü.
Under simple case folding rules the lower of √Ü is «£.
However with full case folding rules this also matches ae.
Which one is correct? Well that depends on who you ask.


See also
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3151">https://github.com/unicode-org/icu4x/issues/3151</a>
in the section "ICU4X title".


<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/784316/">https://lwn.net/Articles/784316/</a>
Working with UTF-8 in the kernel
jlf: interesting read about NTFS caseless, and about a drama because of lack of
support for the turkish case.


<hr><h2 id="String_matching___Collation">String matching - Collation</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/collation/string-search.html">https://unicode-org.github.io/icu/userguide/collation/string-search.html</a>
(ICU) String Search Service
jlf: they give 3 issues applicable to text searching. Accented letters and
conjoined letters are covered by Executor. But ignorable punctuation is not.


<hr><h2 id="Locale">Locale</h2><hr>
02/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://www.php.net/manual/fr/function.setlocale.php">https://www.php.net/manual/fr/function.setlocale.php</a>
Warning
The locale information is maintained per process, not per thread.
If you are running PHP on a multithreaded server API , you may experience sudden changes
in locale settings while a script is running, though the script itself never called setlocale().
This happens due to other scripts running in different threads of the same process at the same time,
changing the process-wide locale using setlocale().
On Windows, locale information is maintained per thread as of PHP 7.0.5.
On Windows, setlocale(LC_ALL, '') sets the locale names from the system's regional/language settings (accessible via Control Panel).


<a target="_blank" rel="noopener noreferrer" href="https://www.gnu.org/software/libc/manual/html_mono/libc.html#Locales">https://www.gnu.org/software/libc/manual/html_mono/libc.html#Locales</a>
Locales and Internationalization


<a target="_blank" rel="noopener noreferrer" href="https://pubs.opengroup.org/onlinepubs/9699919799/">https://pubs.opengroup.org/onlinepubs/9699919799/</a>
IEEE Std 1003.1-2017
Locale


<a target="_blank" rel="noopener noreferrer" href="https://unix.stackexchange.com/questions/87745/what-does-lc-all-c-do/87763#87763">https://unix.stackexchange.com/questions/87745/what-does-lc-all-c-do/87763#87763</a>
What does "LC_ALL=C" do?


<a target="_blank" rel="noopener noreferrer" href="https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe">https://github.com/mpv-player/mpv/commit/1e70e82baa9193f6f027338b0fab0f5078971fbe</a>
stream_libarchive: workaround various types of locale braindeath
(legendary C locales rant)


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/30479607/explain-the-effects-of-export-lang-lc-ctype-and-lc-all">https://stackoverflow.com/questions/30479607/explain-the-effects-of-export-lang-lc-ctype-and-lc-all</a>
    The LANG, LC_CTYPE and LC_ALL are special environment variables which after
    they got exported to the shell environment, are available and ready to be rea
    by certain programs which supports a locale (natural language formatting for C).

    Each variable sets the C library's notion of natural language formatting style
    for particular sets of routines, for example:

        - LC_ALL - Set the entire locale generically
        - LC_CTYPE - Set a locale for the ctype and multibyte functions.
          This controls recognition of upper and lower case, alphabetic or non-
          alphabetic characters, and so on.

    and other such as LC_COLLATE (for string collation routines),
                      LC_MESSAGES (for message catalogs),
                      LC_MONETARY (for formatting monetary values),
                      LC_NUMERIC (for formatting numbers),
                      LC_TIME (for formatting dates and times).

    Regarding LANG, it is used as a substitute for any unset LC_* variable.
    See: man setlocale (BSD), man locale

    So when certain C functions are called (such as setlocale, ctype, multibyte, catopen, printf, etc.),
    they read the locale settings from the configuration files and local environment in order to control
    and format natural language formatting style as per C programming language standards.

    see: setlocale                  <a target="_blank" rel="noopener noreferrer" href="http://www.unix.com/man-page/freebsd/3/setlocale/">http://www.unix.com/man-page/freebsd/3/setlocale/</a>
    see: ctype                      <a target="_blank" rel="noopener noreferrer" href="http://www.unix.com/man-page/freebsd/3/ctype/">http://www.unix.com/man-page/freebsd/3/ctype/</a>
    see: multibyte                  <a target="_blank" rel="noopener noreferrer" href="http://www.unix.com/man-page/freebsd/3/multibyte/">http://www.unix.com/man-page/freebsd/3/multibyte/</a>
    see: catopen                    <a target="_blank" rel="noopener noreferrer" href="http://www.unix.com/man-page/freebsd/3/catopen/">http://www.unix.com/man-page/freebsd/3/catopen/</a>
    see:printf                      <a target="_blank" rel="noopener noreferrer" href="http://www.unix.com/man-page/freebsd/3/printf/">http://www.unix.com/man-page/freebsd/3/printf/</a>
    see: ISO C99                    <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/C99">https://en.wikipedia.org/wiki/C99</a>
    see: C Library - &lt;locale.h&gt;     <a target="_blank" rel="noopener noreferrer" href="https://www.tutorialspoint.com/c_standard_library/locale_h.htm">https://www.tutorialspoint.com/c_standard_library/locale_h.htm</a>


AIX documentation
<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/aix/7.1?topic=globalization-locales">https://www.ibm.com/docs/en/aix/7.1?topic=globalization-locales</a>
- Understanding locale
- Understanding locale categories
- Understanding locale environment variables
- Understanding the locale definition source file
- Multibyte subroutines
- Wide character subroutines
- Bidirectionality and character shaping
- Code set independence
- File name matching
- Radix character handling
- Programming model


<a target="_blank" rel="noopener noreferrer" href="https://bugzilla.mozilla.org/show_bug.cgi?id=1612379">https://bugzilla.mozilla.org/show_bug.cgi?id=1612379</a>
Narrow down the list of ICU locales we ship

<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/data_management.md">https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/data_management.md</a>
Data management in ICU4X


<a target="_blank" rel="noopener noreferrer" href="https://pubs.opengroup.org/onlinepubs/9699919799/">https://pubs.opengroup.org/onlinepubs/9699919799/</a>
localedef - define locale environment
If the locale value begins with a slash, it shall be interpreted as the pathname
of a file that was created in the output format used by the localedef utility;
see OUTPUT FILES under localedef. Referencing such a pathname shall result in
that locale being used for the indicated category.


<hr><h2 id="CLDR_Common_Locale_Data_Repository">CLDR Common Locale Data Repository</h2><hr>
19/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/twitter/twitter-cldr-rb">https://github.com/twitter/twitter-cldr-rb</a>
Ruby implementation of the ICU (International Components for Unicode) that uses
the Common Locale Data Repository to format dates, plurals, and more.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/twitter/twitter-cldr-js">https://github.com/twitter/twitter-cldr-js</a>
JavaScript implementation of the ICU (International Components for Unicode) that uses
the Common Locale Data Repository to format dates, plurals, and more. Based on twitter-cldr-rb.


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.atlassian.net/jira/software/c/projects/CLDR/issues/?filter=allissues">https://unicode-org.atlassian.net/jira/software/c/projects/CLDR/issues/?filter=allissues</a>
CLDR tickets


<hr><h2 id="Case_mappings">Case mappings</h2><hr>
Rule Final_Sigma in default case algorithms.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/php/php-src/pull/10268">https://github.com/php/php-src/pull/10268</a>
jlf: difficult to implement, involves to scan arbitrarily far to the left and
right of capital sigma.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/casemap_charprop.html">https://www.unicode.org/faq/casemap_charprop.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/7360996/unicode-correct-title-case-in-java?noredirect=1&lq=1">https://stackoverflow.com/questions/7360996/unicode-correct-title-case-in-java?noredirect=1&lq=1</a>
Unicode-correct title case in Java


<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/unicode-case-mapping/latest/unicode_case_mapping/">https://docs.rs/unicode-case-mapping/latest/unicode_case_mapping/</a>
Example
assert_eq!(unicode_case_mapping::to_lowercase('ƒ∞'), ['i' as u32, 0x0307]);
assert_eq!(unicode_case_mapping::to_lowercase('√ü'), ['√ü' as u32, 0]);
assert_eq!(unicode_case_mapping::to_uppercase('√ü'), ['S' as u32, 'S' as u32, 0]);
assert_eq!(unicode_case_mapping::to_titlecase('√ü'), ['S' as u32, 's' as u32, 0]);
assert_eq!(unicode_case_mapping::to_titlecase('-'), [0; 3]);
assert_eq!(unicode_case_mapping::case_folded('I'), NonZeroU32::new('i' as u32));
assert_eq!(unicode_case_mapping::case_folded('√ü'), None);
assert_eq!(unicode_case_mapping::case_folded('·∫û'), NonZeroU32::new('√ü' as u32));


<a target="_blank" rel="noopener noreferrer" href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/titlecase.html">https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/titlecase.html</a>
fun Char.titlecase(): String
    val chars = listOf('a', '«Ö', '≈â', '+', '√ü')
    val titlecaseChar = chars.map { it.titlecaseChar() }
    val titlecase = chars.map { it.titlecase() }
    println(titlecaseChar) // [A, «Ö, ≈â, +, √ü]
    println(titlecase) // [A, «Ö,  ºN, +, Ss]
fun Char.titlecase(locale: Locale): String
    val chars = listOf('a', '«Ö', '≈â', '+', '√ü', 'i')
    val titlecase = chars.map { it.titlecase() }
    val turkishLocale = Locale.forLanguageTag("tr")
    val titlecaseTurkish = chars.map { it.titlecase(turkishLocale) }
    println(titlecase) // [A, «Ö,  ºN, +, Ss, I]
    println(titlecaseTurkish) // [A, «Ö,  ºN, +, Ss, ƒ∞]


<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/177">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/177</a>
jlf: good summary, was not so obvious before I understand there are simple and
full case mappings...
    Also note that the Unicode standard only provides defaults for, but then
    goes on to say that locale/language specific mappings should really be used.

    The Unicode standard is very explicit that things like uppercase
    transformations should be able to handle language specific issues such as
    the Turkish dotted and dotless i, and that ‚Äú√ü‚Äù should be uppercased to
    ‚ÄúSS‚Äù in German.

    See:
        Q: Is all of the Unicode case mapping information in UnicodeData.txt?
        A: No. The UnicodeData.txt file includes all of the one-to-one case mappings.
        Since many parsers were built with the expectation that UnicodeData.txt
        would have at most a single character in each case mapping field, the
        file SpecialCasing.txt was added to provide the one-to-many mappings,
        such as the one needed for uppercasing √ü (U+00DF LATIN SMALL LETTER SHARP S).
        In addition, CaseFolding.txt contains additional mappings used in case
        folding and caseless matching. For more information, see Section 5.18,
        Case Mappings in The Unicode Standard.
    and
        A: The Unicode Standard defines the default case mapping for each
        individual character, with each character considered in isolation. This
        mapping does not provide for the context in which the character appears,
        nor for the language-specific rules that must be applied when working in
        natural language text.


<a target="_blank" rel="noopener noreferrer" href="https://www.b-list.org/weblog/2018/nov/26/case/">https://www.b-list.org/weblog/2018/nov/26/case/</a>
Truths programmers should know about case


<hr><h2 id="Collation__sorting">Collation, sorting</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/tr35-collation.html">https://www.unicode.org/reports/tr35/tr35-collation.html</a>
UNICODE LOCALE DATA MARKUP LANGUAGE (LDML)
PART 5: COLLATION


01/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jgm/unicode-collation">https://github.com/jgm/unicode-collation</a>
<a target="_blank" rel="noopener noreferrer" href="https://hackage.haskell.org/package/unicode-collation">https://hackage.haskell.org/package/unicode-collation</a>
Haskell implementation of the Unicode Collation Algorithm


<a target="_blank" rel="noopener noreferrer" href="https://icu4c-demos.unicode.org/icu-bin/collation.html">https://icu4c-demos.unicode.org/icu-bin/collation.html</a>
ICU Collation Demo


<a target="_blank" rel="noopener noreferrer" href="https://www.enterprisedb.com/docs/epas/latest/epas_guide/03_database_administration/06_unicode_collation_algorithm/">https://www.enterprisedb.com/docs/epas/latest/epas_guide/03_database_administration/06_unicode_collation_algorithm/</a>
Unicode Collation Algorithm


<a target="_blank" rel="noopener noreferrer" href="https://www.minaret.info/test/collate.msp">https://www.minaret.info/test/collate.msp</a>
This page provides a means to convert a string of Unicode characters into a binary collation key using
the Java language version ("icu4j") of the IBM International Components for Unicode (ICU) library.
A collation key is the basis for sorting and comparing strings in a language-sensitive Unicode environment.
A collation key is built using a "locale" (a designation for a particular laguage or a variant) and a comparison level.
The levels supported here (Primary, Secondary, Tertiary, Quaternary and Identical) correspond to levels
"L1" through "Ln" as described in Unicode Technical Standard #10 - Unicode Collation Algorithm.
When comparing collation keys for two different strings, both keys must have been created using the same locale
and comparison level in order to be meaningful. The two keys are compared from left to right, byte for byte
until one of the bytes is not equal to the other. Whichever byte is numerically less than the other causes
the source string for that collation key to sort before the other string.


<a target="_blank" rel="noopener noreferrer" href="https://lemire.me/blog/2018/12/17/sorting-strings-properly-is-stupidly-hard/">https://lemire.me/blog/2018/12/17/sorting-strings-properly-is-stupidly-hard/</a>
It's the comments section which is interesting.


<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/sorting-strings-by-unicode-collation-order/11195">https://discourse.julialang.org/t/sorting-strings-by-unicode-collation-order/11195</a>
Not supported


03/08/2022
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/unicode-15-0-beta-and-sorting-collation/83090">https://discourse.julialang.org/t/unicode-15-0-beta-and-sorting-collation/83090</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/emoji/charts-15.0/emoji-ordering.html">https://www.unicode.org/emoji/charts-15.0/emoji-ordering.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Natural_sort_order">https://en.wikipedia.org/wiki/Natural_sort_order</a>
Natural sort order is an ordering of strings in alphabetical order,
except that multi-digit numbers are ordered as a single character.
Natural sort order has been promoted as being more human-friendly ("natural")
than the machine-oriented pure alphabetical order.
For example, in alphabetical sorting "z11" would be sorted before "z2"
because "1" is sorted as smaller than "2",
while in natural sorting "z2" is sorted before "z11" because "2" is sorted as smaller than "11".
Alphabetical sorting:
    z11
    z2
Natural sorting:
    z2
    z11
Functionality to sort by natural sort order is built into many programming languages and libraries.


02/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://www.postgresql.org/message-id/flat/BA6132ED-1F6B-4A0B-AC22-81278F5AB81E%40tripadvisor.com">https://www.postgresql.org/message-id/flat/BA6132ED-1F6B-4A0B-AC22-81278F5AB81E%40tripadvisor.com</a>
The dangers of streaming across versions of glibc: A cautionary tale
SELECT 'Ôº≠' &gt; '‡Æê';
'FULLWIDTH LATIN CAPITAL LETTER M' (U+FF2D)
'TAMIL LETTER AI' (U+0B90)
Across different machines, running the same version of postgres, and in databases
with identical character encodings and collations ('en_US.UTF-8') that select will
return different results if the version of glibc is different.
master:src/backend/utils/adt/varlena.c:1494,1497  These are the lines where postgres
calls strcoll_l and strcoll, in order to sort strings in a locale aware manner.
The reality is that there are different versions of glibc out there in the wild,
and they do not sort consistently across versions/environments.


<a target="_blank" rel="noopener noreferrer" href="https://collations.info/concepts/">https://collations.info/concepts/</a>
a site devoted to working with Collations, Unicode, Encodings, Code Pages, etc in Microsoft SQL Server.


<hr><h2 id="BIDI_title">BIDI title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.iamcal.com/understanding-bidirectional-text/">https://www.iamcal.com/understanding-bidirectional-text/</a>
Understanding Bidirectional (BIDI) Text in Unicode


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/International/articles/inline-bidi-markup/uba-basics">https://www.w3.org/International/articles/inline-bidi-markup/uba-basics</a>
Unicode Bidirectional Algorithm basics
jlf: the example are gif images :-(( no way to copy-paste the characters.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/notes/tn39/">https://www.unicode.org/notes/tn39/</a>
BIDI BRACKETS FOR DUMMIES


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/5801820/how-to-solve-bidi-bracket-issues">https://stackoverflow.com/questions/5801820/how-to-solve-bidi-bracket-issues</a>
How to solve BiDi bracket issues?


<a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/mvidner/e96ac917d9a54e09d9730220a34b0d24">https://gist.github.com/mvidner/e96ac917d9a54e09d9730220a34b0d24</a>
Problems with Bidirectional (BiDi) Text


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/International/questions/qa-bidi-unicode-controls">https://www.w3.org/International/questions/qa-bidi-unicode-controls</a>
How to use Unicode controls for bidi text


<a target="_blank" rel="noopener noreferrer" href="https://github.com/mvidner/bidi-test">https://github.com/mvidner/bidi-test</a>
Testing bidirectional text


<a target="_blank" rel="noopener noreferrer" href="https://terminal-wg.pages.freedesktop.org/bidi/">https://terminal-wg.pages.freedesktop.org/bidi/</a>
BiDi in Terminal Emulators


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/International/articles/inline-bidi-markup/uba-basics">https://www.w3.org/International/articles/inline-bidi-markup/uba-basics</a>
Unicode Bidirectional Algorithm basics
W3C


<a target="_blank" rel="noopener noreferrer" href="http://fribidi.org/">http://fribidi.org/</a>
GNU FriBidi is an implementation of the Unicode Bidirectional Algorithm (bidi).
jlf: dead...
The latest release is fribidi-0.19.7.tar.bz2 from August 4, 2015. This release
is based on Unicode 6.2.0 character database.
---
jlf: maybe not dead, but low activity... v1.0.13
<a target="_blank" rel="noopener noreferrer" href="https://github.com/fribidi/fribidi">https://github.com/fribidi/fribidi</a>


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37990523">https://news.ycombinator.com/item?id=37990523</a>
Ask HN: Bidirectional Text Navigation


<hr><h2 id="Emoji">Emoji</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/emoji/15.0/emoji-test.txt">https://www.unicode.org/Public/emoji/15.0/emoji-test.txt</a>


<a target="_blank" rel="noopener noreferrer" href="https://emojipedia.org/">https://emojipedia.org/</a>


<a target="_blank" rel="noopener noreferrer" href="http://xahlee.info/comp/unicode_emoji.html">http://xahlee.info/comp/unicode_emoji.html</a>


29/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://tonsky.me/blog/emoji/">https://tonsky.me/blog/emoji/</a>


27/02/2023
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=34925446">https://news.ycombinator.com/item?id=34925446</a>
Discussion about emoji and graphemes (again...).
Nothing very interesting in this discussion.
Remember:
The "length" of a string in extended grapheme clusters is not stable across Unicode versions, which seems like a recipe for confusion.
The length in code units is unambiguous and constant across versions.
---
Executor:
NinjaCat = "üê±‚Äçüë§"
NinjaCat~description=
    'UTF-8 not-ASCII (11 bytes)'
NinjaCat~text~characters==
    an Array (shape [3], 3 items)
     1 : ( "üê±"  U+1F431 So 2 "CAT FACE" )
     2 : ( "‚Äç"    U+200D Cf 0 "ZERO WIDTH JOINER", "ZWJ" )
     3 : ( "üë§"  U+1F464 So 2 "BUST IN SILHOUETTE" )


<hr><h2 id="Countries__flags">Countries, flags</h2><hr>
22/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Regional_indicator_symbol">https://en.wikipedia.org/wiki/Regional_indicator_symbol</a>
Regional indicator symbol


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/ISO_3166-1">https://en.wikipedia.org/wiki/ISO_3166-1</a>
ISO 3166-1 (Codes for the representation of names of countries and their subdivisions)


<a target="_blank" rel="noopener noreferrer" href="https://observablehq.com/@jobleonard/which-unicode-flags-are-reversible">https://observablehq.com/@jobleonard/which-unicode-flags-are-reversible</a>


<hr><h2 id="Evidence_of_partial_or_wrong_support_of_Unicode">Evidence of partial or wrong support of Unicode</h2><hr>
13/08/2013
We don‚Äôt need a string type
<a target="_blank" rel="noopener noreferrer" href="https://mortoray.com/2013/08/13/we-dont-need-a-string-type/">https://mortoray.com/2013/08/13/we-dont-need-a-string-type/</a>


01/12/2013
Strings in Ruby are UTF-8 now‚Ä¶ right?
<a target="_blank" rel="noopener noreferrer" href="http://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/">http://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/</a>


14/07/2017
Testing Ruby's Unicode Support
<a target="_blank" rel="noopener noreferrer" href="http://blog.honeybadger.io/ruby-s-unicode-support/">http://blog.honeybadger.io/ruby-s-unicode-support/</a>


22/05/2021
Emoji.length == 2
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=13830177">https://news.ycombinator.com/item?id=13830177</a>
Lot of comments, did not read all, to continue


22/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/">https://manishearth.github.io/blog/2017/01/14/stop-ascribing-meaning-to-unicode-code-points/</a>
Let's Stop Ascribing Meaning to Code Points


18/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/">https://manishearth.github.io/blog/2017/01/15/breaking-our-latin-1-assumptions/</a>
Breaking Our Latin-1 Assumptions


<hr><h2 id="Optimization__SIMD">Optimization, SIMD</h2><hr>
08/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://lemire.me/blog/2018/05/16/validating-utf-8-strings-using-as-little-as-0-7-cycles-per-byte/">https://lemire.me/blog/2018/05/16/validating-utf-8-strings-using-as-little-as-0-7-cycles-per-byte/</a>


[obsolete]
<a target="_blank" rel="noopener noreferrer" href="https://github.com/lemire/fastvalidate-utf-8">https://github.com/lemire/fastvalidate-utf-8</a>
header-only library to validate utf-8 strings at high speeds (using SIMD instructions)
jlf 2023/06/16 (now obsolete)
NOTE: The fastvalidate-utf-8 library is obsolete as of 2022: please adopt the simdutf library.
It is much more powerful, faster and better tested.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/simdutf/simdutf">https://github.com/simdutf/simdutf</a>
simdutf: Unicode at gigabytes per second


08/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/simdjson/simdjson">https://github.com/simdjson/simdjson</a>
simdjson : Parsing gigabytes of JSON per second
The simdjson library uses commonly available SIMD instructions and microparallel algorithms
to parse JSON 4x faster than RapidJSON and 25x faster than JSON for Modern C++.
Minify JSON at 6 GB/s, validate UTF-8 at 13 GB/s, NDJSON at 3.5 GB/s


<a target="_blank" rel="noopener noreferrer" href="https://arxiv.org/abs/2010.03090">https://arxiv.org/abs/2010.03090</a>
Validating UTF-8 In Less Than One Instruction Per Byte
John Keiser, Daniel Lemire
The majority of text is stored in UTF-8, which must be validated on ingestion.
We present the lookup algorithm, which outperforms UTF-8 validation routines used
in many libraries and languages by more than 10 times using commonly available SIMD instructions.
To ensure reproducibility, our work is freely available as open source software.


<a target="_blank" rel="noopener noreferrer" href="https://r-libre.teluq.ca/2178/">https://r-libre.teluq.ca/2178/</a>
    Recherche et analyse de solutions performantes pour le traitement de fichiers JSON dans un langage de haut niveau [r-libre/2178]
Referenced from
    <a target="_blank" rel="noopener noreferrer" href="https://lemire.me/blog/">https://lemire.me/blog/</a>
    Daniel Lemire's blog ‚Äì Daniel Lemire is a computer science professor at the University of Quebec (TELUQ) in Montreal.
    His research is focused on software performance and data engineering. He is a techno-optimist.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/simdutf/simdutf">https://github.com/simdutf/simdutf</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=32700315">https://news.ycombinator.com/item?id=32700315</a>
Unicode routines (UTF8, UTF16, UTF32): billions of characters per second using SSE2, AVX2, NEON, AVX-512.


<a target="_blank" rel="noopener noreferrer" href="https://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/">https://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/</a>
(jlf: also referenced in the section "String comparison")
How the JVM compares your strings using the craziest x86 instruction you've never heard of
---
Comment from a Swift thread:
<a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/string-s-abi-and-utf-8/17676/25">https://forums.swift.org/t/string-s-abi-and-utf-8/17676/25</a>
PCMPxSTRx is no longer faster than equivalent "simple" vector instruction sequences for straightforward comparisons
(this had already been the case for a few years when that article was written, which is curious).
It can be used productively (with some care) for some other operations like substring matching,
but that's not as much of a heavy-hitter. There's a bunch of string stuff that will benefit from general vectorization,
and which is absolutely on our roadmap to tackle, but using the PCMPxSTRx instructions specifically isn't a source of wins on the most important operations.


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=34267936">https://news.ycombinator.com/item?id=34267936</a>
Transcoding Unicode with AVX-512: AMD Zen 4 vs. Intel Ice Lake (lemire.me)


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/java/comments/qafjtg/faster_charset_encoding/">https://www.reddit.com/r/java/comments/qafjtg/faster_charset_encoding/</a>
Java 17 uses avx in both encoding and decoding


<a target="_blank" rel="noopener noreferrer" href="https://lemire.me/blog/2023/02/16/computing-the-utf-8-size-of-a-latin-1-string-quickly-avx-edition/">https://lemire.me/blog/2023/02/16/computing-the-utf-8-size-of-a-latin-1-string-quickly-avx-edition/</a>
Computing the UTF-8 size of a Latin 1 string quickly (AVX edition)


<hr><h2 id="Variation_sequence">Variation sequence</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/UCD/latest/ucd/emoji/emoji-variation-sequences.txt">https://www.unicode.org/Public/UCD/latest/ucd/emoji/emoji-variation-sequences.txt</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/15.1.0/ucd/emoji/emoji-variation-sequences.txt">https://www.unicode.org/Public/15.1.0/ucd/emoji/emoji-variation-sequences.txt</a>
# emoji-variation-sequences.txt


22/05/2021
List of all code points that can display differently via a variation sequence
<a target="_blank" rel="noopener noreferrer" href="http://randomguy32.de/unicode/charts/standardized-variants/#emoji">http://randomguy32.de/unicode/charts/standardized-variants/#emoji</a>
Safari is better to display the characters.
Google Chrome and Opera have the same limitations: some characters are not supported (ex: section Phags-Pa).


<a target="_blank" rel="noopener noreferrer" href="https://sethmlarson.dev/unicode-variation-selectors">https://sethmlarson.dev/unicode-variation-selectors</a>
Mahjong tiles and Unicode variation selectors


<hr><h2 id="Whitespaces__separators">Whitespaces, separators</h2><hr>
22/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/">https://eev.ee/blog/2015/09/12/dark-corners-of-unicode/</a>
A section about wcwidth.
A section about spaces:
    There are actually two definitions of whitespace in Unicode.
    Unicode assigns every codepoint a category, and has three categories for
    what sounds like whitespace:
        ‚ÄúSeparator, space‚Äù;
        ‚ÄúSeparator, line‚Äù;
        ‚ÄúSeparator, paragraph‚Äù.
    CR, LF, tab, and even vertical tab are all categorized as ‚ÄúOther, control‚Äù
    and not as separators.
    The only character in the ‚ÄúSeparator, line‚Äù category is U+2028 LINE SEPARATOR,
    and the only character in ‚ÄúSeparator, paragraph‚Äù is U+2029 PARAGRAPH SEPARATOR.
    Thankfully, all of these have the WSpace property.

    As an added wrinkle, the lone oddball character ‚Äú‚†Ä‚Äù renders like a space in most fonts.
    jlf: 2 cols x 3 lines of debossed dots.
    But it‚Äôs not whitespace, it‚Äôs not categorized as a separator, and it doesn‚Äôt have WSpace.
    It‚Äôs actually U+2800 BRAILLE PATTERN BLANK, the Braille character with none of the dots raised.
    (I say ‚Äúmost fonts‚Äù because I‚Äôve occasionally seen it rendered as a 2√ó4 grid of open circles.)


<hr><h2 id="Hyphenation">Hyphenation</h2><hr>
break words into syllables
    I need to break words into syllables:astronomical --&gt; as - tro - nom - ic - al
    Is it possible to do this (in different languages) using ICU library?  (if no, may be you suggest other tools for it?)

    Andreas Heigl:
    While it looks like this is not something for ICU[1], there are libraries out there handling that - most of the time based on the thesis of Marc Liang.
    I've built an implementation for PHP[2] but there are a lot of others out there[3].

    [1] <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/164#issuecomment-651410272">https://github.com/unicode-org/icu4x/issues/164#issuecomment-651410272</a>
    [2] <a target="_blank" rel="noopener noreferrer" href="https://github.com/heiglandreas/Org_Heigl_Hyphenator">https://github.com/heiglandreas/Org_Heigl_Hyphenator</a>
    [3] <a target="_blank" rel="noopener noreferrer" href="https://github.com/search?q=hyphenate&type=repositories">https://github.com/search?q=hyphenate&type=repositories</a>
    <a target="_blank" rel="noopener noreferrer" href="https://tug.org/docs/liang/liang-thesis.pdf">https://tug.org/docs/liang/liang-thesis.pdf</a>


<hr><h2 id="DNS_title__Domain_Name_title__Domain_Name_System_title">DNS title, Domain Name title, Domain Name System title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="http://lambda-the-ultimate.org/node/5674#comment-97016">http://lambda-the-ultimate.org/node/5674#comment-97016</a>
jlf: I created this section because of this comment
Have you ever looked at how international encoding of DNS names are done in URLs? It uses Punycode, and it's a disaster.
Here's a good starting point to read up on this: <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Internationalized_domain_name">https://en.wikipedia.org/wiki/Internationalized_domain_name</a>


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Internationalized_domain_name">https://en.wikipedia.org/wiki/Internationalized_domain_name</a>
Internationalized domain name
    ToASCII leaves ASCII labels unchanged. It fails if the label is unsuitable for
    the Domain Name System. For labels containing at least one non-ASCII character,
    ToASCII applies the Nameprep algorithm (<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Nameprep)">https://en.wikipedia.org/wiki/Nameprep)</a>
    This converts the label to lowercase and performs other normalization. ToASCII
    then translates the result to ASCII, using Punycode (<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Punycode)">https://en.wikipedia.org/wiki/Punycode)</a>
    Finally, it prepends the four-character string "xn--". This four-character string
    is called the ASCII Compatible Encoding (ACE) prefix. It is used to distinguish
    labels encoded in Punycode from ordinary ASCII labels. The ToASCII algorithm can
    fail in several ways. For example, the final string could exceed the 63-character
    limit of a DNS label. A label for which ToASCII fails cannot be used in an
    internationalized domain name.

    The function ToUnicode reverses the action of ToASCII, stripping off the ACE prefix
    and applying the Punycode decode algorithm. It does not reverse the Nameprep processing,
    since that is merely a normalization and is by nature irreversible. Unlike ToASCII,
    ToUnicode always succeeds, because it simply returns the original string if decoding fails.
    In particular, this means that ToUnicode has no effect on a string that does not
    begin with the ACE prefix.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Punycode">https://en.wikipedia.org/wiki/Punycode</a>
Punycode is a representation of Unicode with the limited ASCII character subset
used for Internet hostnames. Using Punycode, host names containing Unicode characters
are transcoded to a subset of ASCII consisting of letters, digits, and hyphens,
which is called the letter‚Äìdigit‚Äìhyphen (LDH) subset. For example, M√ºnchen
(German name for Munich) is encoded as Mnchen-3ya.


<hr><h2 id="All_languages">All languages</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.omniglot.com/index.htm">https://www.omniglot.com/index.htm</a>
The online encyclopedia of writing systems &amp; languages
jlf: nothing about Unicode but good for culture g√©n√©rale.


<hr><h2 id="Classical_languages">Classical languages</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://docs.cltk.org/en/latest/">https://docs.cltk.org/en/latest/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/cltk/cltk">https://github.com/cltk/cltk</a>
The Classical Language Toolkit
Python library
The Classical Language Toolkit (CLTK) is a Python library offering natural
language processing (NLP) for pre-modern languages. Pre-configured pipelines are
available for 19 languages.
    Akkadian
    Arabic
    Aramaic
    Classical Chinese
    Coptic
    Gothic
    Greek
    Hindi
    Latin
    Middle High German
    English
    French
    Old Church Slavonic
    Old Norse
    Pali
    Panjabi
    Sanskrit (Some parts of the Sanskrit library are forked from the Indic NLP Library)


<hr><h2 id="Arabic_language">Arabic language</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Arabic_script_in_Unicode">https://en.wikipedia.org/wiki/Arabic_script_in_Unicode</a>
Arabic script in Unicode


<hr><h2 id="Indic_languages">Indic languages</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/indic.html">https://www.unicode.org/faq/indic.html</a>
Indic scripts in the narrow sense are the nine major Brahmi-derived scripts of India.
In a wider sense, the term can cover all Brahmic scripts and Kharoshthi.
What is ISCII?
Indian Standard Code for Information Interchange (ISCII) is the character code
for Indian scripts that originate from the Brahmi script.
Keywords:
    nukta
    Vedic Sanskrit
    vowel signs (matras)
    vowel modifiers (candrabindu, anusvara)
    the consonant modifier (nukta)
    Tamil
    Bengali (Bangla) / Assamese Script
    Sindhi implosive consonants


FAQ: How do I collate Indic language data?
Collation order is not the same as code point order. A good treatment of some
issues specific to collation in Indic languages can be found in the paper
Issues in Indic Language Collation by Cathy Wissink (<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/notes/tn1/)">https://www.unicode.org/notes/tn1/)</a>
Collation in general must proceed at the level of language or language variant,
not at the script or codepoint levels. See also UTS #10: Unicode Collation Algorithm.
Some Indic-specific issues are also discussed in that report.


This section illustrates that Unicode‚Äôs concepts like ‚Äúextended grapheme cluster‚Äù
are meant to provide some low-level, general segmentation, and are not going
to be enough for ideal experience for end users.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Devanagari#Conjunct_consonants">https://en.wikipedia.org/wiki/Devanagari#Conjunct_consonants</a>


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Devanagari_conjuncts">https://en.wikipedia.org/wiki/Devanagari_conjuncts</a>
Conjunct consonants are a form of orthographic ligature characteristic of the
Brahmic scripts. They are constructed of more than two consonant letters.
Biconsonantal conjuncts are common, but longer conjuncts are increasingly
constrained by the languages' phonologies and the actual number of conjuncts
observed drops sharply. Ulrich Stiehl includes a five-letter Devanagari conjunct
‡§∞‡•ç‡§§‡•ç‡§∏‡•ç‡§®‡•ç‡§Ø (rtsny)[1] among the top 360 most frequent conjuncts found in Classical
Sanskrit;[2] the complete list appears below. Conjuncts often span a syllable
boundary, and many of the conjuncts below occur only in the middle of words,
where the coda consonants of one syllable are conjoined with the onset c
onsonants of the following syllable.
[1] As in Sanskrit word ‡§ï‡§æ‡§∞‡•ç‡§§‡•ç‡§∏‡•ç‡§®‡•ç‡§Ø (In Bengali Script ‡¶ï‡¶æ‡¶∞‡ßç‡ßé‡¶∏‡ßç‡¶®‡ßç‡¶Ø), meaning "The Whole, Entirety"
[2] Stiehl, Ulrich. "Devanagari-Schreib√ºbungen" (PDF). www.sanskritweb.net.
    <a target="_blank" rel="noopener noreferrer" href="http://www.sanskritweb.net/deutsch/devanagari.pdf">http://www.sanskritweb.net/deutsch/devanagari.pdf</a>

<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/6805311/combining-devanagari-characters">https://stackoverflow.com/questions/6805311/combining-devanagari-characters</a>
Combining Devanagari characters
"‡§¨‡§ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§π‡•ã"~text~graphemes==
    a GraphemeSupplier
     1  : T'‡§¨‡§ø'
     2  : T'‡§ï‡•ç'     &lt;-- According the comments, these 2 graphemes should be only one: ‡§ï‡•ç‡§∞
     3  : T'‡§∞'      &lt;-- even ICU doesn't support that... it's a tailored grapheme cluster
     4  : T'‡§Æ'
     5  : T' '
     6  : T'‡§Æ‡•á'
     7  : T'‡§∞‡•ã'
     8  : T' '
     9  : T'‡§®‡§æ'
     10 : T'‡§Æ'
     11 : T' '
     12 : T'‡§π‡•ã'
"‡§¨‡§ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§π‡•ã"~text~characters==
    an Array (shape [18], 18 items)
     1  : ( "‡§¨"   U+092C Lo 1 "DEVANAGARI LETTER BA" )
     2  : ( "‡§ø"    U+093F Mc 0 "DEVANAGARI VOWEL SIGN I" )
     3  : ( "‡§ï"   U+0915 Lo 1 "DEVANAGARI LETTER KA" )
     4  : ( "‡•ç"    U+094D Mn 0 "DEVANAGARI SIGN VIRAMA" )           &lt;-- influence segmentation
     5  : ( "‡§∞"   U+0930 Lo 1 "DEVANAGARI LETTER RA" )
     6  : ( "‡§Æ"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     7  : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     8  : ( "‡§Æ"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     9  : ( "‡•á"    U+0947 Mn 0 "DEVANAGARI VOWEL SIGN E" )
     10 : ( "‡§∞"   U+0930 Lo 1 "DEVANAGARI LETTER RA" )
     11 : ( "‡•ã"    U+094B Mc 0 "DEVANAGARI VOWEL SIGN O" )
     12 : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     13 : ( "‡§®"   U+0928 Lo 1 "DEVANAGARI LETTER NA" )
     14 : ( "‡§æ"    U+093E Mc 0 "DEVANAGARI VOWEL SIGN AA" )
     15 : ( "‡§Æ"   U+092E Lo 1 "DEVANAGARI LETTER MA" )
     16 : ( " "   U+0020 Zs 1 "SPACE", "SP" )
     17 : ( "‡§π"   U+0939 Lo 1 "DEVANAGARI LETTER HA" )
     18 : ( "‡•ã"    U+094B Mc 0 "DEVANAGARI VOWEL SIGN O" )
In Devanagari, each grapheme cluster consists of an initial letter, optional
pairs of virama (vowel killer) and letter, and an optional vowel sign.
    virama = u'\N{DEVANAGARI SIGN VIRAMA}'
    cluster = u''
    last = None
    for c in s:
        cat = unicodedata.category(c)[0]
        if cat == 'M' or cat == 'L' and last == virama:
            cluster += c
        else:
            if cluster:
                yield cluster
            cluster = c
        last = c
    if cluster:
        yield cluster
---
Let's cover the grammar very quickly: The Devanagari Block.
As a developer, there are two character classes you'll want to concern yourself with:
Sign:
    This is a character that affects a previously-occurring character.
    Example, this character: ‡•ç. The light-colored circle indicates the location
    of the center of the character it is to be placed upon.
Letter / Vowel / Other:
    This is a character that may be affected by signs.
    Example, this character: ‡§ï.
Combination result of ‡•ç and ‡§ï: ‡§ï‡•ç. But combinations can extend, so ‡§ï‡•ç and ‡§∑‡§§‡§ø will
actually become ‡§ï‡•ç‡§∑‡§§‡§ø (in this case, we right-rotate the first character by 90 degrees,
modify some of the stylish elements, and attach it at the left side of the second character).


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=20058454">https://news.ycombinator.com/item?id=20058454</a>
If I type anything like ‡§ï‡§ø‡§Æ‡§™‡§ø (‚Äúkimapi‚Äù) and hit backspace, it turns into ‡§ï‡§ø‡§Æ‡§™ (‚Äúkimapa‚Äù).
That is, the following sequence of codepoints:
    ‚Äé0915 DEVANAGARI LETTER KA
    ‚Äé093F DEVANAGARI VOWEL SIGN I
    ‚Äé092E DEVANAGARI LETTER MA
    ‚Äé092A DEVANAGARI LETTER PA
    ‚Äé093F DEVANAGARI VOWEL SIGN I
made of three grapheme clusters (containing 2, 1, and 2 codepoints respectively),
turns after a single backspace into the following sequence:
    ‚Äé0915 DEVANAGARI LETTER KA
    ‚Äé093F DEVANAGARI VOWEL SIGN I
    ‚Äé092E DEVANAGARI LETTER MA
    ‚Äé092A DEVANAGARI LETTER PA
This is what I expect/find intuitive, too, as a user.
Similarly ‡§Ö‡§®‡•ç‡§Ø‡§ö‡•ç‡§ö is made of 3 grapheme clusters but you hit backspace 7 times to delete it
(though there I'd slightly have preferred ‡§Ö‡§®‡•ç‡§Ø‡§ö‡•ç‡§ö‚Üí‡§Ö‡§®‡•ç‡§Ø‡§ö‡•ç‚Üí‡§Ö‡§®‡•ç‡§Ø‚Üí‡§Ö‡§®‡•ç‚Üí‡§Ö instead of
‡§Ö‡§®‡•ç‡§Ø‡§ö‡•ç‡§ö‚Üí‡§Ö‡§®‡•ç‡§Ø‡§ö‡•ç‚Üí‡§Ö‡§®‡•ç‡§Ø‡§ö‚Üí‡§Ö‡§®‡•ç‡§Ø‚Üí‡§Ö‡§®‡•ç‚Üí‡§Ö‡§®‚Üí‡§Ö that's seen, but one can live with this).


<a target="_blank" rel="noopener noreferrer" href="https://github.com/anoopkunchukuttan/indic_nlp_library">https://github.com/anoopkunchukuttan/indic_nlp_library</a>
The goal of the Indic NLP Library is to build Python based libraries for common
text processing and Natural Language Processing in Indian languages.
The library provides the following functionalities:
    Text Normalization
    Script Information
    Word Tokenization and Detokenization
    Sentence Splitting
    Word Segmentation
    Syllabification
    Script Conversion
    Romanization
    Indicization
    Transliteration
    Translation


<a target="_blank" rel="noopener noreferrer" href="https://github.com/AI4Bharat/indicnlp_catalog">https://github.com/AI4Bharat/indicnlp_catalog</a>
The Indic NLP Catalog
jlf: way beyond Unicode, tons of URLs...


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=20056966">https://news.ycombinator.com/item?id=20056966</a>
jlf: Devnagari seems to be an example where grapheme is not the right segmentation
What does "index" mean? (hindi) "‡§á‡§Ç‡§°‡•á‡§ï‡•ç‡§∏" ‡§ï‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§Ö‡§∞‡•ç‡§• ‡§π‡•à?
Including the quote marks, spaces, and question mark, that's 18 characters.
as a native speaker, shouldn't they be considered 15 characters?
‡§ï‡•ç‡§∏, ‡§ï‡•ç‡§Ø‡§æ and ‡§∞‡•ç‡§• each form individual conjunct consonants.
Counting them as two would then beget the question as to why ‡§°‡•á is not considered
two characters too, seeing as it is formed by combining ‡§° and ‡§è, much like ‡§ï‡•ç‡§∏
is formed by combining ‡§ï‡•ç and ‡§∏.
...
Devnagari allows simple characters to form compound characters.
Regarding ‡§ï‡•ç‡§∏ and ‡§°‡•á, the difference between them is that the former is a combination
of two consonants (pronounced "ks") while the latter is formed by a consonant and
a vowel ("de"). However, looking at the visual representation is wrong, since ‡§°‡§æ
(consonant+vowel) would also look like two characters.


<a target="_blank" rel="noopener noreferrer" href="https://slidetodoc.com/indic-text-segmentation-presented-by-swaran-lata-senior/">https://slidetodoc.com/indic-text-segmentation-presented-by-swaran-lata-senior/</a>
INDIC TEXT SEGMENTATION


<a target="_blank" rel="noopener noreferrer" href="https://github.com/w3c/iip/issues/34">https://github.com/w3c/iip/issues/34</a>
the final rendered state of the text is what influences the segmentation,
rather than the sequence of code points used.


<a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/typography/">https://docs.microsoft.com/en-us/typography/</a>

<a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/typography/script-development/tamil">https://docs.microsoft.com/en-us/typography/script-development/tamil</a>
Developing OpenType Fonts for Tamil Script
The first step is to analyze the input text and break it into syllable clusters.
Then apply font features and computes ligatures and combine marks.


<a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/typography/script-development/devanagari">https://docs.microsoft.com/en-us/typography/script-development/devanagari</a>
Developing OpenType Fonts for Devanagari Script


<a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2018/02/15/picking-apart-the-crashing-ios-string/">https://manishearth.github.io/blog/2018/02/15/picking-apart-the-crashing-ios-string/</a>
Picking Apart the Crashing iOS String
Posted by Manish Goregaokar on February 15, 2018
Indic scripts and consonant clusters
jlf: he's a black belt! or is it his native tongue?


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/75210512/how-to-split-devanagari-bi-tri-and-tetra-conjunct-consonants-as-a-whole-from-a-s">https://stackoverflow.com/questions/75210512/how-to-split-devanagari-bi-tri-and-tetra-conjunct-consonants-as-a-whole-from-a-s</a>
How to split Devanagari bi-tri and tetra conjunct consonants as a whole from a string?
    "‡§π‡§ø‡§®‡•ç‡§¶‡•Ä ‡§Æ‡•Å‡§ñ‡•ç‡§Ø‡§Æ‡§Ç‡§§‡•ç‡§∞‡•Ä ‡§π‡§ø‡§Æ‡§Ç‡§§"
    Current output:
    ‡§π‡§ø  ‡§®‡•ç  ‡§¶‡•Ä   ‡§Æ‡•Å  ‡§ñ‡•ç  ‡§Ø  ‡§Æ‡§Ç  ‡§§‡•ç  ‡§∞‡•Ä    ‡§π‡§ø  ‡§Æ‡§Ç  ‡§§
    Desired ouput:
    ‡§π‡§ø ‡§®‡•ç‡§¶‡•Ä  ‡§Æ‡•Å ‡§ñ‡•ç‡§Ø ‡§Æ‡§Ç ‡§§‡•ç‡§∞‡•Ä  ‡§π‡§ø ‡§Æ‡§Ç ‡§§


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/L2/L2023/23107-terminal-suppt.pdf">https://www.unicode.org/L2/L2023/23107-terminal-suppt.pdf</a>
Proper Complex Script Support in Text Terminals
page 8
Characters in one line will further be grouped into terminal clusters.
A terminal cluster contains the characters that are combined together in the
terminal environment. It is an instance of the tailored grapheme cluster defined
in UAX #29. In Indic scripts, for example, syllables with virama conjoiners in
the middle will be considered one single terminal cluster, while they are
treated as multiple extended grapheme clusters in UAX #29.
---
page9
In some writing systems, the form of a character may depend on the characters
that follow it. One example of this is Devanagari‚Äôs repha forms. This requires
the establishment of a work zone that contains the most recent characters, and
the property of the characters in the work zone is considered volatile and may
change depending on the incoming text from the guest.
When the terminal receives text, it will first append the text into the work
zone and measure the entire work zone to process potential property changes.
If the measurement result says that the text in the work zone could be broken
into multiple clusters, then the work zone will be shrunk to only contain the
last (maybe incomplete) cluster. The text before that will be committed, and its
properties will no longer change. As a result, at any time the work zone will
contain at most one cluster. When the cursor moves (via the terminal receiving a
cursor move command or a newline), all the text in the work zone will be
committed‚Äîeven if it is incomplete‚Äîand the work zone will be cleared.


<a target="_blank" rel="noopener noreferrer" href="https://slideplayer.com/slide/11341056/">https://slideplayer.com/slide/11341056/</a>
INDIC TEXT SEGMENTATION
todo: read


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9219162">https://news.ycombinator.com/item?id=9219162</a>
I Can Text You A Pile of Poo, But I Can‚Äôt Write My Name
March 17th, 2015
jlf: the article is about Bengali, but HN comments are also for other languages.
todo: read


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/L2/L2023/23140-graphemes-expectations.pdf">https://www.unicode.org/L2/L2023/23140-graphemes-expectations.pdf</a>
Unicode 15.1:
Unicode grapheme clusters tend to be closer to the larger user-perceived units.
Hangul text is clearly segmented into syllable blocks. For Brahmic scripts,
things are less clear. Grapheme clusters may contain several base-level units,
but up to Unicode 15 always broke after virama characters. This broke not only
within orthographic syllables, but for a number of scripts also within the
encoding of conjunct forms that users perceive as base-level units, such as
Khmer coengs (see subsection Subscript Consonant Signs of section 16.4 Khmer of
the Unicode Standard). In Unicode 15.1, this is being corrected for six scripts,
while leaving the others broken.


<hr><h2 id="CJK">CJK</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://resources.oreilly.com/examples/9781565922242/blob/master/doc/cjk.inf">https://resources.oreilly.com/examples/9781565922242/blob/master/doc/cjk.inf</a>
Version 2.1 (July 12, 1996)
Online Companion to "Understanding Japanese Information Processing"
This online document provides information on CJK (that is, Chinese, Japanese,
and Korean) character set standards and encoding systems.
---
jlf: 1996... but maybe some things to learn.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Cangjie_input_method">https://en.wikipedia.org/wiki/Cangjie_input_method</a>
Cangjie input method
jlf: nothing about Unicode... but maybe some things to learn.


<hr><h2 id="Korean">Korean</h2><hr>
22/05/2021
<a target="_blank" rel="noopener noreferrer" href="http://gernot-katzers-spice-pages.com/var/korean_hangul_unicode.html">http://gernot-katzers-spice-pages.com/var/korean_hangul_unicode.html</a>
The Korean Writing System


<hr><h2 id="Japanese">Japanese</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://heistak.github.io/your-code-displays-japanese-wrong/">https://heistak.github.io/your-code-displays-japanese-wrong/</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=29022906">https://news.ycombinator.com/item?id=29022906</a>

<a target="_blank" rel="noopener noreferrer" href="https://www.johndcook.com/blog/2022/09/25/katakana-hiragana-unicode/">https://www.johndcook.com/blog/2022/09/25/katakana-hiragana-unicode/</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=32987710">https://news.ycombinator.com/item?id=32987710</a>


<hr><h2 id="Polish">Polish</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.twardoch.com/download/polishhowto/index.html">https://www.twardoch.com/download/polishhowto/index.html</a>
Polish diacritics how to?


<hr><h2 id="IME___Input_Method_Editor">IME - Input Method Editor</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/ime/">https://hsivonen.fi/ime/</a>
An IME is a piece of software that transforms user-generated input events
(mostly keyboard events, but some IMEs allow some auxiliary pointing device interaction)
into text in a manner more complex than a mere keyboard layout.
Basically, if the relationship between the keys that a user presses on a hardware keyboard
and the text that ends up in an applications text buffer is more complex than when writing French,
an IME is in use.


<hr><h2 id="Text_editing">Text editing</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://lord.io/text-editing-hates-you-too/">https://lord.io/text-editing-hates-you-too/</a>
TEXT EDITING HATES YOU TOO


<hr><h2 id="Text_rendering__Text_shaping_library">Text rendering, Text shaping library</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://faultlore.com/blah/text-hates-you/">https://faultlore.com/blah/text-hates-you/</a>
Text Rendering Hates You
Aria Beingessner
September 28th, 2019
jlf: culture g√©n√©rale
todo: read


<a target="_blank" rel="noopener noreferrer" href="https://harfbuzz.github.io/">https://harfbuzz.github.io/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/harfbuzz/harfbuzz">https://github.com/harfbuzz/harfbuzz</a>
jlf: referenced by ICU
Users of ICU Layout are strongly encouraged to consider the HarfBuzz project as
a replacement for the ICU Layout Engine.


Uniscribe if you are writing Windows software


CoreText on macOS


<hr><h2 id="String_Matching">String Matching</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/charmod-norm/">https://www.w3.org/TR/charmod-norm/</a>
String matching

Case folding is the process of making two texts which differ only in case identical for comparison purposes, that is, it is meant for the purpose of string matching.
This is distinct from case mapping, which is primarily meant for display purposes.
As with the default case mappings, Unicode defines default case fold mappings ("case folding") for each Unicode code point.


<hr><h2 id="Fuzzy_String_Matching">Fuzzy String Matching</h2><hr>
29/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/logannc/fuzzywuzzy-rs">https://github.com/logannc/fuzzywuzzy-rs</a>
Rust port of the Python fuzzywuzzy
<a target="_blank" rel="noopener noreferrer" href="https://github.com/seatgeek/fuzzywuzzy">https://github.com/seatgeek/fuzzywuzzy</a> --&gt; moved to <a target="_blank" rel="noopener noreferrer" href="https://github.com/seatgeek/thefuzz">https://github.com/seatgeek/thefuzz</a>


<hr><h2 id="Levenshtein_distance_and_string_similarity">Levenshtein distance and string similarity</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ztane/python-Levenshtein/">https://github.com/ztane/python-Levenshtein/</a>
The Levenshtein Python C extension module contains functions for fast computation of Levenshtein distance and string similarity


<hr><h2 id="String_comparison">String comparison</h2><hr>
31/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/49662585/how-do-i-compare-a-unicode-string-that-has-different-bytes-but-the-same-value">https://stackoverflow.com/questions/49662585/how-do-i-compare-a-unicode-string-that-has-different-bytes-but-the-same-value</a>
A pair NFC considers different but a user might consider the same is '¬µ' (MICRO SIGN) and 'Œº' (GREEK SMALL LETTER MU).
NFKC will collapse these two.


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/">https://www.unicode.org/reports/tr10/</a>
Unicode¬Æ Technical Standard #10
UNICODE COLLATION ALGORITHM
Collation is the general term for the process and function of determining the sorting order of strings of characters.
Collation varies according to language and culture: Germans, French and Swedes sort the same characters differently.
It may also vary by specific application: even within the same language, dictionaries may sort differently than phonebooks or book indices.
For non-alphabetic scripts such as East Asian ideographs, collation can be either phonetic or based on the appearance of the character.
Collation can also be customized according to user preference, such as ignoring punctuation or not, putting uppercase before lowercase (or vice versa), and so on.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Unicode_equivalence">https://en.wikipedia.org/wiki/Unicode_equivalence</a>
Short definition of NFD, NFC, NFKD, NFKC

    In this article, a short paragraph which confirms that it's important to keep
    the original string unchanged !
    Errors due to normalization differences
    When two applications share Unicode data, but normalize them differently, errors and data loss can result.
    In one specific instance, OS X normalized Unicode filenames sent from the Samba file- and printer-sharing software.
    Samba did not recognise the altered filenames as equivalent to the original, leading to data loss.[4][5]
    Resolving such an issue is non-trivial, as normalization is not losslessly invertible.
    <a target="_blank" rel="noopener noreferrer" href="http://sourceforge.net/p/netatalk/bugs/348/">http://sourceforge.net/p/netatalk/bugs/348/</a>
    #348 volcharset:UTF8 doesn't work from Mac


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/normalization.html">https://www.unicode.org/faq/normalization.html</a>
Mode detailled description of normalization


PHP
    <a target="_blank" rel="noopener noreferrer" href="http://php.net/manual/en/collator.compare.php">http://php.net/manual/en/collator.compare.php</a>
    Collator::compare -- collator_compare ‚Äî Compare two Unicode strings
    Object oriented style
        public int Collator::compare ( string $str1 , string $str2 )
    Procedural style
        int collator_compare ( Collator $coll , string $str1 , string $str2 )


    <a target="_blank" rel="noopener noreferrer" href="http://php.net/manual/en/class.collator.php">http://php.net/manual/en/class.collator.php</a>
    Provides string comparison capability with support for appropriate locale-sensitive sort orderings.


Swift
    <a target="_blank" rel="noopener noreferrer" href="https://developer.apple.com/library/prerelease/watchos/documentation/Swift/Conceptual/Swift_Programming_Language/StringsAndCharacters.html">https://developer.apple.com/library/prerelease/watchos/documentation/Swift/Conceptual/Swift_Programming_Language/StringsAndCharacters.html</a>
        Two String values (or two Character values) are considered equal if their extended grapheme clusters are canonically equivalent.
        Extended grapheme clusters are canonically equivalent if they have the same linguistic meaning and appearance,
        even if they are composed from different Unicode scalars behind the scenes.

        .characters.count
        for character in dogString.characters
        for codeUnit in dogString.utf8
        for codeUnit in dogString.utf16
        for scalar in dogString.unicodeScalars

        Nothing about ordered comparison in Swift doc ?

    <a target="_blank" rel="noopener noreferrer" href="http://oleb.net/blog/2014/07/swift-strings/">http://oleb.net/blog/2014/07/swift-strings/</a>

        Ordering strings with the &lt; and &gt; operators uses the default Unicode collation algorithm.
        In the example below, "√©" is smaller than i because the collation algorithm specifies
        that characters with combining marks follow right after their base character.
            "r√©sum√©" &lt; "risotto" // -&gt; true
        The String type does not (yet?) come with a method to specify the language to use for collation.
        You should continue to use
            -[NSString compare:options:range:locale:]
        or
            -[NSString localizedCompare:]
        if you need to sort strings that are shown to the user.

        In this example, specifying a locale that uses the German phonebook collation yields a different result than the default string ordering:
            let muffe = "Muffe"
            let m√ºller = "M√ºller"
            muffe &lt; m√ºller // -&gt; true

            // Comparison using an US English locale yields the same result
            let muffeRange = muffe.startIndex..&lt;muffe.endIndex
            let en_US = NSLocale(localeIdentifier: "en_US")
            muffe.compare(m√ºller, options: nil, range: muffeRange, locale: en_US) // -&gt; .OrderedAscending

            // Germany phonebook ordering treats "√º" as "ue".
            // Thus, "M√ºller" &lt; "Muffe"
            let de_DE_phonebook = NSLocale(localeIdentifier: "de_DE@collation=phonebook")
            muffe.compare(m√ºller, options: nil, range: muffeRange, locale: de_DE_phonebook) // -&gt; .OrderedDescending


Java
    <a target="_blank" rel="noopener noreferrer" href="https://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/">https://jcdav.is/2016/09/01/How-the-JVM-compares-your-strings/</a>
    How the JVM compares your strings using the craziest x86 instruction you've never heard of.
    ---
    A comment about this article:
    PCMPxSTRx is no longer faster than equivalent "simple" vector instruction sequences for straightforward comparisons
    (this had already been the case for a few years when that article was written, which is curious).
    It can be used productively (with some care) for some other operations like substring matching,
    but that's not as much of a heavy-hitter.
    There's a bunch of string stuff that will benefit from general vectorization, and which is absolutely on our roadmap to tackle,
    but using the PCMPxSTRx instructions specifically isn't a source of wins on the most important operations


C#
    <a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/dotnet/standard/base-types/comparing">https://docs.microsoft.com/en-us/dotnet/standard/base-types/comparing</a>
    <a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/dotnet/core/extensions/performing-culture-insensitive-string-comparisons">https://docs.microsoft.com/en-us/dotnet/core/extensions/performing-culture-insensitive-string-comparisons</a>


<hr><h2 id="JSON">JSON</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/programming/comments/q5vmxc/parsing_json_is_a_minefield_2018/">https://www.reddit.com/r/programming/comments/q5vmxc/parsing_json_is_a_minefield_2018/</a>
<a target="_blank" rel="noopener noreferrer" href="https://seriot.ch/projects/parsing_json.html">https://seriot.ch/projects/parsing_json.html</a>
Parsing JSON is a Minefield
Search for "unicode"


30/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://datatracker.ietf.org/doc/html/rfc8259">https://datatracker.ietf.org/doc/html/rfc8259</a>
The JavaScript Object Notation (JSON) Data Interchange Format
See this section about strings and encoding:
<a target="_blank" rel="noopener noreferrer" href="https://datatracker.ietf.org/doc/html/rfc8259#section-7">https://datatracker.ietf.org/doc/html/rfc8259#section-7</a>


<hr><h2 id="TOML_serialization_format">TOML serialization format</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/toml-lang/toml">https://github.com/toml-lang/toml</a>
Tom's Obvious, Minimal Language
TOML is a nice serialization format for human-maintained data structures.
It‚Äôs line-delimited and‚Äîof course!‚Äîallows comments, and any Unicode code point can be expressed in simple hexadecimal.
TOML is fairly new, and its specification is still in flux;


<hr><h2 id="CBOR_Concise_Binary_Representation">CBOR Concise Binary Representation</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://cbor.io/">https://cbor.io/</a>
RFC 8949 Concise Binary Object Representation
CBOR improves upon JSON‚Äôs efficiency and also allows for storage of binary strings.
Whereas JSON encoders must stringify numbers and escape all strings,
CBOR stores numbers ‚Äúliterally‚Äù and prefixes strings with their length,
which obviates the need to escape those strings.


<a target="_blank" rel="noopener noreferrer" href="https://www.rfc-editor.org/rfc/rfc8949.html">https://www.rfc-editor.org/rfc/rfc8949.html</a>
RFC 8949 Concise Binary Object Representation (CBOR)
In contrast to formats such as JSON, the Unicode characters in this type are never escaped.
Thus, a newline character (U+000A) is always represented in a string as the byte 0x0a,
and never as the bytes 0x5c6e (the characters "\" and "n")
nor as 0x5c7530303061 (the characters "\", "u", "0", "0", "0", and "a").


<hr><h2 id="Binary_encoding_in_Unicode">Binary encoding in Unicode</h2><hr>
10/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://qntm.org/unicodings">https://qntm.org/unicodings</a>
Efficiently encoding binary data in Unicode
in UTF-8, use Base64 or Base85
in UTF-16, use Base32768
in UTF-32, use Base65536


<a target="_blank" rel="noopener noreferrer" href="https://qntm.org/safe">https://qntm.org/safe</a>
What makes a Unicode code point safe?


<a target="_blank" rel="noopener noreferrer" href="https://github.com/qntm/safe-code-point">https://github.com/qntm/safe-code-point</a>
Ascertains whether a Unicode code point is 'safe' for the purposes of encoding binary data


<a target="_blank" rel="noopener noreferrer" href="https://github.com/qntm/base2048">https://github.com/qntm/base2048</a>
Binary encoding optimised for Twitter
Originally, Twitter allowed Tweets to be at most 140 characters.
On 26 September 2017, Twitter allowed 280 characters.
Maximum Tweet length is indeed 280 Unicode code points.
Twitter divides Unicode into 4,352 "light" code points (U+0000 to U+10FF inclusive)
and 1,109,760 "heavy" code points (U+1100 to U+10FFFF inclusive).
Base2048 solely uses light characters, which means a new "long" Tweet can contain
at most 280 characters of Base2048. Base2048 is an 11-bit encoding, so those 280
characters encode 3080 bits i.e. 385 octets of data, significantly better than Base65536.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/qntm/base65536">https://github.com/qntm/base65536</a>
Unicode's answer to Base64
Base2048 renders Base65536 obsolete for its original intended purpose of sending
binary data through Twitter.
However, Base65536 remains the state of the art for sending binary data through
text-based systems which naively count Unicode code points, particularly those
using the fixed-width UTF-32 encoding.


<hr><h2 id="Invalid_format">Invalid format</h2><hr>
22/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/52131881/does-the-winapi-ever-validate-utf-16">https://stackoverflow.com/questions/52131881/does-the-winapi-ever-validate-utf-16</a>
Does the WinApi ever validate UTF-16?
Windows wide characters are arbitrary 16-bit numbers (formerly called "UCS-2",
before the Unicode Standard Consortium purged that notation). So you cannot
assume that it will be a valid UTF-16 sequence. (MultiByteToWideChar is a
notable exception that does return only UTF-16)


28/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://invisible-island.net/xterm/bad-utf8/">https://invisible-island.net/xterm/bad-utf8/</a>
Unicode replacement character in the Linux console.
This test text examines, how UTF-8 decoders handle various types of
corrupted or otherwise interesting UTF-8 sequences.
jlf : difficult to understand what is the conclusion...
What I notice in this review is :
Unicode 10.0.0's chapter 3 (June 2017): each of the ill-formed code units is separately replaced by U+FFFD.
That recommendation first appeared in Unicode 6's chapter 3 on conformance (February 2011).
However the comments about ‚Äúbest practice‚Äù were removed in Unicode 11.0.0 (June 2018).
The W3C WHATWG page entitled Encoding Standard started in January 2013.
    The constraints in the utf-8 decoder above match ‚ÄúBest Practices for Using
    U+FFFD‚Äù from the Unicode standard. No other behavior is permitted per the
    Encoding Standard (other algorithms that achieve the same result are
    obviously fine, even encouraged).
Although Unicode withdrew the recommendation more than two years ago, to date (August 2020) that is not yet corrected in the WHATWG page.


30/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/broken-utf-8/">https://hsivonen.fi/broken-utf-8/</a>
---
The Unicode Technical Committee retracted the change in its meeting on August 3
2017, so the concern expressed below is now moot.
---
Not all byte sequences are valid UTF-8. When decoding potentially invalid UTF-8
input into a valid Unicode representation, something has to be done about invalid input.
The na√Øve answer is to ignore invalid input until finding valid input again (i.e.
finding the next byte that has a lead-byte value), but this is dangerous and
should never be done. The danger is that silently dropping bogus bytes might
make a string that didn‚Äôt look dangerous with the bogus bytes present become
valid active content. Most simply, &lt;scrÔøΩipt&gt; (ÔøΩ standing in for a bogus byte)
could become &lt;script&gt; if the error is ignored. So it‚Äôs non-controversial that
every sequence of bogus bytes should result in at least one REPLACEMENT CHARACTER
and that the next lead-valued byte is the first byte that‚Äôs no longer part of
the invalid sequence.
But how many REPLACEMENT CHARACTERs should be generated for a sequence of
multiple bogus bytes?
jlf: the answer is not clear to me...


<a target="_blank" rel="noopener noreferrer" href="https://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt">https://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt</a>
UTF-8 decoder capability and stress test


<hr><h2 id="Mojibake">Mojibake</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/LuminosoInsight/python-ftfy">https://github.com/LuminosoInsight/python-ftfy</a>
ftfy can fix mojibake (encoding mix-ups), by detecting patterns of characters
that were clearly meant to be UTF-8 but were decoded as something else


03/07/2021
Notebook in python-ftfy:
Services such as Slack and Discord don't use Unicode for their emoji.
They use ASCII strings like :green-heart: and turn them into images.
These won't help you test anything.
I recommend getting emoji for your test cases by copy-pasting them from emojipedia.org.
<a target="_blank" rel="noopener noreferrer" href="https://emojipedia.org/">https://emojipedia.org/</a>


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Mojibake">https://en.wikipedia.org/wiki/Mojibake</a>


<hr><h2 id="Filenames">Filenames</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://opensource.apple.com/source/subversion/subversion-52/subversion/notes/unicode-composition-for-filenames.auto.html">https://opensource.apple.com/source/subversion/subversion-52/subversion/notes/unicode-composition-for-filenames.auto.html</a>
2 problems follow:
 1) We can't generally depend on the OS to give us back the
     exact filename we gave it
 2) The same filename may be encoded in different codepoints


<a target="_blank" rel="noopener noreferrer" href="https://linux.die.net/man/1/convmv">https://linux.die.net/man/1/convmv</a>
convmv - converts filenames from one encoding to another


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33986655">https://news.ycombinator.com/item?id=33986655</a>
jlf: discussion about text vs byte for filenames


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33991506">https://news.ycombinator.com/item?id=33991506</a>
Python already has the "surrogateescape" error handler [0] that performs
something similar to what you described: undecodable bytes are translated into
unpaired U+DC80 to U+DCFF surrogates. Of course, this isn't standardized in any
way, but I've found it useful myself for smuggling raw pathnames through Java.
[0] <a target="_blank" rel="noopener noreferrer" href="https://peps.python.org/pep-0383/">https://peps.python.org/pep-0383/</a>


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33988943">https://news.ycombinator.com/item?id=33988943</a>
I‚Äôm a little confused, how can a file name be non-decodable? A file with that
name exists, so someone somewhere knows how to decode it. Why wouldn‚Äôt Python
just always use the same encoding as the OS it‚Äôs running on? Is this some
locale-related thing?
---
    &gt; A file with that name exists, so someone somewhere knows how to decode it.
    No. A unix filename is just a bunch of bytes (two of them being off-limits).
    There is no requirement that it be in any encoding.
    You can always use a fallback encoding (an iso-8859) to get something out of
    the garbage, but it's just that, garbage.
    Windows has a similar issue, NTFS paths are sequences of UCS2 code units, but
    there's no guarantee that they form any sort of valid UTF-16 string, you can
    find random lone surrogates for instance.
    And I'm sure network filesystems have invented their own even worse issues,
    because being awful is what they do.

    &gt; Why wouldn‚Äôt Python just always use the same encoding as the OS it‚Äôs running on?
    1. because OS don't really have encodings, Python has a function to try and
       retrieve FS encoding[0] but per the above there's no requirement that it
       is correct for any file, let alone the one you actually want to open
       (hell technically speaking it's not even a property of the FS)
    2. because OS lie and user configurations are garbage, you can't even trust
       the user's locale to be configured properly for reading files (an other
       mistake Python 3 made, incidentally)
    3. because the user may not even have created the file, it might come from a
       broken archive, or some random download from someone having fun with
       filenames, or from fetching crap from an FTP or network share

    There are a few FS / FS configurations which are reliable, in that case they
    either error or pre-mangle the files on intake.
    IIRC ZFS can be configured to only accept valid UTF-8 filenames, HFS(+)
    requires valid unicode (stored as UTF-16) and APFS does as well (stored as UTF-8).

    [0] <a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/sys.html#sys.getfilesystem...">https://docs.python.org/3/library/sys.html#sys.getfilesystem...</a>


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33986421">https://news.ycombinator.com/item?id=33986421</a>
Stefan Karpinski:
On UNIX, paths are UTF-8 by convention, but not forced to be valid. Treating
paths as UTF-8 works very well as long as you hadn't also make the mistake of
requiring your UTF-8 strings to be valid (which Python did, unfortunately).
On Windows, paths are UTF-16 by convention, but also not forced to be valid.
However, invalid UTF-16 can be faithfully converted to WTF-8 and converted back
losslessly, so you can translate Windows paths to WTF-16 and everything Just
Works‚Ñ¢ [1].
There aren't any operating systems I'm aware of where paths are actually
Shift-JIS by convention, so that seems like a non-issue. Using "UTF-8 by
convention" strings works on all modern OSes.

    [1] Ok, here's why the WTF-8 thing works so well. If we write WTF-16 for
    potentially invalid UTF-16 (just arbitrary sequences of 16-bit code units), then
    the mapping between WTF-16 and WTF-8 space is a bijection because it's
    losslessly round-trippable. But more importantly, this WTF-8/16 bijection is
    also a homomorphism with respect to pretty much any string operation you can
    think of. For example `utf16_concat(a, b) == utf8_concat(wtf8(a), wtf8(b))` for
    arbitrary UTF-16 strings a and b. Similar identities hold for other string
    operations like searching for substrings or splitting on specific strings.
---
&gt; There aren't any operating systems I'm aware of where paths are actually
  Shift-JIS by convention, so that seems like a non-issue. Using "UTF-8 by
  convention" strings works on all modern OSes.
Nonsense. Unix paths use the system locale by convention, and it's entirely
normal for that to be Shift-JIS.


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33985510">https://news.ycombinator.com/item?id=33985510</a>
Stefan Karpinski:
Absolutely right. Deprecating direct string indexing would have been the right
move. Require writing `str.chars()` to get something that lets you slice by
Unicode characters (i.e. code points); provide `str.graphemes()` and
`str.grapheme_clusters()` to get something that lets you slice by graphemes and
grapheme clusters, respectively. Cache an index structure that lets you do that
kind of indexing efficiently once you've asked for it the first time. Provide an
API to clear the caches.
Not allowing strings to represent invalid Unicode is also a huge mistake (and
essentially forced by the representation strategy that they adopted). It forces
any programmer who wants to robustly handle potentially invalid string data to
use byte vectors instead. Which is exactly what they did with OS paths, but
that's far from the only place you can get invalid strings. You can get invalid
strings almost anywhere! Worse, since it's incredibly inconvenient to work with
byte vectors when you want to do stringlike stuff, no one does it unless forced
to, so this design choice effectively guarantees that all Python code that works
with strings will blow up if it encounters anything invalid‚Äîwhich is a very
common occurrence.
If only there was a type that behaves like a string and supports all the handy
string operations but which handles invalid data gracefully. Then you could
write robust string code conveniently. But at that point, you should just make
that the standard string type! This isn't hypothetical, it's exactly how Burnt
Sushi's bstr type [1] works in Rust and how the standard String type works in
Julia.
[1] <a target="_blank" rel="noopener noreferrer" href="https://github.com/BurntSushi/bstr">https://github.com/BurntSushi/bstr</a>
---
Jasper_
It's worth noting that Python str's are sequences of code points, not scalar
values. This was a truly horrendous mistake made mostly out of ignorance, but
now they rely upon it in surrogateescape to hide "invalid" data, so...
I have ranted for long hours go friends about the insanity of Python 3's text
model before. It's mostly the blind leading the blind.
---
Animats:
Unicode string indexing should have been made lazy, rather than deprecated.
Random access to strings is rare. Mostly, operations are moving forward linearly
or using saved positions.
So, only build the index for random access if needed. Optimize "advance one
glyph" and "back up one glyph" expressed as indexing, and you'll get most of the
frequently used cases. Have the "index" functions that return a string index
return an opaque type that's a byte index. Attempting to convert that to an
integer forces creation of the string index.
This preserves the user visible semantics but keeps performance.
PyPy does something like this.


<hr><h2 id="WTF8">WTF8</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9611710">https://news.ycombinator.com/item?id=9611710</a>
The WTF-8 encoding (simonsapin.github.io)
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9613971">https://news.ycombinator.com/item?id=9613971</a>
<a target="_blank" rel="noopener noreferrer" href="https://simonsapin.github.io/wtf-8/#acknowledgments">https://simonsapin.github.io/wtf-8/#acknowledgments</a>
Thanks to Coralie Mercier for coining the name WTF-8.
---
The name is unserious but the project is very serious, its writer has responded
to a few comments and linked to a presentation of his on the subject[0].
It's an extension of UTF-8 used to bridge UTF-8 and UCS2-plus-surrogates:
while UTF8 is the modern encoding you have to interact with legacy systems,
for UNIX's bags of bytes you may be able to assume UTF8 (possibly ill-formed)
but a number of other legacy systems used UCS2 and added visible surrogates
(rather than proper UTF-16) afterwards.
Windows and NTFS, Java, UEFI, Javascript all work with UCS2-plus-surrogates.
Having to interact with those systems from a UTF8-encoded world is an issue
because they don't guarantee well-formed UTF-16, they might contain unpaired
surrogates which can't be decoded to a codepoint allowed in UTF-8 or UTF-32
(neither allows unpaired surrogates, for obvious reasons).
WTF8 extends UTF8 with unpaired surrogates (and unpaired surrogates only,
paired surrogates from valid UTF16 are decoded and re-encoded to a proper
UTF8-valid codepoint) which allows interaction with legacy UCS2 systems.
WTF8 exists solely as an internal encoding (in-memory representation),
but it's very useful there.
[0] <a target="_blank" rel="noopener noreferrer" href="http://exyr.org/2015/!!Con_WTF-8/slides.pdf">http://exyr.org/2015/!!Con_WTF-8/slides.pdf</a>

<a target="_blank" rel="noopener noreferrer" href="https://twitter.com/koalie/status/506821684687413248">https://twitter.com/koalie/status/506821684687413248</a>
Coralie Mercier
@koalie
I have a hunch we use "wtf-8" encoding.
Appreciate the irony of:
"√É∆í√Ü‚Äô√É‚Äö√Ü‚Äô√É∆í√Ç¬¢√É‚Äö√¢‚Äö¬¨√É‚Äö√Ö¬°√É∆í√Ü‚Äô√É‚Äö√¢‚Ç¨≈°√É∆í√¢‚Ç¨≈°√É‚Äö√Ç the future of publishing at W3C"


16/07/2021
Windows allows unpaired surrogates in filenames


<a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/issues/32334">https://github.com/golang/go/issues/32334</a>
syscall: Windows filenames with unpaired surrogates are not handled correctly #32334


<a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/issues/12056">https://github.com/rust-lang/rust/issues/12056</a>
path: Windows paths may contain non-utf8-representable sequences #12056
I don't know the precise details, but there exist portions of Windows in which
paths are UCS2 rather than UTF-16. I ignored it because I thought it wasn't going
to be an issue but at some point someone (and I wish I could remember who) showed
me some output that showed that they were actually getting a UCS2 path from some
Windows call and Path was unable to parse it.
---
JLF: this is the birth of WTF-8 in 2014.
The result is:
<a target="_blank" rel="noopener noreferrer" href="https://simonsapin.github.io/wtf-8">https://simonsapin.github.io/wtf-8</a>


<hr><h2 id="Codepoint_grapheme_indexation">Codepoint/grapheme indexation</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://nullprogram.com/blog/2019/05/29/">https://nullprogram.com/blog/2019/05/29/</a>


ObjectIcon
    <a target="_blank" rel="noopener noreferrer" href="http://objecticon.sourceforge.net/Unicode.html">http://objecticon.sourceforge.net/Unicode.html</a>
    ucs (standing for Unicode character string) is a new builtin type, whose behaviour closely mirrors
    that of the conventional Icon string. It operates by providing a wrapper around a conventional
    conventional Icon string, which must be in utf-8 format. This has several advantages, and only one
    serious disadvantage, namely that a utf-8 string is not randomly accessible, in the sense that one
    cannot say where the representation for unicode character i begins. To alleviate this disadvantage,
    the ucs type maintains an index of offsets into the utf-8 string to make random access faster. The
    size of the index is only a few percent of the total allocation for the ucs object.
Jlf: I made a code review, but could not understand how they do that :-(
Not clear if it's a codepoint indexation or a grapheme indexation.


<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/864994/">https://lwn.net/Articles/864994/</a>
jlf: discussion about Raku NFG and its technical limitations.
It's also the traditional discussion about "why do you need a direct access to the graphemes".


<hr><h2 id="Rope">Rope</h2><hr>
See also ZenoString (from Alan Kay - Saxonica)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/josephg/librope">https://github.com/josephg/librope</a>
Little C library for heavyweight utf-8 strings (rope).


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=8065608">https://news.ycombinator.com/item?id=8065608</a>
Discussion about ropes, ideal of strings...


<a target="_blank" rel="noopener noreferrer" href="https://github.com/xi-editor/xi-editor/blob/e8065a3993b80af0aadbca0e50602125d60e4e38/doc/rope_science/rope_science_03.md">https://github.com/xi-editor/xi-editor/blob/e8065a3993b80af0aadbca0e50602125d60e4e38/doc/rope_science/rope_science_03.md</a>


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=34948308">https://news.ycombinator.com/item?id=34948308</a>
Several references to older papers


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37820532">https://news.ycombinator.com/item?id=37820532</a>
Text showdown: Gap Buffers vs. Ropes


<a target="_blank" rel="noopener noreferrer" href="https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation">https://code.visualstudio.com/blogs/2018/03/23/text-buffer-reimplementation</a>
Text Buffer Reimplementation


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Piece_table">https://en.wikipedia.org/wiki/Piece_table</a>
In computing, a piece table is a data structure typically used to represent a
text document while it is edited in a text editor.


<hr><h2 id="Encoding_title">Encoding title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.iana.org/assignments/character-sets/character-sets.xhtml">https://www.iana.org/assignments/character-sets/character-sets.xhtml</a>
Character Sets
(IANA Character Sets registry)
These are the official names for character sets that may be used in
the Internet and may be referred to in Internet documentation.  These
names are expressed in ANSI_X3.4-1968 which is commonly called
US-ASCII or simply ASCII.  The character set most commonly use in the
Internet and used especially in protocol standards is US-ASCII, this
is strongly encouraged.  The use of the name US-ASCII is also
encouraged.
---
jlf: see encoding.spec.whatwg.org elsewhere in this document. They say:
"User agents have also significantly deviated from the labels listed in the
IANA Character Sets registry. To stop spreading legacy encodings further,
this specification is exhaustive about the aforementioned details and therefore
has no need for the registry."


<a target="_blank" rel="noopener noreferrer" href="https://thephd.dev/the-c-c++-rust-string-text-encoding-api-landscape">https://thephd.dev/the-c-c++-rust-string-text-encoding-api-landscape</a>
OCTOBER 12, 2022
JeanHeyd Meneide
Project Editor for ISO/IEC JTC1 SC22 WG14 - Programming Languages, C.
The Wonderfully Terrible World of C and C++ Encoding APIs (with Some Rust)
---
jlf:
Is he criticizing the work of Zach Laine? ( <a target="_blank" rel="noopener noreferrer" href="https://github.com/tzlaine/text">https://github.com/tzlaine/text</a> )
"someone was doing something wrong on the internet and I couldn‚Äôt let that pass:"

Same person:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ThePhD">https://github.com/ThePhD</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/soasis">https://github.com/soasis</a>

Any Encoding, Ever - ztd.text and Unicode for C++ - JUNE 30, 2021 : <a target="_blank" rel="noopener noreferrer" href="https://thephd.dev/any-encoding-ever-ztd-text-unicode-cpp">https://thephd.dev/any-encoding-ever-ztd-text-unicode-cpp</a>

Starting a Basis - Shepherd's Oasis and Text - MAY 01, 2020: <a target="_blank" rel="noopener noreferrer" href="https://thephd.dev/basis-shepherds-oasis-text-encoding">https://thephd.dev/basis-shepherds-oasis-text-encoding</a>

<a target="_blank" rel="noopener noreferrer" href="https://ztdtext.readthedocs.io/en/latest/index.html">https://ztdtext.readthedocs.io/en/latest/index.html</a>
ztd.text
The premiere library for handling text in different encoding forms and reducing
transcoding bugs in your C++ software.
List of encodings: <a target="_blank" rel="noopener noreferrer" href="https://ztdtext.readthedocs.io/en/latest/encodings.html">https://ztdtext.readthedocs.io/en/latest/encodings.html</a>
List of Unicode encodings: <a target="_blank" rel="noopener noreferrer" href="https://ztdtext.readthedocs.io/en/latest/known%20unicode%20encodings.html">https://ztdtext.readthedocs.io/en/latest/known%20unicode%20encodings.html</a>
Design Goals and Philosophy: <a target="_blank" rel="noopener noreferrer" href="https://ztdtext.readthedocs.io/en/latest/design.html">https://ztdtext.readthedocs.io/en/latest/design.html</a>
---
jlf: don't know what to think about that...
related to <a target="_blank" rel="noopener noreferrer" href="https://github.com/soasis">https://github.com/soasis</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/soasis/text">https://github.com/soasis/text</a>
JeanHeyd Meneide
This repository is an implementation of an up and coming proposal percolating
through SG16, P1629 - Standard Text Encoding
( <a target="_blank" rel="noopener noreferrer" href="https://thephd.dev/_vendor/future_cxx/papers/d1629.html">https://thephd.dev/_vendor/future_cxx/papers/d1629.html</a> )
---
<a target="_blank" rel="noopener noreferrer" href="https://github.com/soasis">https://github.com/soasis</a>
Shepherd's Oasis
Software Services and Consulting.


<a target="_blank" rel="noopener noreferrer" href="https://encoding.spec.whatwg.org/">https://encoding.spec.whatwg.org/</a>
Encoding
The Encoding Standard defines encodings and their JavaScript API.
---
The table below lists all encodings and their labels user agents must support.
User agents must not support any other encodings or labels.
&lt;table&gt;
---
Most legacy encodings make use of an index. An index is an ordered list of entries,
each entry consisting of a pointer and a corresponding code point. Within an index
pointers are unique and code points can be duplicated.
Note: An efficient implementation likely has two indexes per encoding.
One optimized for its decoder and one for its encoder.


<a target="_blank" rel="noopener noreferrer" href="https://www.git-tower.com/help/guides/faq-and-tips/faq/encoding/windows">https://www.git-tower.com/help/guides/faq-and-tips/faq/encoding/windows</a>
Character encoding for commit messages
---
When Git creates and stores a commit, the commit message entered by the user is
stored as binary data and there is no conversion between encodings. The encoding
of your commit message is determined by the client you are using to compose the
commit message.
Git stores the name of the commit encoding if the config key "i18n.commitEncoding"
is set (and if it's not the default value "utf-8").
If you commit changes from the command line, this value must match the encoding
set in your shell environment. Otherwise, a wrong encoding is stored with the
commit and can result in garbled output when viewing the commit history.
If you view the commit log on the command line, the config value "i18n.logOutputEncoding"
(which defaults to "i18n.commitEncoding") needs to match your shell encoding as well.
The command converts messages from the commit encoding to the output encoding.
If your shell encoding does not match the output encoding, you will again receive
garbled output!


<a target="_blank" rel="noopener noreferrer" href="https://www.git-scm.com/docs/gitattributes/2.18.0#_working_tree_encoding">https://www.git-scm.com/docs/gitattributes/2.18.0#_working_tree_encoding</a>
gitattributes - Defining attributes per path
working-tree-encoding
Git recognizes files encoded in ASCII or one of its supersets (e.g. UTF-8,
ISO-8859-1, ‚Ä¶‚Äã) as text files. Files encoded in certain other encodings (e.g.
UTF-16) are interpreted as binary and consequently built-in Git text processing
tools (e.g. git diff) as well as most Git web front ends do not visualize the
contents of these files by default.
In these cases you can tell Git the encoding of a file in the working directory
with the working-tree-encoding attribute. If a file with this attribute is added
to Git, then Git reencodes the content from the specified encoding to UTF-8.
Finally, Git stores the UTF-8 encoded content in its internal data structure
(called "the index"). On checkout the content is reencoded back to the specified
encoding.
---
jlf: there is a number of pitfalls, read the article.


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/436220/how-to-determine-the-encoding-of-text">https://stackoverflow.com/questions/436220/how-to-determine-the-encoding-of-text</a>
How to determine the encoding of text
jlf: for Python, not reviewed, may bring interesting infos.


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/90838/how-can-i-detect-the-encoding-codepage-of-a-text-file">https://stackoverflow.com/questions/90838/how-can-i-detect-the-encoding-codepage-of-a-text-file</a>
How can I detect the encoding/codepage of a text file?
jlf: for C#, not reviewed, may bring interesting infos.


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/7048745/what-is-the-difference-between-utf-8-and-iso-8859-1">https://stackoverflow.com/questions/7048745/what-is-the-difference-between-utf-8-and-iso-8859-1</a>
What is the difference between UTF-8 and ISO-8859-1?
jlf: the interesting part are the comments about ISO-8859-1.
---
ISO 8859-1 is a single-byte encoding that can represent the first 256 Unicode characters.
---
cp1252 is a superset of the ISO-8859-1, containing additional printable
characters in the 0x80-0x9F range, notably the Euro symbol ‚Ç¨ and the much
maligned "smart quotes" ‚Äú‚Äù. This frequently leads to a mismatch where 8859-1 can
be displayed as 1252 perfectly fine, and 1252 may seem to display fine as 8859-1,
but will misbehave when one of those extra symbols shows up.
---
jlf: so the previous comment says that ISO-8859-1 is not defined in the 0x80-0x9F
range... IS IT or IS IT NOT???
---
One more important thing to realise: if you see iso-8859-1, it probably refers
to Windows-1252 rather than ISO/IEC 8859-1. They differ in the range 0x80‚Äì0x9F,
where ISO 8859-1 has the C1 control codes, and Windows-1252 has useful visible
characters instead.
For example, ISO 8859-1 has 0x85 as a control character (in Unicode, U+0085, ``),
while Windows-1252 has a horizontal ellipsis (in Unicode, U+2026 HORIZONTAL ELLIPSIS, ‚Ä¶).
The WHATWG Encoding spec (as used by HTML) expressly declares iso-8859-1 to be
a label for windows-1252, and web browsers do not support ISO 8859-1 in any way.


<a target="_blank" rel="noopener noreferrer" href="https://www.mobilefish.com/tutorials/character_encoding/character_encoding_quickguide_iso8859_1.html">https://www.mobilefish.com/tutorials/character_encoding/character_encoding_quickguide_iso8859_1.html</a>
jlf: not sure this page is a good reference. The fact they wrote "Unicode, a 16-bit character set."
brings a doubt about the rest of their page...
I reference it for their definition of ISO-8859-1.
---
HTML and HTTP protocols make frequent reference to ISO Latin-1 and the character
code ISO-8859-1. The HTTP specification mandates the use of the code ISO-8859-1
as the default character code that is passed over the network.
ISO-8859-1 explicitly does not define displayable characters for positions 0-31
and 127-159, and the HTML standard does not allow those to be used for displayable
characters. The only characters in this range that are used are 9, 10 and 13,
which are tab, newline and carriage return respectively.
Note: ISO-8859-1 is also known as Latin-1.
---
jlf: so they say
- 00..1F is not defined except 09, 0A, 0D (so they are different from
 <a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/ISO/IEC_8859-1)">https://en.wikipedia.org/wiki/ISO/IEC_8859-1)</a> where all 00..1F is undefined.
- 7F..9F is not defined
Confirmed by their text file:
<a target="_blank" rel="noopener noreferrer" href="https://www.mobilefish.com/download/character_set/iso8859_1.txt">https://www.mobilefish.com/download/character_set/iso8859_1.txt</a>


<hr><h2 id="ICU_title">ICU title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org">https://icu.unicode.org</a>


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/">https://unicode-org.github.io/icu/</a>
ICU documentation


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu-docs/apidoc/released/icu4c/">https://unicode-org.github.io/icu-docs/apidoc/released/icu4c/</a>
Entry point of API Reference


<a target="_blank" rel="noopener noreferrer" href="https://icu-project.org/docs/">https://icu-project.org/docs/</a>
ICU Documents and Papers
jlf: old?


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.atlassian.net/jira/software/c/projects/ICU/issues/?filter=allissues">https://unicode-org.atlassian.net/jira/software/c/projects/ICU/issues/?filter=allissues</a>
ICU tickets


<a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/icu">https://github.com/microsoft/icu</a>
jlf: fork by Microsoft


<a target="_blank" rel="noopener noreferrer" href="http://stackoverflow.com/questions/8253033/what-open-source-c-or-c-libraries-can-convert-arbitrary-utf-32-to-nfc">http://stackoverflow.com/questions/8253033/what-open-source-c-or-c-libraries-can-convert-arbitrary-utf-32-to-nfc</a>
What open source C or C++ libraries can convert arbitrary UTF-32 to NFC?

std::string normalize(const std::string &amp;unnormalized_utf8) {
    // FIXME: until ICU supports doing normalization over a UText
    // interface directly on our UTF-8, we'll use the insanely less
    // efficient approach of converting to UTF-16, normalizing, and
    // converting back to UTF-8.

    // Convert to UTF-16 string
    auto unnormalized_utf16 = icu::UnicodeString::fromUTF8(unnormalized_utf8);

    // Get a pointer to the global NFC normalizer
    UErrorCode icu_error = U_ZERO_ERROR;
    const auto *normalizer = icu::Normalizer2::getInstance(nullptr, "nfc", UNORM2_COMPOSE, icu_error);
    assert(U_SUCCESS(icu_error));

    // Normalize our string
    icu::UnicodeString normalized_utf16;
    normalizer-&gt;normalize(unnormalized_utf16, normalized_utf16, icu_error);
    assert(U_SUCCESS(icu_error));

    // Convert back to UTF-8
    std::string normalized_utf8;
    normalized_utf16.toUTF8String(normalized_utf8);

    return normalized_utf8;
}


<a target="_blank" rel="noopener noreferrer" href="https://begriffs.com/posts/2019-05-23-unicode-icu.html">https://begriffs.com/posts/2019-05-23-unicode-icu.html</a>
Unicode programming, with examples


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Trie">https://en.wikipedia.org/wiki/Trie</a>
Tries are a form of string-indexed look-up data structure, which is used to store a dictionary list
of words that can be searched on in a manner that allows for efficient generation of completion lists.
Tries can be efficacious on string-searching algorithms such as predictive text,
approximate string matching, and spell checking in comparison to a binary search trees.
A trie can be seen as a tree-shaped deterministic finite automaton.


<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org/design/struct/utrie">https://icu.unicode.org/design/struct/utrie</a>
ICU Code Point Tries
We use a form of "trie" adapted to single code points.
The bits in the code point integer are divided into two or more parts.
The first part is used as an array offset, the value there is used as a start offset into another array.
The next code point bit field is used as an additional offset into that array, to fetch another value.
The final part yields the data for the code point.
Non-final arrays are called index arrays or tables.
---
For a general-purpose structure, we want to be able to be able to store a unique value for every character.
This determines the number of bits needed in the last index table.
With 136,690 characters assigned in Unicode 10, we need at least 18 bits.
We allocate data values in blocks aligned at multiples of 4, and we use 16-bit index words shifted left by 2 bits.
This leads to a small loss in how densely the data table can be used, and how well it can be compacted, but not nearly as much as if we were using 32-bit index words.


<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org/design/struct/tries/bytestrie">https://icu.unicode.org/design/struct/tries/bytestrie</a>
It maps from arbitrary byte sequences to 32-bit integers.
(Small non-negative integers are stored more efficiently. Negative integers are the least efficient.)
The BytesTrie and UCharsTrie structures are nearly the same, except that the UCharsTrie uses fewer, larger units.


<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org/design/struct/tries/ucharstrie">https://icu.unicode.org/design/struct/tries/ucharstrie</a>
Same design as a BytesTrie, but mapping any UnicodeString (any sequence of 16-bit units) to 32-bit integer values.


<a target="_blank" rel="noopener noreferrer" href="https://icu.unicode.org/charts/charset">https://icu.unicode.org/charts/charset</a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu/blob/main/icu4c/source/data/mappings/convrtrs.txt">https://github.com/unicode-org/icu/blob/main/icu4c/source/data/mappings/convrtrs.txt</a>
    ICU alias table
    jlf: the ultimate reference?
    ---
    # Here is the file format using BNF-like syntax:
    #
    # converterTable ::= tags { converterLine* }
    # converterLine ::= converterName [ tags ] { taggedAlias* }'\n'
    # taggedAlias ::= alias [ tags ]
    # tags ::= '{' { tag+ } '}'
    # tag ::= standard['*']
    # converterName ::= [0-9a-zA-Z:_'-']+
    # alias ::= converterName
    ---
    standard
    # The * after the standard tag denotes that the previous alias is the
    # preferred (default) charset name for that standard. There can only
    # be one of these default charset names per converter.
    ---
    Affinity tags
    If an alias is given to more than one converter, it is considered to be an
    ambiguous alias, and the affinity list will choose the converter to use when
    a standard isn't specified with the alias.
    The general ordering is from specific and frequently used to more general
    or rarely used at the bottom.
    {   UTR22           # Name format specified by <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr22/">https://www.unicode.org/reports/tr22/</a>
        IBM             # The IBM CCSID number is specified by ibm-*
        WINDOWS         # The Microsoft code page identifier number is specified by windows-*. The rest are recognized IE names.
        JAVA            # Source: Sun JDK. Alias name case is ignored, but dashes are not ignored.
        IANA            # Source: <a target="_blank" rel="noopener noreferrer" href="http://www.iana.org/assignments/character-sets">http://www.iana.org/assignments/character-sets</a>
        MIME            # Source: <a target="_blank" rel="noopener noreferrer" href="http://www.iana.org/assignments/character-sets">http://www.iana.org/assignments/character-sets</a>
        }



<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu/tree/main/icu4c/source/data/mappings">https://github.com/unicode-org/icu/tree/main/icu4c/source/data/mappings</a>
Encodings


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.atlassian.net/browse/ICU-22422">https://unicode-org.atlassian.net/browse/ICU-22422</a>
Collation folding
jlf: see Markus Scherer feedback


<a target="_blank" rel="noopener noreferrer" href="https://sourceforge.net/p/icu/mailman/icu-design/thread/SN6PR00MB04468327B475F4D6A19CF26FAFFFA%40SN6PR00MB0446.namprd00.prod.outlook.com/#msg38268251">https://sourceforge.net/p/icu/mailman/icu-design/thread/SN6PR00MB04468327B475F4D6A19CF26FAFFFA%40SN6PR00MB0446.namprd00.prod.outlook.com/#msg38268251</a>
[icu-design] Collation Folding Tables
jlf: this is a discussion related to ICU-22422


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/#Collation_Folding">https://www.unicode.org/reports/tr10/#Collation_Folding</a>
Collation Folding
    Matching can be done by using the collation elements, directly, as discussed above.
    However, because matching does not use any of the ordering information, the same
    result can be achieved by a folding. That is, two strings would fold to the same
    string if and only if they would match according to the (tailored) collation.
    For example, a folding for a Danish collation would map both "G√•rd" and "gaard"
    to the same value. A folding for a primary-strength folding would map "Resume"
    and "r√©sum√©" to the same value. That folded value is typically a lowercase string,
    such as "resume".
    jlf:
    Chrome matches "G√•rd" with "gard", but not with "gaard".

    A comparison between folded strings cannot be used for an ordering of strings,
    but it can be applied to searching and matching quite effectively. The data for
    the folding can be smaller, because the ordering information does not need to be
    included. The folded strings are typically much shorter than a sort key, and are
    human-readable, unlike the sort key. The processing necessary to produce the
    folding string can also be faster than that used to create the sort key.


Transliterate "micro sign" to "u" using Transliterator from icu4j
jlf: next is an answer on icu-support@lists.sourceforge.net
<a target="_blank" rel="noopener noreferrer" href="https://sourceforge.net/p/icu/mailman/message/58712806/">https://sourceforge.net/p/icu/mailman/message/58712806/</a>
    On Wed, Dec 13, 2023 at 7:52‚ÄØPM &lt;go.al.ni@gmail.com&gt; wrote:
    &gt; Micro sign transliterated to "m" in one case, but not in another.

    While I don't know enough about the Any-Latin transliteration rules to
    be able to tell you why this happens, the thing that happens is that
    when you have any preceding Greek letter the transliterator will
    afterwards treat also the micro sign (U+00B5) as a Greek letter, while
    it otherwise will leave it as-is, as any other symbol.

    If you want to transliterate only Greek letters you could explicitly
    create a Greek transliterator, which then will always treat also the
    micro sign (U+00B5) as a Greek letter:

    var tr = Transliterator.getInstance("Greek-Latin");

    Or, if you want to first treat any symbols that are also Greek letters
    explicitly as Greek letters and then perform the Any-Latin
    transliteration:

    var tr = Transliterator.getInstance("Greek-Latin; Any-Latin;");

    Or, if you want just Any-Latin but with a special case for the micro
    sign (U+00B5):

    var tr = Transliterator.createFromRules("MyAnyLatin", "¬µ &gt; m;
    ::Any-Latin;", Transliterator.FORWARD);


[icu-support] CollationKey for efficient collation-aware in-place substring comparison

    Question
    <a target="_blank" rel="noopener noreferrer" href="https://sourceforge.net/p/icu/mailman/message/58741675/">https://sourceforge.net/p/icu/mailman/message/58741675/</a>

        I have a question regarding the use of CollationKey
        &lt;<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu-docs/apidoc/dev/icu4j/com/ibm/icu/text/CollationKey.html>">https://unicode-org.github.io/icu-docs/apidoc/dev/icu4j/com/ibm/icu/text/CollationKey.html></a>
        to check whether one string "contains" the other (i.e. right string is
        found anywhere in the left string, accounting for any specified rule-based
        collation using ICU4J). With this, my use case in Java would be something
        like: *contains(String left, String right, String collation)*. Suppose that
        *collation* here is a parameter indicating the collation at hand (for
        example: "Latin1_General_CS_AI"), and is used to get the appropriate
        instance of *com.ibm.icu.text.Collator* (exact routing for this collation
        is handled elsewhere in the codebase).

        Problem description

        Due to the nature of this operation, using *Collator.compare(String,
        String)* proves inefficient for this problem, because it would require
        allocating O(N) substrings of *left *before calling *compare(left.substring(),
        right)*. Suppose N here is the length of the *left* string.

        Example: *contains*("Abƒá", "a", "Latin1_General_CS_AI"); // returns false
          - calls: *collator.compare("A", "a")* // returns false ("A" here is
        "Abƒá".substring(0,1))
          - calls: *collator.compare("b", "a")* // returns false ("b" here is
        "Abƒá".substring(1,2))
          - calls: *collator.compare("ƒá", "a")* // returns false ("ƒá" here is
        "Abƒá".substring(2,3))
        Here, this approach allocates *3 new strings* in order to do the
        comparisons.

        Using CollationKey

        As I understood, *com.ibm.icu.text.CollationKey* is the way to go for
        repeated comparison of strings. Here, I would like to compare strings in a
        way that only requires generating one key for *left* (let's call it
        *leftKey*) and one key for *right* (let's call it *rightKey*), and then
        comparing these arrays in-place, byte per byte.

        However, it doesn't seem that this operation is supported out-of-the-box
        with *CollationKey*. While one can easily use two collation keys
        for equality comparison and collation-aware ordering, I'm not sure if this
        holds for substring operations as well? Given a collation key for "Abƒá", is
        there a constant-time way to obtain collation keys for "A", "b", and "ƒá"?
        Ideally, I would want to only traverse the "Abƒá" collation key (*leftKey*)
        as a plain byte array, and do in-place comparison with the "ƒá" collation
        key (*rightKey*) as a plain byte array. However, it doesn't seem
        straightforward given the structure of the collation key (suffixes, etc.)

        public boolean contains(String left, String right, String collation) {
        &gt;   Collator collator = ...(collation);
        &gt;   // get collation keys
        &gt;   CollationKey leftKey = collator.getCollationKey(left);
        &gt;   CollationKey rightKey = collator.getCollationKey(right);
        &gt;   // get byte arrays
        &gt;   byte[] lBytes = leftKey.toByteArray();
        &gt;   byte[] rBytes = rightKey.toByteArray();
        &gt;   // in-place comparison
        &gt;   for (int i = 0; i &lt;= lBytes.length - rBytes.length; i++) {
        &gt;     if (compareKeys(lBytes, rBytes, i)) {
        &gt;       return true;
        &gt;     }
        &gt;   }
        &gt;   return false;
        &gt; }

        Suppose there's a simple helper function such as:

        &gt; private boolean compareKeys(byte[] lBytes, byte[] rBytes, int offset) {
        &gt;   int len = rBytes.length;
        &gt;   // compare lBytes[i, i+len] to rBytes[0, len] in-place, byte by byte...
        &gt; }

        Could you please provide any support regarding how to implement this
        solution so that it fully takes into account the collation key byte array
        structure? As of now, this simple comparison doesn't work because there are
        some suffixes in both *leftKey* and *rightKey*, so exact comparison is not
        possible, but I'm wondering if there is a way to go around this.

        Alternative

        It turns out that making use of *Collator.compare(Object, **Object**)* instead
        of *Collator.compare(String, **String**)* doesn't prove to be any better
        either, because it does *toString()* anyway, regressing the performance in
        a similar fashion. Ideally, an implementation such as
        *Collator.compare(Character, **Character**)* could do the trick, however
        only under the condition that it would *not allocate* a new *String* for
        the two arguments. This would allow traversing *left* and *right* strings
        and comparing individual characters just by using *String.charAt* (with no
        extra *String* allocation whatsoever).

        However, I don't believe there is currently anything like
        *Collator.compare(**Character**, **Character**)* that works exactly like
        this. So for now, I'm trying to implement this functionality using
        *CollationKey*.

    Answer from Markus Sherer
    <a target="_blank" rel="noopener noreferrer" href="https://sourceforge.net/p/icu/mailman/message/58741856/">https://sourceforge.net/p/icu/mailman/message/58741856/</a>

        Yes, but CollationKey is too low-level, and you would have to compute and
        store the CollationKey for the entire left string at once, which could be
        large.
        ‚ÄúDon't do this at home‚Äù :-)

        Please use class
        &lt;<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu-docs/apidoc/released/icu4j/com/ibm/icu/text/StringSearch.html>">https://unicode-org.github.io/icu-docs/apidoc/released/icu4j/com/ibm/icu/text/StringSearch.html></a>
        StringSearch
        &lt;<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu-docs/apidoc/released/icu4j/com/ibm/icu/text/StringSearch.html>">https://unicode-org.github.io/icu-docs/apidoc/released/icu4j/com/ibm/icu/text/StringSearch.html></a>
        <a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/collation/string-search.html">https://unicode-org.github.io/icu/userguide/collation/string-search.html</a>

        I don't remember if StringSearch automatically loads "search" tailorings;
        it's possible that you may have to request that explicitly.

        <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/tr35-collation.html#Collation_Type_Fallback">https://www.unicode.org/reports/tr35/tr35-collation.html#Collation_Type_Fallback</a>
        <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/#Searching">https://www.unicode.org/reports/tr10/#Searching</a>


<hr><h2 id="ICU_demos">ICU demos</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://icu4c-demos.unicode.org/icu-bin/icudemos">https://icu4c-demos.unicode.org/icu-bin/icudemos</a>
todo: review


<a target="_blank" rel="noopener noreferrer" href="https://icu4c-demos.unicode.org/icu-bin/collation.html">https://icu4c-demos.unicode.org/icu-bin/collation.html</a>
ICU Collation Demo


<a target="_blank" rel="noopener noreferrer" href="https://icu4c-demos.unicode.org/icu-bin/convexp">https://icu4c-demos.unicode.org/icu-bin/convexp</a>
Demo Converter Explorer


<a target="_blank" rel="noopener noreferrer" href="https://icu4c-demos.unicode.org/icu-bin/scompare">https://icu4c-demos.unicode.org/icu-bin/scompare</a>
ICU Unicode String Comparison
Interactive demo application


<hr><h2 id="ICU_bindings">ICU bindings</h2><hr>
02/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://gitlab.pyicu.org/main/pyicu">https://gitlab.pyicu.org/main/pyicu</a>
Python extension wrapping the ICU C++ libraries.


02/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://docs.microsoft.com/en-us/windows/win32/intl/international-components-for-unicode--icu-">https://docs.microsoft.com/en-us/windows/win32/intl/international-components-for-unicode--icu-</a>
In Windows 10 Creators Update, ICU was integrated into Windows, making the C APIs and data publicly accessible.
The version of ICU in Windows only exposes the C APIs.
It is impossible to ever expose the C++ APIs due to the lack of a stable ABI in C++.
Getting started
1) Your application needs to target Windows 10 Version 1703 (Creators Update) or higher.
2) Add in the header:
    #include &lt;icu.h&gt;
3) Link to:
    icu.lib
Example:
    void FormatDateTimeICU()
    {
        UErrorCode status = U_ZERO_ERROR;

        // Create a ICU date formatter, using only the 'short date' style format.
        UDateFormat* dateFormatter = udat_open(UDAT_NONE, UDAT_SHORT, nullptr, nullptr, -1, nullptr, 0, &amp;status);

        if (U_FAILURE(status))
        {
            ErrorMessage(L"Failed to create date formatter.");
            return;
        }

        // Get the current date and time.
        UDate currentDateTime = ucal_getNow();

        int32_t stringSize = 0;

        // Determine how large the formatted string from ICU would be.
        stringSize = udat_format(dateFormatter, currentDateTime, nullptr, 0, nullptr, &amp;status);

        if (status == U_BUFFER_OVERFLOW_ERROR)
        {
            status = U_ZERO_ERROR;
            // Allocate space for the formatted string.
            auto dateString = std::make_unique&lt;UChar[]&gt;(stringSize + 1);

            // Format the date time into the string.
            udat_format(dateFormatter, currentDateTime, dateString.get(), stringSize + 1, nullptr, &amp;status);

            if (U_FAILURE(status))
            {
                ErrorMessage(L"Failed to format the date time.");
                return;
            }

            // Output the formatted date time.
            OutputMessage(dateString.get());
        }
        else
        {
            ErrorMessage(L"An error occured while trying to determine the size of the formatted date time.");
            return;
        }

        // We need to close the ICU date formatter.
        udat_close(dateFormatter);
    }


<a target="_blank" rel="noopener noreferrer" href="http://www.boost.org/doc/libs/1_58_0/libs/locale/doc/html/index.html">http://www.boost.org/doc/libs/1_58_0/libs/locale/doc/html/index.html</a>
Boost.Locale creates the natural glue between the C++ locales framework, iostreams, and the powerful ICU library


<a target="_blank" rel="noopener noreferrer" href="http://blog.lukhnos.org/post/6441462604/using-os-xs-built-in-icu-library-in-your-own">http://blog.lukhnos.org/post/6441462604/using-os-xs-built-in-icu-library-in-your-own</a>
Using OS X‚Äôs Built-in ICU Library in Your Own Project


<hr><h2 id="ICU4X_title">ICU4X title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://icu4x.unicode.org/">https://icu4x.unicode.org/</a>
lead by Shane Carr (<a target="_blank" rel="noopener noreferrer" href="https://www.sffc.xyz)">https://www.sffc.xyz)</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x">https://github.com/unicode-org/icu4x</a>


<a target="_blank" rel="noopener noreferrer" href="https://docs.rs">https://docs.rs</a>
jlf: if there is a version number in the path, you can replace it with "latest"


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/unicode_license.html">https://www.unicode.org/faq/unicode_license.html</a>
jlf: ICU4X uses UNICODE LICENSE V3
The Unicode License is a permissive MIT type of license. However, there are
several additional considerations identified separately in the associated
Unicode Terms of Use (<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/copyright.html).">https://www.unicode.org/copyright.html).</a>
---
Comparison with other licenses:
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses">https://en.wikipedia.org/wiki/Comparison_of_free_and_open-source_software_licenses</a>
jlf: hum... the "unicode license" is not in this table...


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/q4xaig/icu_vs_rust_icu/">https://www.reddit.com/r/rust/comments/q4xaig/icu_vs_rust_icu/</a>
icu vs rust_icu
Oct 10, 2021
---
jlf : here "icu" is ICU4X and rust_icu is another crate.
Well... it's a mess, plenty of separated crates more or less finalized.
There is a comment from an ICU4X committer saying "ICU4X does not have normalization".
Of course, it's now supported but it's to say that ICU4X is far to be as complete
as ICU.


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=35608997">https://news.ycombinator.com/item?id=35608997</a>
ICU4X 1.2: Now With Text Segmentation and More on Low-Resource Devices
MONDAY, APRIL 17, 2023


<a target="_blank" rel="noopener noreferrer" href="http://blog.unicode.org/2022/09/announcing-icu4x-10.html">http://blog.unicode.org/2022/09/announcing-icu4x-10.html</a>
    SEPTEMBER 29, 2022
    Announcing ICU4X 1.0

    This week, after 2¬Ω years of work by Google, Mozilla, Amazon, and community partners,
    the Unicode Consortium has published ICU4X 1.0, its first stable release.
        Lightweight:
        ICU4X is Unicode's first library to support static data slicing and dynamic data loading.
        Portable:
        ICU4X supports multiple programming languages out of the box. ICU4X can be used
        in the Rust programming language natively, with official wrappers in C++ via the
        foreign function interface (FFI) and JavaScript via WebAssembly.

    ICU4X does not seek to replace ICU4C or ICU4J; rather, it seeks to replace the large number
    of non-Unicode, often-unmaintained, often-incomplete i18n libraries that have been written
    to bring i18n to new programming languages and resource-constrained environments

    One of the most visible departures that ICU4X makes from ICU4C and ICU4J is an
    explicit data provider argument on most constructor functions.
    ICU4X team member Manish Goregaokar wrote a blog post series detailing how the zero-copy deserialization works under the covers.
        <a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2022/08/03/zero-copy-1-not-a-yoking-matter/">https://manishearth.github.io/blog/2022/08/03/zero-copy-1-not-a-yoking-matter/</a>
        <a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2022/08/03/zero-copy-2-zero-copy-all-the-things/">https://manishearth.github.io/blog/2022/08/03/zero-copy-2-zero-copy-all-the-things/</a>
        <a target="_blank" rel="noopener noreferrer" href="https://manishearth.github.io/blog/2022/08/03/zero-copy-3-so-zero-its-dot-dot-dot-negative/">https://manishearth.github.io/blog/2022/08/03/zero-copy-3-so-zero-its-dot-dot-dot-negative/</a>
        (jlf: Related to ICU4X, but should I read that ? It's internal Rust stuff)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/cpp.md">https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/cpp.md</a>
Using ICU4X from C++


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/programming/comments/xrmine/the_unicode_consortium_announces_icu4x_10_its_new/">https://www.reddit.com/r/programming/comments/xrmine/the_unicode_consortium_announces_icu4x_10_its_new/</a>
    The C and C++ APIs are header-only, you use them by linking to the icu_capi crate (more on this here).
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/cpp.md">https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/cpp.md</a>
    The C API is just not that idiomatic, so we don't advertise it as much.
    It exists more as a crutch for other languages to be able to call in, and it's optimized for cross language interop.
    That said, it has been pointed out to me that it's not that unidiomatic when you compare it with other large C libraries,
    so perhaps that's okay. We do have some tests that use it directly and it's .... fine to work with.
    Not an amazing experience, not terrible either.
    ---
    jlf: to investigate
    The C wrapper is probably better to use from Executor, because there is no hidden magic for memory management.
    The C++ wrapper is difficult to understand (at least to me, for the moment) because it's modern C++.


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/xrh7h6/announcing_icu4x_10_new_internationalization/">https://www.reddit.com/r/rust/comments/xrh7h6/announcing_icu4x_10_new_internationalization/</a>
    icu_segmenter implements rule based segmentation, so you can actually customize
    the segmentation rules based on your needs by writing some toml and feeding it to datagen.
    The concept of a "character" or "word" has no single cross-linguistic meaning;
    it is not uncommon to need to tailor these algorithms by use case or even just
    the language being used. E.g. handling viramas in Indic scripts as a part of
    grapheme segmentation is a thing people might need, but may also not need,
    and UAX29 doesn't support that at the moment¬π. CLDR contains a bunch of common
    tailorings for specific locales here, but as I mentioned folks may tailor further based on use case.

    Furthermore, icu_segmenter supports dictionary-based segmentation:
    for languages like Japanese and Thai where spaces are not typically used,
    you need a large dictionary to be able to segment them accurately
    (and again, it's language-specific).
    ICU4X's flexible data model means that you don't need to ship your application
    with this data and instead fetch it when it's actually necessary.
    We both support using dictionaries and an LSTM model depending on your code size/data size needs.


<a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/document/d/1ojrOdIchyIHYbg2G9APX8j2p0XtmVLj0f9jPIbFYVUE/edit#heading=h.xy9pq2mk1ypz">https://docs.google.com/document/d/1ojrOdIchyIHYbg2G9APX8j2p0XtmVLj0f9jPIbFYVUE/edit#heading=h.xy9pq2mk1ypz</a>
ICU4X Segmenter Investigation


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/1397">https://github.com/unicode-org/icu4x/issues/1397</a>
Character names
jlf: Not yet supported by ICU4X, too bad... I need that for Executor.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/545">https://github.com/unicode-org/icu4x/issues/545</a>
Reconsider UTF-32 support
jlf: see also the comments about PyICU


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/131">https://github.com/unicode-org/icu4x/issues/131</a>
Port BytesTrie to ICU4X #131
with feedback from Markus Scherer (ICU)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2721">https://github.com/unicode-org/icu4x/issues/2721</a>
Specialized zerovec collections for stringy types
Sketch of a potential AsciiTrie.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/2722">https://github.com/unicode-org/icu4x/pull/2722</a>
Experimental AsciiTrie implementation


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2755">https://github.com/unicode-org/icu4x/issues/2755</a>
Get word break type
When you iterate through text using the WordBreakIterator, you get the boundaries of words, spaces, punctuation, etc.
It does not appear to tell you what kind of token or break that is has found.
The C-language version of ICU has a function on the iterator called getRuleStatus()
that returns an enum that describes the last break it found. The documentation is here:
<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/boundaryanalysis/">https://unicode-org.github.io/icu/userguide/boundaryanalysis/</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/2777/files">https://github.com/unicode-org/icu4x/pull/2777/files</a>
added initial benchmarks for normalizer.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/2877">https://github.com/unicode-org/icu4x/discussions/2877</a>
How to use segmenter


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2886">https://github.com/unicode-org/icu4x/issues/2886</a>
Consider providing mapping to Standardized Variants for CJK Compatibility Ideographs
Across GitHub, I found 3 users of this feature in unicode-normalization:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/sunfishcode/basic-text">https://github.com/sunfishcode/basic-text</a> (by the implementor of the unicode-normalization feature)
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/logannc/fuzzywuzzy-rs">https://github.com/logannc/fuzzywuzzy-rs</a> (unclear to me why you'd want this for a fuzzy match; I'd expect a fuzzy match not to want to distinguish the variations)
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/crlf0710/runestr-rs">https://github.com/crlf0710/runestr-rs</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2975">https://github.com/unicode-org/icu4x/issues/2975</a>
How supported do we consider non-keyextract users?


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2908">https://github.com/unicode-org/icu4x/issues/2908</a>
Time zone needs for calendar application
Use case by team member of Mozilla Thunderbird


Not related to Unicode, but related to the fact I put the ICU4X cdylib in Executor github...
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ankane/polars-ruby/blob/master/ext/polars/Cargo.toml">https://github.com/ankane/polars-ruby/blob/master/ext/polars/Cargo.toml</a>
Is it a way to avoid bundling the original rust lib?
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=34425233">https://news.ycombinator.com/item?id=34425233</a>
---
Not clear to me: for Python, are the lib binaries installed by
<a target="_blank" rel="noopener noreferrer" href="https://pypi.org/project/polars/">https://pypi.org/project/polars/</a>
?
apparently yes, see <a target="_blank" rel="noopener noreferrer" href="https://pypi.org/project/polars/#files">https://pypi.org/project/polars/#files</a>
---
For ruby, is it built by a github workflow?
<a target="_blank" rel="noopener noreferrer" href="https://github.com/ankane/polars-ruby/blob/master/.github/workflows/release.yml">https://github.com/ankane/polars-ruby/blob/master/.github/workflows/release.yml</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/2779/files">https://github.com/unicode-org/icu4x/pull/2779/files</a>
add collator initial bench


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3151">https://github.com/unicode-org/icu4x/issues/3151</a>
icu_casemapping feature request: methods fold and full_fold should apply Turkic mappings depending on locale
    ---
    Markus Scherer:
    Applying Turkic case foldings automatically is dangerous.
    While case mappings are intended for human consumption and take a locale parameter,
    case foldings are used for processing (case-insensitive matching) not for display,
    and in most cases it is very surprising when "IBM" and "ibm" don't match when the
    locale is Turkish or Azerbaijani.
    It is much safer to let the developer control this explicitly. (By comparison,
    ICU4C/ICU4J have folding functions that take a boolean parameter for default vs.
    Turkic foldings. This also models the boolean condition in the relevant Unicode
    data file.)
    ---
    lucatrv
    If I understand correctly, icu_collator should be used when strings need to be sorted,
    while a case-folding method of icu_casemapping should be used when strings need
    just to be matched. However icu_collator can also be used to match strings, see
    for instance examples using Ordering::Equal here, so it is not clear to me which
    one to use in this case.
    Finally, another source of confusion (at least for me) is that icu_casemapping
    can be used for both case mapping and case folding, but its documentation mentions
    only "Case mapping for Unicode characters and strings".
    ---
    sffc
    The collator does a fuzzier match. The example you cited shows that it considers
    "√†s" and "as" to be equal, for example.
    @markusicu is it safe to say that most users who are looking for a fuzzy string
    comparison utility should favor the collator over casefold+nfd?
    ---
    sffc
    See also <a target="_blank" rel="noopener noreferrer" href="https://github.com/tc39/ecma402/issues/256">https://github.com/tc39/ecma402/issues/256</a>
    ---
    hsivonen
    Casefold+NFD and ignoring combining diacritics after the NFD operation gives a
    general case-insensitive, diacritic-insensitive match. To further match the root
    search collation (apart from the Hangul aspect for which I don't understand the
    use case), you'd have to also ignore certain Arabic marks and the Thai phinthu
    (virama). (The Hebrew aspect of the search root is gone from CLDR trunk already.)

    Apart from Turkic case-insensitivity, the key thing that the search collation
    tailorings provide on top of the above is being able to have a diacritic-insensitive
    mode where certain things that technically are diacritics but that are on a
    per-language basis considered to form a distinct base letter are not ignored on
    a locale-sensitive basis. For example, o and √∂ are distinct for Finnish, Swedish,
    Icelandic, and Turkish (not sure if them being equal for Estonian search is
    intentional or a CLDR bug) in collator-based search even when ignoring diacritics.

    Based on observing the performance of Firefox's ctrl/cmd-f (not collator based)
    relative to Chrome's and Safari's (collator-based), I believe that casefold+NFD
    and ignoring certain things post-NFD will be faster than collator-based search.
    However, if you also want not to ignore certain diacritics on a per-locale basis,
    it's up to you to implement those rules. That is, ICU4X doesn't do it for you.
    You can find out what the rules are by reading the CLDR search collation sources.

    (FWIW, Firefox's ctrl/cmd-f does not have locale-dependent rules for diacritics.
    The checkbox either ignores all of them or none.)

    ECMA-402 and ICU4X don't have API surface for collator-based substring match.
    You can only do full-string comparison, so you can search in the sense of
    filtering a set/list of items by a search key.
    ---
    Markus Scherer
        &gt; If I understand correctly, CaseMapping::to_full_fold applies full case folding
        &gt; + NFD and ignores combining diacritics.
    I think not. I believe it just applies the ‚Äúfull‚Äù Case_Folding mappings to each
    character, as opposed to the Simple_Case_Folding. Normalization and removing
    diacritics etc. would be separate steps / function calls.
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr44/#Case_Folding">https://www.unicode.org/reports/tr44/#Case_Folding</a>

        &gt; Therefore it actually provides the fuzziest match (general case-insensitive
        &gt; and diacritic-insensitive match). To my understanding this should be equivalent
        &gt; to the icu_collator primary strength level,
        &gt; <a target="_blank" rel="noopener noreferrer" href="https://icu4x.unicode.org/doc/icu_collator/enum.Strength.html#variant.Primary">https://icu4x.unicode.org/doc/icu_collator/enum.Strength.html#variant.Primary</a>
    No. Similar in effect, but as Henri said, collation mappings do a lot more, such
    as ignoring control codes and variation selectors.

        &gt; which I guess is independent from locale
    Not really. There are language-specific collation mappings, such as German "√§"="ae"
    (on primary level), but of course for the majority of Unicode characters each
    tailoring behaves like the Unicode default.

    Collation also provides for a number of parametric settings, although most of
    those are relevant for sorting, not for matching and searching. They do let you
    select things like ‚Äúignore punctuation‚Äù and ‚Äúignore diacritics but not case‚Äù.
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/tr35-collation.html#Setting_Options">https://www.unicode.org/reports/tr35/tr35-collation.html#Setting_Options</a>
    ---
    lucatrv
    Referring to Section 3.13, Default Case Algorithms in the Unicode Standard,
    now I understand that CaseMapping::full_fold applies the toCasefold(X)
    operation (R4 page 155), which is the Case_Folding property.

    To allow proper caseless matching of strings interpreted as identifiers,
    in my opinion another method CaseMapping::NFKC_full_fold should be added,
    to apply the toNFKC_Casefold(X) operation (R5 page 155), which is the
    NFKC_Casefold property. Then another method should be added to allow
    identifier caseless matching, which could be either the combined function
    toNFKC_Casefold(NFD(X)) (D147 page 158) or the lower level NFD(X)
    normalization function. Otherwise to keep things simpler, maybe just a
    method named CaseMapping::caseless could be added which applies
    toNFKC_Casefold(NFD(X)) (D147 page 158). Do you agree, or otherwise how can
    I perform proper caseless categorization and matching?
    ---
    eggrobin
    For case-insensitive identifier comparison (identifiers include programming
    language identifiers, but also things like usernames: @EGGROBIN and @eggrobin
    are the same person), Unicode provides the operation toNFKC_Casefold, used
    in the definition of identifier caseless match (D147 in Default Caseless Matching).

    Earlier versions of Unicode (prior to 5.2) recommended the use of NFKC and
    casefolding directly, without the removal of default ignorable code points
    performed by toNFKC_Casefold.

    The foldings thus have stability guarantees that make them suitable for usage
    in identifier comparison in conjunction with NFKC
    (see <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/policies/stability_policy.html#Case_Folding).">https://www.unicode.org/policies/stability_policy.html#Case_Folding).</a>

    As @markusicu wrote above, since identifier systems typically need to use a
    locale-independent comparison, the Turkic foldings need to be used with great
    care: whether @eggrobin is the same as @EGGROBIN should not depend on
    someone‚Äôs language.

    @markusicu is it safe to say that most users who are looking for a fuzzy
    string comparison utility should favor the collator over casefold+nfd?
    ^ @macchiati for advice on the most recommended way to perform fuzzy string
    matching.

    I am neither Markus nor Mark, but I would say that for general-purpose
    matching that does not have stability requirements, something collation-based
    is more appropriate. In particular, Chrome‚Äôs Ctrl+F search uses that.

    This is, as has been mentioned, fuzzier (beyond the accents already mentioned,
    note that ≈íUF and ≈ìuf are primary-equal to oeuf, whereas they are not
    identifier caseless matches).

    An important consideration is that, being unstable (there is a somewhat
    squishy stability policy, see <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/policies/collation_stability.html">https://www.unicode.org/policies/collation_stability.html</a>
    and <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/collation/ducet-changes.html),">https://www.unicode.org/collation/ducet-changes.html),</a> fuzzy matching
    based on collation can be improved. Most recently the UTC approved (in
    consensus 174-C4) a change to the collation of punctuation marks that look
    like the ASCII ' and ", which has the effect that O'Connor will now be
    primary-equal to O‚ÄôConnor.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3178">https://github.com/unicode-org/icu4x/issues/3178</a>
Consider supporting three layers of collation data for search collations
Markus Scherer
Outside of ICU4X we usually try to make code &amp; data work according to the
algorithms, not according to what the known data looks like right now.
ICU4C/J allow users to build custom tailorings at build time and at runtime.
It should be possible to tailor relative to something that is tailored in the
intermediate root search.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3174#issuecomment-1624080765">https://github.com/unicode-org/icu4x/issues/3174#issuecomment-1624080765</a>
Should search collation be a different data key + constructor? #3174
---
jlf
Don't know if this long comment brings something useful for Rexx.
They are searching for use-cases.
whole-string matching, collation, substring or prefix matching.
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr10/#Searching:">https://www.unicode.org/reports/tr10/#Searching:</a> It's typically used for a substring
match, like Ctrl-F in a browser.
Why is collation the way it is? There's a use case for diacritic-insensitive string
matching. And there is also the observation that you need special handling for certain
diacritics like German umlauts.
It seems weird that Thai for example has certain tailorings that are not in other
Brahmic languages.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/3981#discussioncomment-6882618">https://github.com/unicode-org/icu4x/discussions/3981#discussioncomment-6882618</a>
String search with collators
references this ICU link:
<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/collation/string-search.html">https://unicode-org.github.io/icu/userguide/collation/string-search.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3174#issuecomment-1624080765">https://github.com/unicode-org/icu4x/issues/3174#issuecomment-1624080765</a>
Should search collation be a different data key + constructor?
jlf: referenced from #3981 with this comment:
We've had discussions about search collations in the past, such as #3174
Basically, we need a client with a clear and compelling use case who ideally can
make some contributions, and then the team can provide mentorship to help land
this type of feature.


icu_collator version 1.3.3 is released.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/releases/tag/ind%2Ficu_collator%401.3.3">https://github.com/unicode-org/icu4x/releases/tag/ind%2Ficu_collator%401.3.3</a>

<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/icu_collator/latest/icu_collator/">https://docs.rs/icu_collator/latest/icu_collator/</a>
Comparing strings according to language-dependent conventions.
jlf: with examples
jlf: implementation notes. <a target="_blank" rel="noopener noreferrer" href="https://docs.rs/icu_collator/latest/icu_collator/docs/index.html">https://docs.rs/icu_collator/latest/icu_collator/docs/index.html</a>
They use NFD?
"The key design difference between ICU4C and ICU4X is that ICU4C puts the
canonical closure in the data (larger data) to enable lookup directly by
precomposed characters while ICU4X always omits the canonical closure and always
normalizes to NFD on the fly."
jlf: ok, on the fly, so part of their algorithm.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/3231#discussioncomment-5599221">https://github.com/unicode-org/icu4x/discussions/3231#discussioncomment-5599221</a>
    @sffc , Will ICU4X Test Data provider give correct results for Lao language?
    I was running segment_utf16 on Lao string but its results are not inline with ICU4C results.

    The ICU4X Test Data provider supports Japanese and Thai. For the other languages,
    you should follow the steps in the tutorial to generate your own data; in general
    the testdata provider is intended for testing. You can also track #2945 which
    will make it possible to get full data without needing to build it using the tool.


<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=ZzsbN7HBd7E">https://www.youtube.com/watch?v=ZzsbN7HBd7E</a>
Rust Z√ºrisee, Dec 2022: Next Generation i18n with Rust Using ICU4X
Talk by Shane Carr (starts at 11:20, with some intros from the organizers first)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/3522">https://github.com/unicode-org/icu4x/discussions/3522</a>
Some word segmentation results are different than we get in ICU4C
- Khmer string ·ûò·ûì·ûª·ûü·üí·ûü·ûë·û∂·üÜ·ûÑ·û¢·ûü·üã is giving 13 index as a breakpoint in ICU4X while ICU4C gives 6
- ‡∫Æ‡ªà‡∫™‡∫ª‡ªà‡∫™‡∫µ 5 in ICU4C while 7 in ICU4X
- ‡∏Å‡∏£‡∏∞‡πÄ‡∏û‡∏£‡∏≤ 3 in ICU4C while 7 in ICU4X
I'm using the full data blob with all keys and locales.
jlf: see the discussion, there is some code.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2945">https://github.com/unicode-org/icu4x/issues/2945</a>
Default constructors with full data
jlf: remember "close #2743 in favour of #2945. the solution we're working on there trivially extends to FFI."
sffc
We have built data providers as a first-class feature in ICU4X. We currently tutor
clients on how to build their data file and detail all the knobs at their disposal,
which is essential to ICU4X's mission.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3552#issuecomment-1600050638">https://github.com/unicode-org/icu4x/issues/3552#issuecomment-1600050638</a>
/// ICU4C's TestGreekUpper
#[test]
fn test_greek_upper() {
    let cm = CaseMapping::new_with_locale(&amp;locale!("el"));

    // <a target="_blank" rel="noopener noreferrer" href="https://unicode-org.atlassian.net/browse/ICU-5456">https://unicode-org.atlassian.net/browse/ICU-5456</a>
    assert_eq!(cm.to_full_uppercase_string("Œ¨Œ¥ŒπŒ∫ŒøœÇ, Œ∫ŒµŒØŒºŒµŒΩŒø, ŒØœÅŒπŒ¥Œ±"), "ŒëŒîŒôŒöŒüŒ£, ŒöŒïŒôŒúŒïŒùŒü, ŒôŒ°ŒôŒîŒë");
    // <a target="_blank" rel="noopener noreferrer" href="https://bugzilla.mozilla.org/show_bug.cgi?id=307039">https://bugzilla.mozilla.org/show_bug.cgi?id=307039</a>
    // <a target="_blank" rel="noopener noreferrer" href="https://bug307039.bmoattachments.org/attachment.cgi?id=194893">https://bug307039.bmoattachments.org/attachment.cgi?id=194893</a>
    assert_eq!(cm.to_full_uppercase_string("Œ†Œ±œÑŒ¨œÑŒ±"), "Œ†ŒëŒ§ŒëŒ§Œë");
    assert_eq!(cm.to_full_uppercase_string("ŒëŒ≠œÅŒ±œÇ, ŒúœÖœÉœÑŒÆœÅŒπŒø, Œ©œÅŒ±ŒØŒø"), "ŒëŒïŒ°ŒëŒ£, ŒúŒ•Œ£Œ§ŒóŒ°ŒôŒü, Œ©Œ°ŒëŒôŒü");
    assert_eq!(cm.to_full_uppercase_string("ŒúŒ±ŒêŒøœÖ, Œ†œåœÅŒøœÇ, Œ°œçŒ∏ŒºŒπœÉŒ∑"), "ŒúŒëŒ™ŒüŒ•, Œ†ŒüŒ°ŒüŒ£, Œ°Œ•ŒòŒúŒôŒ£Œó");
    assert_eq!(cm.to_full_uppercase_string("Œ∞, Œ§Œ∑œÅœé, ŒúŒ¨ŒπŒøœÇ"), "Œ´, Œ§ŒóŒ°Œ©, ŒúŒëŒ™ŒüŒ£");
    assert_eq!(cm.to_full_uppercase_string("Œ¨œÖŒªŒøœÇ"), "ŒëŒ´ŒõŒüŒ£");
    assert_eq!(cm.to_full_uppercase_string("ŒëŒ´ŒõŒüŒ£"), "ŒëŒ´ŒõŒüŒ£");
    assert_eq!(cm.to_full_uppercase_string("ŒÜŒ∫ŒªŒπœÑŒ± œÅŒÆŒºŒ±œÑŒ± ŒÆ Œ¨Œ∫ŒªŒπœÑŒµœÇ ŒºŒµœÑŒøœáŒ≠œÇ"), "ŒëŒöŒõŒôŒ§Œë Œ°ŒóŒúŒëŒ§Œë ŒóÃÅ ŒëŒöŒõŒôŒ§ŒïŒ£ ŒúŒïŒ§ŒüŒßŒïŒ£");
    // <a target="_blank" rel="noopener noreferrer" href="http://www.unicode.org/udhr/d/udhr_ell_monotonic.html">http://www.unicode.org/udhr/d/udhr_ell_monotonic.html</a>
    assert_eq!(cm.to_full_uppercase_string("ŒïœÄŒµŒπŒ¥ŒÆ Œ∑ Œ±ŒΩŒ±Œ≥ŒΩœéœÅŒπœÉŒ∑ œÑŒ∑œÇ Œ±ŒæŒπŒøœÄœÅŒ≠œÄŒµŒπŒ±œÇ"), "ŒïŒ†ŒïŒôŒîŒó Œó ŒëŒùŒëŒìŒùŒ©Œ°ŒôŒ£Œó Œ§ŒóŒ£ ŒëŒûŒôŒüŒ†Œ°ŒïŒ†ŒïŒôŒëŒ£");
    assert_eq!(cm.to_full_uppercase_string("ŒΩŒøŒºŒπŒ∫Œøœç ŒÆ Œ¥ŒπŒµŒ∏ŒΩŒøœçœÇ"), "ŒùŒüŒúŒôŒöŒüŒ• ŒóÃÅ ŒîŒôŒïŒòŒùŒüŒ•Œ£");
    // <a target="_blank" rel="noopener noreferrer" href="http://unicode.org/udhr/d/udhr_ell_polytonic.html">http://unicode.org/udhr/d/udhr_ell_polytonic.html</a>
    assert_eq!(cm.to_full_uppercase_string("·ºòœÄŒµŒπŒ¥·Ω¥ ·º° ·ºÄŒΩŒ±Œ≥ŒΩ·ΩΩœÅŒπœÉŒ∑"), "ŒïŒ†ŒïŒôŒîŒó Œó ŒëŒùŒëŒìŒùŒ©Œ°ŒôŒ£Œó");
    assert_eq!(cm.to_full_uppercase_string("ŒΩŒøŒºŒπŒ∫Œø·ø¶ ·º¢ Œ¥ŒπŒµŒ∏ŒΩŒø·ø¶œÇ"), "ŒùŒüŒúŒôŒöŒüŒ• ŒóÃÅ ŒîŒôŒïŒòŒùŒüŒ•Œ£");
    // From Google bug report
    assert_eq!(cm.to_full_uppercase_string("ŒùŒ≠Œø, ŒîŒ∑ŒºŒπŒøœÖœÅŒ≥ŒØŒ±"), "ŒùŒïŒü, ŒîŒóŒúŒôŒüŒ•Œ°ŒìŒôŒë");
    // <a target="_blank" rel="noopener noreferrer" href="http://crbug.com/234797">http://crbug.com/234797</a>
    assert_eq!(cm.to_full_uppercase_string("ŒïŒªŒ¨œÑŒµ ŒΩŒ± œÜŒ¨œÑŒµ œÑŒ± Œ∫Œ±ŒªœçœÑŒµœÅŒ± œÄŒ±œäŒ¥Œ¨Œ∫ŒπŒ±!"), "ŒïŒõŒëŒ§Œï ŒùŒë Œ¶ŒëŒ§Œï Œ§Œë ŒöŒëŒõŒ•Œ§ŒïŒ°Œë Œ†ŒëŒ™ŒîŒëŒöŒôŒë!");
    assert_eq!(cm.to_full_uppercase_string("ŒúŒ±ŒêŒøœÖ, œÑœÅœåŒªŒµœä"), "ŒúŒëŒ™ŒüŒ•, Œ§Œ°ŒüŒõŒïŒ™");
    assert_eq!(cm.to_full_uppercase_string("Œ§Œø Œ≠ŒΩŒ± ŒÆ œÑŒø Œ¨ŒªŒªŒø."), "Œ§Œü ŒïŒùŒë ŒóÃÅ Œ§Œü ŒëŒõŒõŒü.");
    // <a target="_blank" rel="noopener noreferrer" href="http://multilingualtypesetting.co.uk/blog/greek-typesetting-tips/">http://multilingualtypesetting.co.uk/blog/greek-typesetting-tips/</a>
    assert_eq!(cm.to_full_uppercase_string("œÅœâŒºŒ≠ŒπŒ∫Œ±"), "Œ°Œ©ŒúŒïŒ™ŒöŒë");
    assert_eq!(cm.to_full_uppercase_string("ŒÆ."), "ŒóÃÅ.");
}


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/3688#discussioncomment-6456010">https://github.com/unicode-org/icu4x/discussions/3688#discussioncomment-6456010</a>
Recommended data provider type for libraries depending on ICU4X
---
I finished creating a library that uses ICU4X as its backend, while learning Rust.
For my library I used the DataProvider for as the interface to CLDR data
(currently just using icu_testdata, though seen the page to generate customised
datasets).
So now I am wondering what would be the recommended data provider to use for a
library using ICU4X as its backend?
---
If you know the data you want at build time, I suggest using a baked data provider,
otherwise use a Blob one with postcard.
You can generate data using these steps
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/data_management.md">https://github.com/unicode-org/icu4x/blob/main/docs/tutorials/data_management.md</a>
In the 1.3 release there will be a compiled_data feature that lets you include
data by default, kinda like testdata but intended for production.
---
compiled_data feature may just be what my library could use without the need for
users to supply data provider for my library, if I understand the intended
purpose of this up coming feature. Where is this feature located in the master,
so I may start looking at it for design purposes, while waiting for 1.3 release?
---
jlf: this answer is seriously incomprehensible!
The feature is present on all of the component crates and it exposes functions
like DateTimeFormatter::try_new() that don't have a provider argument.
<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu4x/docs/icu/datetime/struct.DateTimeFormatter.html#method.try_new">https://unicode-org.github.io/icu4x/docs/icu/datetime/struct.DateTimeFormatter.html#method.try_new</a>
The crate also does contain an unstable baked provider that users can pass in
themselves, but note that it only implements data stuff from that particular
crate and they'll need to combine it with providers from other crates if the
type they are using uses data from everywhere (like DateTimeFormat: it uses
plurals and decimal data too)
<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu4x/docs/icu/datetime/provider/struct.Baked.html">https://unicode-org.github.io/icu4x/docs/icu/datetime/provider/struct.Baked.html</a>
---
This is a good question; what should intermediate libraries expose to their
users? I'll schedule this for a discussion at an upcoming developers call.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3709">https://github.com/unicode-org/icu4x/issues/3709</a>
Chinese and Dangi inconsistent with ICU implementations for extreme dates
The current implementation of the Chinese calendar, as well as the Dangi calendar
in #3694, are not consistent with ICU for all dates; based on writing a number of
manual test cases (see the aforementioned PR), this seems to only be an issue for
dates very far in the past or far in the future (ex. year -3000 ISO).
Furthermore, the ICU4X Chinese/Dangi and astronomy functions are newly-written
and have several algorithms based on the most recent edition of Calendrical
Calculations, while the existing ICU code seems to be from 2000, incorporating
algorithms from the 1997 edition of Calendrical Calculations.
---
jlf: I take note of this because it's interesting to see the differences with ICU.


Calendars
in <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/3744#discussion_r1277062568">https://github.com/unicode-org/icu4x/pull/3744#discussion_r1277062568</a>
they reference this common lisp code
<a target="_blank" rel="noopener noreferrer" href="https://github.com/EdReingold/calendar-code2/blob/main/calendar.l#L2352">https://github.com/EdReingold/calendar-code2/blob/main/calendar.l#L2352</a>
---
jlf: I take note of this to remember
;;;; The Functions (code, comments, and definitions) contained in this
;;;; file (the "Program") were written by Edward M. Reingold and Nachum
;;;; Dershowitz (the "Authors")
;;;; These Functions are explained in the Authors'
;;;; book, "Calendrical Calculations", 4th ed. (Cambridge University
;;;; Press, 2016)
---
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Calendrical_Calculations">https://en.wikipedia.org/wiki/Calendrical_Calculations</a>
<a target="_blank" rel="noopener noreferrer" href="https://reingold.co/calendars.shtml">https://reingold.co/calendars.shtml</a>
The resource page for the book makes all the source code for the book available for download.
<a target="_blank" rel="noopener noreferrer" href="https://www.cambridge.org/ch/universitypress/subjects/computer-science/computing-general-interest/calendrical-calculations-ultimate-edition-4th-edition?format=PB&isbn=9781107683167#resources">https://www.cambridge.org/ch/universitypress/subjects/computer-science/computing-general-interest/calendrical-calculations-ultimate-edition-4th-edition?format=PB&isbn=9781107683167#resources</a>
The code has been ported to Python
<a target="_blank" rel="noopener noreferrer" href="https://github.com/espinielli/pycalcal">https://github.com/espinielli/pycalcal</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/uni-algo/uni-algo/issues/31">https://github.com/uni-algo/uni-algo/issues/31</a>
L with stroke letter (U+0141, U+0142) doesn't normalize.
    auto const polish = std::string{"ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º ƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª"};
    auto norm = una::norm::to_unaccent_utf8(polish);
    Everything is normalized except '≈Ç' and '≈Å'.
Everything is normalized except '≈Ç' and '≈Å'.
---
Strokes are not accents. As far as I know there is no data table in Unicode that
maps L with stroke to L so no plans to implemented it, you need to do it
manually if needed.
--
jlf: idem with utf8proc
"ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º ƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª"~text~nfc(stripmark:)=     -- T'ace≈Çnoszz ACE≈ÅNOSZZ'
"ƒÖƒáƒô≈Ç≈Ñ√≥≈õ≈∫≈º ƒÑƒÜƒò≈Å≈É√ì≈ö≈π≈ª"~text~nfd(stripmark:)=     -- T'ace≈Çnoszz ACE≈ÅNOSZZ'
---
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/%C5%81">https://en.wikipedia.org/wiki/%C5%81</a>
    Character                   ≈Å           ≈Ç
    Unicode                     321 0141    322 0142
    CP 852                      157 9D      136 88
    CP 775                      173 AD      136 88
    Mazovia                     156 9C      146 92
    Windows-1250, ISO-8859-2    163 A3      179 B3
    Windows-1257, ISO-8859-13   217 D9      249 F9
    Mac Central European        252 FC      184 B8


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2715">https://github.com/unicode-org/icu4x/issues/2715</a>
Minor and patch release policy


<a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/document/d/1yg_2l5FFo0aAuNi4jpgcIhIYjHqJyUoJWtMduyQ0vR8/edit">https://docs.google.com/document/d/1yg_2l5FFo0aAuNi4jpgcIhIYjHqJyUoJWtMduyQ0vR8/edit</a>
ICU4X Data Versioning Design
This document has been migrated to Markdown in
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/2919">https://github.com/unicode-org/icu4x/pull/2919</a>
jlf: I don't see any markdown...


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/1471">https://github.com/unicode-org/icu4x/issues/1471</a>
Decide on data file versioning policy
jlf: For the comment of Marcus Scherer


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/165">https://github.com/unicode-org/icu4x/issues/165</a>
Data Version
jlf: maybe to read
As far as semantic versioning, I no longer give deference to it as the preferred
way to do versioning or see the topic so singularly after seeing this talk.
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=oyLBGkS5ICk">https://www.youtube.com/watch?v=oyLBGkS5ICk</a>
jlf: Spec-ulation Keynote - Rich Hickey
The comments say it's good, did not watch.


DateTime
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3347">https://github.com/unicode-org/icu4x/issues/3347</a>
DateTimeFormatter still lacks power user APIs
jlf: this ticket contains potentially interesting links:
    Class hiearchy: <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/380">https://github.com/unicode-org/icu4x/issues/380</a>
    Design doc: <a target="_blank" rel="noopener noreferrer" href="https://docs.google.com/document/d/1vJKR1s--RBmXLNIJSCtiTNPp08mab7ZwcTGxIZ9-ytI/edit#">https://docs.google.com/document/d/1vJKR1s--RBmXLNIJSCtiTNPp08mab7ZwcTGxIZ9-ytI/edit#</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/4334#discussion_r1403198515">https://github.com/unicode-org/icu4x/pull/4334#discussion_r1403198515</a>
Add is_normalized_up_to to Normalizer
#4334
jlf remember:
the Web-exposed ICU4C-backed behavior of current String.prototype.normalize in
both SpiderMonkey and V8 retains unpaired surrogates in the normalization process
(even after the first point in the string that needs to change under normalization).
We've previously decided that ICU4X operates on the Unicode Scalar Value / Rust
char value space and, therefore, will perform replacement of unpaired surrogates
with the REPLACEMENT CHARACTER.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4365">https://github.com/unicode-org/icu4x/issues/4365</a>
Segmenter does not work correctly in some languages
"as `‡¶®‡¶Æ‡¶∏‡ßç‡¶ï‡¶æ‡ß∞, ‡¶Ü‡¶™‡ßã‡¶®‡¶æ‡ß∞ ‡¶ï‡¶ø ‡¶ñ‡¶¨‡ß∞?`"'0D'x"hi `‡§π‡•à‡§≤‡•ã, ‡§ï‡•ç‡§Ø‡§æ ‡§π‡§æ‡§≤ ‡§π‡•à‡§Ç?`"'0D'x"mai `‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§Ö‡§π‡§æ‡§Å ‡§ï‡•á‡§®‡§æ ‡§õ‡§•‡§ø?`"'0D'x"mr `‡§®‡§Æ‡§∏‡•ç‡§ï‡§æ‡§∞, ‡§ï‡§∏‡•á ‡§Ü‡§π‡§æ‡§§?`"'0D'x"ne `‡§®‡§Æ‡§∏‡•ç‡§§‡•á, ‡§ï‡§∏‡•ç‡§§‡•ã ‡§π‡•Å‡§®‡•Å‡§π‡•Å‡§®‡•ç‡§õ?`"'0D'x"or `‡¨®‡¨Æ‡¨∏‡≠ç‡¨ï‡¨æ‡¨∞ ‡¨§‡≠Å‡¨Æ‡≠á ‡¨ï‡≠á‡¨Æ‡¨ø‡¨§‡¨ø ‡¨Ö‡¨õ?`"'0D'x"sa `‡§π‡•á ‡§§‡•ç‡§µ‡§Ç ‡§ï‡§ø‡§¶‡§Ç ‡§Ö‡§∏‡§ø?`"'0D'x"te `‡∞π‡∞æ‡∞Ø‡±ç, ‡∞é‡∞≤‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?`"
icu4c: 151
rust: 161
executor: 151
---
ICU4X and ICU4C are just using different definitions of EGCs; ICU4C has had a
tailoring for years which has just been incorporated into Unicode 15.1, whereas
ICU4X implements the 15.0 version without that tailoring.
The difference is the handling of aksaras in some indic scripts:
in Unicode 15.1 (and in any recent ICU4C) ‡§ï‡•ç‡§Ø‡§æ is one EGC, but it is two EGCs
(‡§ï‡•ç, ‡§Ø‡§æ) in untailored Unicode 15.0 (and in ICU4X).
---
eggrobin
(For what it‚Äôs worth, ‡§ï‡•ç‡§Ø‡§æ would be three legacy grapheme clusters, namely ‡§ï‡•ç, ‡§Ø,
and ‡§æ, see Table 1a of UAX29, whereas it is two 15.0 extended grapheme clusters
and a single 15.1 extended grapheme cluster.)
---
Fixed by #4536


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/4334">https://github.com/unicode-org/icu4x/pull/4334</a>
is_normalized_up_to and unpaired surrogates
---
jlf: interesting discussion about the support of ill-formed strings


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/4389">https://github.com/unicode-org/icu4x/pull/4389</a>
Line breaking
---
jlf: they don't want to support a tailored line breaking, because this requires
more than one code point of lookahead.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4342">https://github.com/unicode-org/icu4x/issues/4342</a>
Add functions to get ICU4X, CLDR, and Unicode versions
---
jlf: strange that they did not consider that earlier...


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2689">https://github.com/unicode-org/icu4x/issues/2689</a>
Consider exposing sort keys
---
jlf : interesting for the description of the use cases (encryption, xpath)
I created a section Xpath with their comments.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3336">https://github.com/unicode-org/icu4x/issues/3336</a>
Add support for Unicode BCP 47 locale identifiers
---
jlf: what is that?
it's defined in <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/">https://www.unicode.org/reports/tr35/</a>
UNICODE LOCALE DATA MARKUP LANGUAGE (LDML)
Unicode LDML uses stable identifiers based on [BCP47] for distinguishing among
languages, locales, regions, currencies, time zones, transforms, and so on.
<a target="_blank" rel="noopener noreferrer" href="https://www.rfc-editor.org/rfc/bcp/bcp47.txt">https://www.rfc-editor.org/rfc/bcp/bcp47.txt</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3247#issuecomment-1856577508">https://github.com/unicode-org/icu4x/issues/3247#issuecomment-1856577508</a>
This month @anba landed Intl.Segmenter in Firefox based on the ICU4X Segmenter impl, reviewed by @dminor
<a target="_blank" rel="noopener noreferrer" href="https://phabricator.services.mozilla.com/D195803">https://phabricator.services.mozilla.com/D195803</a>
I had been under the impression that Intl.Segmenter was not implementable without
support for random access in order to implement the containing() function.
It looks like @anba's implementation loops from the start of the string and
repeatedly calls next() until we reach the index. While this strategy gets the
job done, I'm concerned about the performance of this with large strings where
we need to reach an index deep into the string. I therefore hope that we can
continue to prioritize this issue on the basis of 402 compatibility.
---
jlf: to watch


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4523">https://github.com/unicode-org/icu4x/issues/4523</a>
Linebreak generated before CL (Close Punctuation)
---
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr14/#CL">https://www.unicode.org/reports/tr14/#CL</a>
UNICODE LINE BREAKING ALGORITHM


<a target="_blank" rel="noopener noreferrer" href="https://github.com/typst/typst/issues/3082">https://github.com/typst/typst/issues/3082</a>
Chinese punctuation is placed at the beginning of the line in some cases
---
jlf: Linebreak referenced from icu4x/issues/4523
The example is wrong, a better example is provided in icu4x/issues/4523.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/4389">https://github.com/unicode-org/icu4x/pull/4389</a>
Fix Unicode 15.0 line breaking
jlf: Linebreak


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4146">https://github.com/unicode-org/icu4x/issues/4146</a>
icu_segmenter::LineSegmenter incorrectly applies rule LB8a
---
jlf: Linebreak, for the examples of line breaks.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/discussions/4525#discussioncomment-8155602">https://github.com/unicode-org/icu4x/discussions/4525#discussioncomment-8155602</a>
Mapping between browser Intl and ICU4X
jlf: I don't understand what they are talking about, but there are maybe good
to know informations in this thread. In particular this URL:
"Sensitivity" in browsers maps to a combination of strength and case level.
<a target="_blank" rel="noopener noreferrer" href="https://searchfox.org/mozilla-central/rev/1aa61dcd48e128a8cbfbe59b7ba43d31bd3c248a/intl/components/src/Collator.cpp#171-185">https://searchfox.org/mozilla-central/rev/1aa61dcd48e128a8cbfbe59b7ba43d31bd3c248a/intl/components/src/Collator.cpp#171-185</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3284#issuecomment-1911226051">https://github.com/unicode-org/icu4x/issues/3284#issuecomment-1911226051</a>
Should the Segmenter types accept a locale?
---
Steven Loomis:
Please put it into the API.
I was doing planning on a work item to move this forward.
This is for example languages that want to keep "ch" together etc.
---
jlf: so it appears from the discussion that ICU4C implements specific rules that
are not part of UAX #29.
---
sffc
The conclusions from the discussion of this issue with the CLDR design group:
- Grapheme clusters should not be language-specific; baked into much low-level
  processing (e.g., Swift, font mappings) which we don‚Äôt want to be language-specific
- Content locale/text language parameter (not UI locale): Potential for accuracy;
  make it optional, name it well
- Ok to leave the locale on the constructor; benefit: more specific data loading
  even for existing dictionaries &amp; models
My suggested path forward for this issue, then, is to add an options bag to the
WordSegmenter, LineSegmenter, and SentenceSegmenter constructors with an optional
content_locale field of type &amp;LanguageIdentifier.
---
Steven Loomis
This makes no sense and contradicts the long standing requests.
I would have joined, did not realize this was coming up today.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/58">https://github.com/unicode-org/icu4x/issues/58</a>
Design a cohesive solution for supported locales


<a target="_blank" rel="noopener noreferrer" href="https://github.com/tc39/proposal-intl-segmenter/issues/133">https://github.com/tc39/proposal-intl-segmenter/issues/133</a>
Custom Dictionaries
and a political point of view from a Hong Kong immigrant.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3284">https://github.com/unicode-org/icu4x/issues/3284</a>
Should the Segmenter types accept a locale?
Markus Scherer:
    No language parameter for grapheme cluster segmenter
+1
    Language parameter for the other three segmenters
+1


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3990">https://github.com/unicode-org/icu4x/issues/3990</a>
Consider supporting retrieval of the language preference list from the system
---
jlf: some infos and pointers, for general culture.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4705">https://github.com/unicode-org/icu4x/issues/4705</a>
Bridge the gap between icu::properties::Script and icu::locid::subtags::Script
---
jlf: this is about script names
---
Markus Scherer
    Conversion is probably fine, but in the end they are just script codes, so
    it also makes sense to define the full set once and have Unicode APIs use a
    subset of the values.
    The ones in the UCD are a subset of the full set.
    And only the ones in the UCD have Unicode-defined long value names (identifiers).
Eggrobin
    <a target="_blank" rel="noopener noreferrer" href="https://unicode.org/iso15924/codelists.html">https://unicode.org/iso15924/codelists.html</a>
    <a target="_blank" rel="noopener noreferrer" href="https://unicode.org/iso15924/iso15924.txt">https://unicode.org/iso15924/iso15924.txt</a>
    The PVA column is from <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/Public/UCD/latest/ucd/PropertyValueAliases.txt.">https://www.unicode.org/Public/UCD/latest/ucd/PropertyValueAliases.txt.</a>
Markus Scherer
    Also <a target="_blank" rel="noopener noreferrer" href="https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry">https://www.iana.org/assignments/language-subtag-registry/language-subtag-registry</a>
    look for Type: script

    which becomes this in CLDR:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/cldr/blob/main/common/validity/script.xml">https://github.com/unicode-org/cldr/blob/main/common/validity/script.xml</a>

    Note that the CLDR list includes one or more private use script subtags:

    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/#unicode_script_subtag_validity">https://www.unicode.org/reports/tr35/#unicode_script_subtag_validity</a>
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr35/#Private_Use_Codes">https://www.unicode.org/reports/tr35/#Private_Use_Codes</a>
    Qaag is current but yucky... Don't include Qaai which has become an alias for Zinh


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3014">https://github.com/unicode-org/icu4x/issues/3014</a>
Provide the Numeric_Value character property
ICU4X is missing an API for querying the Numeric_Value property of a character.
Markus Scherer
    Note that Numeric_Value is easy when Numeric_Type=Decimal or Numeric_Type=Digit.
    And maybe you need/want it only if Numeric_Type=Decimal.

    When Numeric_Type=Numeric, then the Numeric_Value can be negative, huge, or a fraction.
    These are rarely useful. <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr44/#Numeric_Value">https://www.unicode.org/reports/tr44/#Numeric_Value</a>

    I would start with an API that returns the value of a decimal digit.
Markus Scherer
    Most of the nt=digit characters are not part of a contiguous 0..9 range of characters.
    In particular, there is often no zero.
    Some of them are simply nt=digit because their nv is 0..9 although they are
    part of a larger set of "numbered list bullets" where the nv&gt;9 numbers have nt=numeric.

    In UTS46, they are variously disallowed/mapped/valid.
    See <a target="_blank" rel="noopener noreferrer" href="https://util.unicode.org/UnicodeJsps/list-unicodeset.jsp?a=%5B%3Ant%3Ddigit%3A%5D&g=uts46&i=">https://util.unicode.org/UnicodeJsps/list-unicodeset.jsp?a=%5B%3Ant%3Ddigit%3A%5D&g=uts46&i=</a>

    It makes sense to me to have an API that returns the nv of nt=decimal but
    the nv of other characters is rarely useful to programmers.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4771">https://github.com/unicode-org/icu4x/issues/4771</a>
LineBreakStrictness::Anywhere gives the wrong breakpoints for Arabic in icu_segmenter
I am aware this is probably a unicode spec issue, rather than a rust library issue, but I thought I would point it out regardless.
This is the minimal application I was using to test this behavior:
    use icu_segmenter::{LineBreakOptions, LineBreakStrictness, LineSegmenter};
    fn main() {
        let test = "ÿßŸÑÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ";

        let mut options = LineBreakOptions::default();
        options.strictness = LineBreakStrictness::Anywhere;

        let segmenter = LineSegmenter::new_auto_with_options(options);

        let breakpoints = segmenter.segment_str(test);

        for bp in breakpoints {
            println!("{bp}:  {}", &amp;test[bp..]);
        }
    }
This gives the following output:
(jlf: bbedit doesn't support well this text, can't indent the whole block, can't
indent a single line)
0:  ÿßŸÑÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
2:  ŸÑÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
4:  ÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
6:  ŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
8:  ŸÑ ŸàÿßŸÑŸÑŸäŸÑ
10:   ŸàÿßŸÑŸÑŸäŸÑ
11:  ŸàÿßŸÑŸÑŸäŸÑ
13:  ÿßŸÑŸÑŸäŸÑ
15:  ŸÑŸÑŸäŸÑ
17:  ŸÑŸäŸÑ
19:  ŸäŸÑ
21:  ŸÑ
23:
as you can tell, it is breaking after every single letter, without respect to the letters' connections. However, as I am sure you are aware, the letters' connections are not optional.
The output I expected is the following:
0:  ÿßŸÑÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
2:  ŸÑÿÆŸäŸÑ ŸàÿßŸÑŸÑŸäŸÑ
10:   ŸàÿßŸÑŸÑŸäŸÑ
11:  ŸàÿßŸÑŸÑŸäŸÑ
13:  ÿßŸÑŸÑŸäŸÑ
15:  ŸÑŸÑŸäŸÑ
23:
Putting the break points across the visual boundaries of the letters. This is not the current orthodoxy, but any looser breaks than that and you'd be rendering the text illegible and unnatural.
Note: This is how old written manuscripts break their words.
---
Closed as not planned


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/4780">https://github.com/unicode-org/icu4x/issues/4780</a>
Unexpected grapheme boundary with regional indicators (GB12)
use icu::segmenter::GraphemeClusterSegmenter;
    fn main() {
        let segmenter = GraphemeClusterSegmenter::new();
        let text = "üá∫üá∏üè¥Û†ÅßÛ†Å¢Û†Å•Û†ÅÆÛ†ÅßÛ†Åø";
        segmenter
            .segment_str(text)
            .for_each(|i| println!("{}", i));
    }
Reports the following break points:
    0
    4
    8
    36
which means "üá∫üá∏" is split into two graphemes, which should be disallowed per GB12
---
This is fixed by #4536.
---
jlf: utf8proc is ok
"üá∫üá∏"~graphemes==
    a CharacterSupplier
     1 : T'üá∫üá∏'
"üá∫üá∏"~unicodecharacters==
    an Array (shape [2], 2 items)
     1 : ( "üá∫"   U+1F1FA So 1 "REGIONAL INDICATOR SYMBOL LETTER U" )
     2 : ( "üá∏"   U+1F1F8 So 1 "REGIONAL INDICATOR SYMBOL LETTER S" )


#4536
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/pull/4536">https://github.com/unicode-org/icu4x/pull/4536</a>
Update grapheme cluster break rules to Unicode 15.1
jlf: lot of discussions about stability that I did not try to understand.


<hr><h2 id="utf8proc_title">utf8proc title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://codeberg.org/dnkl/foot/pulls/100">https://codeberg.org/dnkl/foot/pulls/100</a>
Grapheme shaping using libutf8proc #100
jlf tag: character width
jlf: to read?


<hr><h2 id="Twitter_text_parsing">Twitter text parsing</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/twitter/twitter-text">https://github.com/twitter/twitter-text</a>
Twitter Text Libraries. This code is used at Twitter to tokenize and parse text
to meet the expectations for what can be used on the platform.

<a target="_blank" rel="noopener noreferrer" href="https://swiftpack.co/package/nysander/twitter-text">https://swiftpack.co/package/nysander/twitter-text</a>
This is the Swift implementation of the twitter-text parsing library.
The library has methods to parse Tweets and calculate length, validity, parse @mentions, #hashtags, URLs, and more.


<hr><h2 id="terminal___console___cmd">terminal / console / cmd</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/bash/comments/wfbf3w/determine_if_the_termconsole_supports_utf8/">https://www.reddit.com/r/bash/comments/wfbf3w/determine_if_the_termconsole_supports_utf8/</a>
Determine if the term/console supports UTF8?


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/388490/how-to-use-unicode-characters-in-windows-command-line">https://stackoverflow.com/questions/388490/how-to-use-unicode-characters-in-windows-command-line</a>
jlf: with my current version of Windows (21H2 - 10.0.19044), I have the input bug describe below:

    In general using codepage 65001 will only work without bugs in Windows 10 with the Creators update.
    In Windows 7 it will have both output and input bugs.
    In Windows 8 and older versions of Windows 10 it only has the input bug, which limits input to 7-bit ASCII.
    Eryk Sun Sep 9, 2017 at 13:43

jlf: the sentence above is not true, I have the input bug with my version of Windows which is AFTER Creators update.


<a target="_blank" rel="noopener noreferrer" href="http://archives.miloush.net/michkap/archive/2006/03/13/550191.html">http://archives.miloush.net/michkap/archive/2006/03/13/550191.html</a>
Who broke the UTF-8 support?
by Michael S. Kaplan, published on 2006/03/13 03:21 -05:00, original URI: <a target="_blank" rel="noopener noreferrer" href="http://blogs.msdn.com/b/michkap/archive/2006/03/13/550191.aspx">http://blogs.msdn.com/b/michkap/archive/2006/03/13/550191.aspx</a>
---
jlf : we are in 2022 and the UTF-8 support in cmd is still broken...


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/39736901/chcp-65001-codepage-results-in-program-termination-without-any-error">https://stackoverflow.com/questions/39736901/chcp-65001-codepage-results-in-program-termination-without-any-error</a>
jlf : Thanks to this post, I suddenly understood why ooRexxShell no longer supports UTF-8 input.
      It's because I deactivated readline on Dec 20, 2020.
      When readline is on, ooRexxShell delegates to cmd to read a line:
          set /p inputrx="My prompt&gt; "
      This input mode is not impacted by the UTF-8 input bug!


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/10651975/unicode-utf-8-with-git-bash">https://stackoverflow.com/questions/10651975/unicode-utf-8-with-git-bash</a>
git-bash (Windows)


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window">https://stackoverflow.com/questions/57131654/using-utf-8-encoding-chcp-65001-in-command-prompt-windows-powershell-window</a>
Using UTF-8 Encoding (CHCP 65001) in Command Prompt / Windows Powershell (Windows 10)
Describes how to set the system locale (language for non-Unicode programs) to UTF-8.
Optional reading: Why the Windows PowerShell ISE is a poor choice
---
jlf: this is a clear description of the UTF-8 input bug.
For ReadFile from the console, even in Windows 10, you'll be limited to 7-bit ASCII if the
input codepage is set to UTF-8, due to buggy assumptions in the console host, conhost.exe.
In Windows 10, it returns non-ASCII characters as null ("\0") in the buffer.
In older versions, the read succeeds with 0 bytes read, which looks like EOF.
Eryk Sun Jul 21, 2019 at 13:31


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/49476326/displaying-unicode-in-powershell/49481797#49481797">https://stackoverflow.com/questions/49476326/displaying-unicode-in-powershell/49481797#49481797</a>
Displaying Unicode in Powershell


<a target="_blank" rel="noopener noreferrer" href="https://akr.am/blog/posts/using-utf-8-in-the-windows-terminal">https://akr.am/blog/posts/using-utf-8-in-the-windows-terminal</a>
Using UTF-8 in the Windows Terminal
<a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/terminal">https://github.com/microsoft/terminal</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Microsoft/Cascadia-Code">https://github.com/Microsoft/Cascadia-Code</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/PowerShell/PowerShell/issues/7233">https://github.com/PowerShell/PowerShell/issues/7233</a>
Make console windows fully UTF-8 by default on Windows, in line with the behavior on Unix-like platforms
mklement0 opened this issue on Jul 5, 2018
---
jlf still opened on 2023.08.08


<a target="_blank" rel="noopener noreferrer" href="https://github.com/contour-terminal/terminal-unicode-core">https://github.com/contour-terminal/terminal-unicode-core</a>
Unicode Core specification for Terminal (grapheme clusters, character widths, ...)
jlf: only a poor tex file... dead? no commit since 2 years.
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37804829">https://news.ycombinator.com/item?id=37804829</a>
ZERO comment in HN


<hr><h2 id="QT_Title">QT Title</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://bugreports.qt.io/browse/QTBUG-48726">https://bugreports.qt.io/browse/QTBUG-48726</a>
Combining diacritics misplaced when using monospace fonts
jlf tag: character width


<hr><h2 id="IBM_OS">IBM OS</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/personal-communications/15.0?topic=pages-contents#ToC">https://www.ibm.com/docs/en/personal-communications/15.0?topic=pages-contents#ToC</a>
Host Code Page Reference
Referenced in a IBM-MAIN thread about "TN3270, EBCDIC and ASCII"
&gt;&gt;There are a few layers to getting the codepages right for using a terminal
&gt;&gt;emulator and ISPF Edit and Browse on the host.
&gt;&gt;For example, in Personal Communications I first define my host codepage.  I
&gt;&gt;have a lot of choices. From 420 (Arabic) to 1130 (Vietnamese).  I tend to
&gt;&gt;use 1047 (U.S.) to get my square brackets right.
jlf: tables of character codes


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/3.1.0?topic=317-zos-unix-directory-list-utility-line-commands">https://www.ibm.com/docs/en/zos/3.1.0?topic=317-zos-unix-directory-list-utility-line-commands</a>
z/OS UNIX directory list utility line commands
Referenced in a IBM-MAIN thread about "TN3270, EBCDIC and ASCII"
&gt;&gt;Then on the host side. If you are using the ISPF UDLIST interface to Unix
&gt;&gt;(OMVS) you can use either EBCDIC, ASCII, or UTF8 for EDIT or VIEW.
Actions:
E‚Äîedit regular file
EA‚Äîedit ASCII file
EU‚Äîedit UTF-8 file
V‚Äîview regular file
VA‚Äîview ASCII file
VU‚Äîview UTF8 file


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/3.1.0?topic=information-pdf-browse-primary-commands">https://www.ibm.com/docs/en/zos/3.1.0?topic=information-pdf-browse-primary-commands</a>
PDF Browse primary commands
Referenced in a IBM-MAIN thread about "TN3270, EBCDIC and ASCII"
&gt;&gt;In ISPF Browse, you can use the DISPLAY command to view data as UTF8,
&gt;&gt;UTF32, UCS2, UNICODE, ASCII, USASCII, and EBCDIC, or specify the numeric
&gt;&gt;CCSID.
Syntax diagram DISPLAY CCSIDccsid_number
    ASCII
    USASCII
    EBCDIC
    UCS2
    UTF8
    UTF16
    UTF32
Syntax diagram FIND
    UTF8
    ASCII
    USASCII


<hr><h2 id="IBM_RPG_Lang">IBM RPG Lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/i/7.4?topic=cdt-processing-string-data-by-natural-size-each-character">https://www.ibm.com/docs/en/i/7.4?topic=cdt-processing-string-data-by-natural-size-each-character</a>
Processing string data by the natural size of each character
    String data can have characters of different sizes.
    - UTF-8 data can have characters with 1, 2, 3, or 4 bytes.
      For example, the character 'a' has one byte, and the character '√°' has two bytes.
      UTF-8 data is defined as alphanumeric with CCSID(*UTF8) or CCSID(1208).

    - UTF-16 data can have characters with 2 or 4 bytes.
      UTF-16 data is defined as UCS-2 with CCSID(*UTF16) or CCSID(1200).

    - EBCDIC mixed SBCS/DBCS data can have characters with 1 or 2 bytes.
      Additionally, double-byte data is surrouned by shift bytes.
      The shift-out byte x'0E' begins a section of DBCS data and the shift-in
      byte x'0F' ends the section of DBCS data.

    - ASCII mixed SBCS/DBCS data can have characters with 1 or 2 bytes.
      ASCII mixed SBCS/DBCS data is defined as alphanumeric with a CCSID that
      represents mixed SBCS/DBCS data such as 950.

    Default behaviour, CHARCOUNT STDCHARSIZE
    By default, data is processed using the standard-character-size mode.
    The compiler processes string data by bytes or double bytes without regard
    for size of each character.

    When CHARCOUNT NATURAL is in effect:
    The compiler processes string operations by the natural size of each character.
    The compiler sets the CHARCOUNT NATURAL mode for a file if the CHARCOUNT is
    not specified for the file.
    The CHARCOUNT mode for the file affects the movement of data from RPG fields
    to the output buffer and key buffer used for the file operations.

<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/i/7.4?topic=fdk-charcountnatural-stdcharsize">https://www.ibm.com/docs/en/i/7.4?topic=fdk-charcountnatural-stdcharsize</a>
CHARCOUNT(*NATURAL | *STDCHARSIZE)
    The CHARCOUNT keyword controls how RPG handles string truncation when moving
    data from RPG program variables to the output buffer and key buffer for the file.

    *NATURAL
    If the data type of the field in the output buffer or key buffer is relevant
    according to the CHARCOUNTTYPES Control keyword, any necessary truncation
    when data is moved is done according to the CHARCOUNT NATURAL mode for assignment.

    *STDCHARSIZE
    Any necessary truncation when data is moved is done by bytes or double bytes,
    without regard for the size of each character.
    When the CHARCOUNT keyword is not specified, the current CHARCOUNT setting
    is used for the file, as determined by the CHARCOUNT Control keyword or the
    most recent /CHARCOUNT directive preceding the definition for the file.

<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/i/7.4?topic=keywords-charcounttypesutf8-utf16-jobrun-mixedebcdic-mixedascii">https://www.ibm.com/docs/en/i/7.4?topic=keywords-charcounttypesutf8-utf16-jobrun-mixedebcdic-mixedascii</a>
CHARCOUNTTYPES(*UTF8 *UTF16 *JOBRUN *MIXEDEBCDIC *MIXEDASCII)
    The Control keyword CHARCOUNTTYPES specifies the types of data that are
    processed by characters rather than by bytes or double bytes when
    CHARCOUNT NATURAL mode is in effect.

    *UTF8
    Specify *UTF8 if your module might work with UTF-8 data which has characters
    of different lengths. For example, the UTF-8 character 'a' has one byte, and
    the UTF-8 character '√°' has two bytes.

    *UTF16
    Specify *UTF16 if your module might work with UTF-16 data which has some
    4-byte characters.

    *JOBRUN
    Specify *JOBRUN if your job CCSID might support mixed SBCS and DBCS data,
    and the RPG variables in your module defined to have the job CCSID might
    contain some DBCS data.

    *MIXEDEBCDIC
    Specify *MIXEDEBCDIC if your module might work with EBCDIC data which
    supports both SBCS and DBCS characters. This includes data defined with
    CCSID(*JOBRUNMIX) and data defined with a mixed SBCS/DBCS CCSID such as 937.

    *MIXEDASCII
    Specify *ASCII if your module might work with ASCII data which supports
    both SBCS and DBCS characters.


<hr><h2 id="IBM_z_OS">IBM z/OS</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/2.5.0?topic=mvs-zos-unicode-services-users-guide-reference">https://www.ibm.com/docs/en/zos/2.5.0?topic=mvs-zos-unicode-services-users-guide-reference</a>
Unicode services


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/2.5.0?topic=reference-application-programmer-information">https://www.ibm.com/docs/en/zos/2.5.0?topic=reference-application-programmer-information</a>
Character conversion
Case conversion
Normalization
Collation
Bidi transformation
Stringprep conversion
---
jlf:
There is this note at the begining of the page "Bidi transformation":
    "IBM does not intend to enhance the bidi transformation service. Instead, it is
    recommended that you use the character conversion 'extended bidi support' for all
    new development and for the highest level of bidi support."
Can't find where is described this 'extended bidi support'.


<a target="_blank" rel="noopener noreferrer" href="https://www-40.ibm.com/servers/resourcelink/svc00100.nsf/pages/zOSV2R5IndexFile/$file/index.html">https://www-40.ibm.com/servers/resourcelink/svc00100.nsf/pages/zOSV2R5IndexFile/$file/index.html</a>
search Ctrl+F "unicode": only one result:
cunu100_v2r5.pdf	SA38-0680-50	z/OS Unicode Services User's Guide and Reference


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/2.5.0">https://www.ibm.com/docs/en/zos/2.5.0</a>
    Search "Unicode" in z/OS 2.5 documentation:
    <a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/search/unicode?scope=SSLTBW_2.5.0">https://www.ibm.com/docs/en/search/unicode?scope=SSLTBW_2.5.0</a>
jlf: not sure it's very interesting... All the links are just one page with few informations.


<a target="_blank" rel="noopener noreferrer" href="https://listserv.ua.edu/cgi-bin/wa?A2=IBM-MAIN;5304fbc3.2304&S=">https://listserv.ua.edu/cgi-bin/wa?A2=IBM-MAIN;5304fbc3.2304&S=</a>
Re: TSO Rexx C2X Incorrect Output
Events such as this affirm my belief in minimal munging of user data by default.
jlf: this sentence is to remember when designing how Unicode should be supported
by Rexx...


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/76569347/what-are-the-supported-code-points-for-special-characters-for-valid-z-os-datas">https://stackoverflow.com/questions/76569347/what-are-the-supported-code-points-for-special-characters-for-valid-z-os-datas</a>
What are the supported code points for 'special characters' for valid z/OS datasets?
jlf: the link above was given in this IBM-MAIN thread
<a target="_blank" rel="noopener noreferrer" href="https://listserv.ua.edu/cgi-bin/wa?A2=ind2307&L=IBM-MAIN&D=0&P=121856">https://listserv.ua.edu/cgi-bin/wa?A2=ind2307&L=IBM-MAIN&D=0&P=121856</a>
---
Matt Hogstrom:
I did some testing by creating a file in USS in CP047 with the characters ‚Äú@#$‚Äù
and then used iconv to convert them to a variety of code pages and compare the
results. Some conversions failed but when looking at the code pages that failed
they didn‚Äôt appear to me to be what I would consider mainstream.  For the ones
I‚Äôm familiar with they all converted correctly.

The command was
    'iconv -f 1047 -t 37 special &gt; converted;chtag -t -c 37 converted;cmp special converted‚Äô
I changed to the encoding of 37 to other code pages and most worked fine.
You can find the list of cps supported by issuing 'iconv -l‚Äô and there are a lot
of them.


<a target="_blank" rel="noopener noreferrer" href="https://listserv.ua.edu/cgi-bin/wa?A2=ind2307&L=IBM-MAIN&D=0&P=183611">https://listserv.ua.edu/cgi-bin/wa?A2=ind2307&L=IBM-MAIN&D=0&P=183611</a>
Python 3.11 on z/OS - UTF-8 errors
---
I am trying to get a python package (psutil) to run on z/OS.
I downloaded the package from github and then tar'ed it and uploaded it binary
to my home-dir in OMVS.
In my homedir I untar'ed to files and ran the command "chtag -tc IBM-1047 *' to
set the files to UTF-8.
I got make to work by converting the tab char to x'05' - no problem - and I got
the C compiler to work also.
Now my problem is that I can not make Python compile the setup.py file.
It dies with a UTF-error on a char x'97' in statement 48 pos 2:
    from _common import AIX  # NOQA
---
It's this package
<a target="_blank" rel="noopener noreferrer" href="https://github.com/giampaolo/psutil/blob/master/INSTALL.rst">https://github.com/giampaolo/psutil/blob/master/INSTALL.rst</a>
---
I believe UTF-8 is IBM-1208.
---
Have you tried the z/OS Open Tools phytonport - <a target="_blank" rel="noopener noreferrer" href="https://github.com/ZOSOpenTools">https://github.com/ZOSOpenTools</a>
---
Have you considered cloning the repository and utilizing Git's file
tagging feature? It can handle the tagging process for you. If you don't
have internet access, a suggestion would be to tag all the files as
ISO8859-1. It's advisable to avoid using UTF-8, as it may cause issues
with some ported tools that will not work. That includes the majority of
Rocket ported tools. If you list the IBM Python runtime library you will
notice that all source files are tagged "iso8859-1" even though Python
mandates UTF-8.
---
I'm doing this on the company sandbox so I can not make a git clone.
And trying 8859-1 (cp 819) does not change anything:
    /home/bc6608/psutil:chtag -p setup.py
    t ISO8859-1   T=on  setup.py

    PYTHONWARNINGS=all python3 setup.py build_ext -i `python3 -c  "import sys, os;  py36 = sys.version_info[:2] &gt;= (3, 6);  cpus = os.cpu_count() or 1 if py36 else 1;  print('--parallel %s' % cpus if cpus &gt; 1 else '')"`
    Traceback (most recent call last):
     File "/home/bc6608/psutil/setup.py", line 47, in &lt;module&gt;
       from _common import AIX  # NOQA
       ^^^^^^^^^^^^^^^^^^^^^^^
    UnicodeDecodeError: 'utf-8' codec can't decode byte 0x97 in position 2: invalid start byte
---
Found the error.
The error was not the codepage of the setup.py, but the codepage of the imported file _common .
Once it got chtag -tc 1047 _common.py I got further.
---
I can‚Äôt recreate your problem but I used a different method. I downloaded a zip file from Github, uploaded it to z/OS and followed these steps:
    jar xf psutill-master.zip
    cd psutil-master
    chtag -R -tc iso8859-1 .
    python3 setup.py
---
A quick question -
Will the same chtag command work for, say, Java packages/projects?
    Answer: yes
Or, would I have to use chtag -R -tc UTF-8 if a project expects to things to be in UTF8?
    Answer:
    I'd like to understand your reasons for wanting to encode your Java source
    files in UTF-8. It's important to note that the default encoding on z/OS is
    IBM-1047 (EBCDIC). We typically use ISO8859-1 and have to specify the
    "-encoding iso8859-1" option when using the javac compiler. As mentioned
    earlier, tagging files as UTF-8 can lead to unexpected issues, which is why
    it's not commonly done. If you examine the file attributes of modern
    languages like Python, Node.js, Go, etc., you'll notice that their source
    files are tagged as ISO8859-1.
    A while ago, one of our ported tools developers provided me with a detailed
    explanation regarding the challenges associated with UTF-8 for ported tools.
    Although I don't recall all the specifics, it had something to do with
    double conversions. Therefore, the general rule of thumb is to avoid using
    UTF-8 unless it is necessary, such as when embedding a YAML document into a
    Java JAR file.
---
We specify &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; in
our Maven builds as most of the time we are building off host on machines with
UTF8 locales. However,  we tag our files ISO8859-1 on z/OS other then some YAML
docs that must be tagged UTF-8 or else SnakeYaml barfs when reading it from the
class path which doesn‚Äôt support tags :). The server runs with file.encoding=ISO8859-1
as well. If we cared about the euro sign we could change it to ISO8859-15 which
is still an 8-bit character set. It‚Äôs those pesky codes above 0x7F in UTF-8 that
cause the issues.


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/support/pages/system/files/inline-files/Managing%20the%20code%20page%20conversion%20when%20migrating%20zOS%20source%20files%20to%20Git%20-%201.0.pdf">https://www.ibm.com/support/pages/system/files/inline-files/Managing%20the%20code%20page%20conversion%20when%20migrating%20zOS%20source%20files%20to%20Git%20-%201.0.pdf</a>
(PDF)
Managing the code page conversion when migrating z/OS source files to Git
---
Git has proven to be the de-facto standard in the Open Source world, and the
z/OS platform can interact with Git through the z/OS Git client, which is
maintained by Rocket Software in its ‚ÄúOpen Source Languages and Tools for z/OS‚Äù
package.


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/2.2.0?topic=server-different-end-line-characters-in-text-files">https://www.ibm.com/docs/en/zos/2.2.0?topic=server-different-end-line-characters-in-text-files</a>
Different end of line characters in text files
---
In general, z/OS UNIX text files contain a newline character at the end of each
line. In ASCII, newline is X'0A'. In EBCDIC, newline is X'15'. (For example,
ASCII code page ISO8859-1 and EBCDIC code page IBM-1047 translate back and forth
between these characters.) Windows programs normally use a carriage return
followed by a line feed character at the end of each line of a text file. In
ASCII, carriage return/line feed is X'0D'/X'0A'. In EBCDIC, carriage return/line
feed is X'0D'/X'15'. The tr command shown in the preceding example deletes all
of the carriage return characters. (Line feed and newline characters have the
same hexadecimal value.) The SMB server can translate end of line characters
from ASCII to EBCDIC and back but it does not change the type of delimiter (PC
versus z/OS UNIX) nor the number of characters in the file.


<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/zos/2.5.0?topic=options-record-format-recfm">https://www.ibm.com/docs/en/zos/2.5.0?topic=options-record-format-recfm</a>
Record Format (RECFM)
RECFM specifies the characteristics of the records in the data set as fixed-length (F),
variable-length (V), ASCII variable-length (D), or undefined-length (U). Blocked
records are specified as FB, VB, or DB. Spanned records are specified as VS, VBS,
DS, or DBS. You can also specify the records as fixed-length standard by using
FS or FBS. You can request track overflow for records other than standard format
by adding a T to the RECFM parameter (for example, by coding FBT). Track overflow
is ignored for PDSEs.
The type of print control can be specified to be in ANSI format-A, or in machine code
format-M. See
Using Optional Control Characters (<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/SSLTBW_2.5.0/com.ibm.zos.v2r5.idad400/occ.htm#occ)">https://www.ibm.com/docs/en/SSLTBW_2.5.0/com.ibm.zos.v2r5.idad400/occ.htm#occ)</a>
and z/OS DFSMS Macro Instructions for Data Sets (<a target="_blank" rel="noopener noreferrer" href="https://www.ibm.com/docs/en/SSLTBW_2.5.0/com.ibm.zos.v2r5.idad500/abstract.htm)">https://www.ibm.com/docs/en/SSLTBW_2.5.0/com.ibm.zos.v2r5.idad500/abstract.htm)</a>
for information about control characters.


<a target="_blank" rel="noopener noreferrer" href="https://docs.tibco.com/pub/mftps-zos/8.0.0/doc/html/GUID-A0CF702B-C126-43BE-86B2-8DF589FAD6BF.html">https://docs.tibco.com/pub/mftps-zos/8.0.0/doc/html/GUID-A0CF702B-C126-43BE-86B2-8DF589FAD6BF.html</a>
TIBCO¬Æ Managed File Transfer Platform Server for z/OS
RECFM={ F | FB | V | VB | U | VS | VBS}
Default=V
This parameter defines the significance of the character logical record length
(semantics of LRECL boundaries). You can specify fixed, variable, or system default
The valid values are as follows:
    - F: each string contains exactly the number of characters defined by the string length parameter.
    - FB:   all blocks and all logical record are fixed in size.
            One or more logical records reside in each block.
    - V:    the length of each string is less than or equal to the string length parameter.
    - VB:   blocks as well as logical record length can be of any size.
            One or more logical records reside in each block.
    - U:    blocks are of variable size. No logical records are used.
            The logical record length is displayed as zero.
            This record format is usually only used in load libraries.
            Block size must be used if you are specifying U.
    - VS:   records are variable and can span logical blocks.
            RECFM=VS is not supported when checkpoint restart is used.
    - VBS:  blocks as well as logical record length can be of any size.
            One or more logical records reside in each block.
            Records are variable and can span logical blocks.
            RECFM=VBS is not supported when checkpoint restart is used.


<hr><h2 id="macOS_OS">macOS OS</h2><hr>
you can enter emoji (and other Unicode characters) using standard operating
system tools‚Äîlike ctrl cmd space.


<a target="_blank" rel="noopener noreferrer" href="https://eclecticlight.co/2021/05/08/explainer-unicode-normalization-and-apfs/">https://eclecticlight.co/2021/05/08/explainer-unicode-normalization-and-apfs/</a>
Explainer: Unicode, normalization and APFS
hoakley  May 8, 2021
---
One of the oldest problems with Apple‚Äôs APFS file system is how it encodes file
and directory names using Unicode.


<hr><h2 id="Windows_OS">Windows OS</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/intl/international-support">https://learn.microsoft.com/en-us/windows/win32/intl/international-support</a>
jlf: I search which functionalities are available only to unicode apps...
- can be multilingual without managing code pages
- IME? not sure if it's only for unicode apps
- other?


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/59404120/what-is-the-difference-in-using-cstringw-cstringa-and-ct2w-ct2a-to-convert-strin">https://stackoverflow.com/questions/59404120/what-is-the-difference-in-using-cstringw-cstringa-and-ct2w-ct2a-to-convert-strin</a>
What is the difference in using CStringW/CStringA and CT2W/CT2A to convert strings?
    CString offers a number of conversion constructors to convert between ANSI and
    Unicode encoding. They are as convenient as they are dangerous, often masking bugs.

    By contrast, the Cs2d macros (where s = source, d = destination) work on raw
    C-style strings; no CString instances are created in the process of converting
    between character encodings.

    Both of the above perform a conversion with an implied ANSI codepage (either
    CP_THREAD_ACP or CP_ACP in case the _CONVERSION_DONT_USE_THREAD_LOCALE
    preprocessor symbol is defined). CP_ACP is particularly troublesome, as it's a
    process-global setting, that any thread can change at any time.

    Which one should you choose for your conversions? Neither of the above. Use the
    EX versions instead (see string and text classes for a full list).


<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/cpp/atl/string-and-text-classes?view=msvc-170">https://learn.microsoft.com/en-us/cpp/atl/string-and-text-classes?view=msvc-170</a>
String and Text Classes


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/15362859/getclipboarddata-cf-unicodetext">https://stackoverflow.com/questions/15362859/getclipboarddata-cf-unicodetext</a>
GetClipboardData (CF_UNICODETEXT)


<a target="_blank" rel="noopener noreferrer" href="https://jerrington.me/posts/2015-12-31-windows-debugging-for-fun-and-profit.html">https://jerrington.me/posts/2015-12-31-windows-debugging-for-fun-and-profit.html</a>
jlf: I reference this page for the code related to clipboard. Search for "locale".


<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/dataxchg/standard-clipboard-formats">https://learn.microsoft.com/en-us/windows/win32/dataxchg/standard-clipboard-formats</a>
Standard Clipboard Formats
CF_LOCALE
    Locale identifier (LCID) associated with text in the clipboard.
    The system uses the code page associated with CF_LOCALE to implicitly
    convert from CF_TEXT to CF_UNICODETEXT.
CF_TEXT
    Text format. Each line ends with a carriage return/linefeed (CR-LF) combination.
    A null character signals the end of the data. Use this format for ANSI text.
CF_UNICODETEXT
    Unicode text format. Each line ends with a carriage return/linefeed (CR-LF)
    combination. A null character signals the end of the data.


Locale
    <a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/intl/language-identifiers">https://learn.microsoft.com/en-us/windows/win32/intl/language-identifiers</a>
    A language identifier is a standard international numeric abbreviation
    for the language in a country or geographical region. Each language has
    a unique language identifier (data type LANGID), a 16-bit value that
    consists of a primary language identifier and a sublanguage identifier.
    +-------------------------+-------------------------+
    |     SubLanguage ID      |   Primary Language ID   |
    +-------------------------+-------------------------+
    15                    10  9                         0   bit

    <a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/intl/sort-order-identifiers">https://learn.microsoft.com/en-us/windows/win32/intl/sort-order-identifiers</a>
    A sort order identifier is defined in the form "_sortorder", at the end
    of the locale name used in the identifier, for example, "de-DE_phoneb",
    where "phoneb" is the sort order.
    The corresponding locale identifier is created as follows:
    MAKELCID(MAKELANGID(LANG_GERMAN, SUBLANG_GERMAN), SORT_GERMAN_PHONE_BOOK).

    <a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/intl/locale-identifiers">https://learn.microsoft.com/en-us/windows/win32/intl/locale-identifiers</a>
    Each locale has a unique identifier, a 32-bit value that consists of a
    language identifier and a sort order identifier.
    +-------------+---------+-------------------------+
    |   Reserved  | Sort ID |      Language ID        |
    +-------------+---------+-------------------------+
    31         20 19     16 15                      0   bit

    <a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/api/winnls/nf-winnls-getuilanguageinfo">https://learn.microsoft.com/en-us/windows/win32/api/winnls/nf-winnls-getuilanguageinfo</a>

    <a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/previous-versions/windows/embedded/ms930130(v=msdn.10)?redirectedfrom=MSDN">https://learn.microsoft.com/en-us/previous-versions/windows/embedded/ms930130(v=msdn.10)?redirectedfrom=MSDN</a>
    Locale Code Table
    jlf: obsolete, but for the moment I don't haver better.
    Correspondance  Locale identifier (LCID) &lt;--&gt; Default code page
    ---
    LCID    Code page   Language: sublanguage
    0x0436  1252        Afrikaans: South Africa
    0x041c  1250        Albanian: Albania
    0x1401  1256        Arabic: Algeria
    0x3c01  1256        Arabic: Bahrain
    etc...


<a target="_blank" rel="noopener noreferrer" href="https://devblogs.microsoft.com/oldnewthing/20161007-00/?p=94475">https://devblogs.microsoft.com/oldnewthing/20161007-00/?p=94475</a>
How can I get the default code page for a locale?
    UINT GetAnsiCodePageForLocale(LCID lcid)
    {
      UINT acp;
      int sizeInChars = sizeof(acp) / sizeof(TCHAR);
      if (GetLocaleInfo(lcid,
                        LOCALE_IDEFAULTANSICODEPAGE |
                        LOCALE_RETURN_NUMBER,
                        reinterpret_cast&lt;LPTSTR&gt;(&amp;acp),
                        sizeInChars) != sizeInChars) {
        // Oops - something went wrong
      }
      return acp;
    }


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/ltli/#dfn-locale-neutral">https://www.w3.org/TR/ltli/#dfn-locale-neutral</a>
Locale neutral
jlf: je pige que dalle
Locale-neutral. A non-linguistic field is said to be locale-neutral when it is
stored or exchanged in a format that is not specifically appropriate for any
given language, locale, or culture and which can be interpreted unambiguously
for presentation in a locale aware way.
Many specifications use a serialization scheme, such as those provided by
[XMLSCHEMA11-2] or [JSON-LD], to provide a locale neutral encoding of
non-linguistic fields in document formats or protocols.
A locale-neutral representation might itself be linked to a specific cultural
preference, but such linkages should be minimized.


<a target="_blank" rel="noopener noreferrer" href="http://archives.miloush.net/michkap/archive/2005/04/18/409095.html">http://archives.miloush.net/michkap/archive/2005/04/18/409095.html</a>
A few of the gotchas of WideCharToMultiByte
by Michael S. Kaplan, published on 2005/04/18 02:01 -04:00, original URI: <a target="_blank" rel="noopener noreferrer" href="http://blogs.msdn.com/b/michkap/archive/2005/04/18/409095.aspx">http://blogs.msdn.com/b/michkap/archive/2005/04/18/409095.aspx</a>


<a target="_blank" rel="noopener noreferrer" href="http://archives.miloush.net/michkap/archive/2005/04/19/409566.html">http://archives.miloush.net/michkap/archive/2005/04/19/409566.html</a>
A few of the gotchas of MultiByteToWideChar
by Michael S. Kaplan, published on 2005/04/19 04:30 -04:00, original URI: <a target="_blank" rel="noopener noreferrer" href="http://blogs.msdn.com/b/michkap/archive/2005/04/19/409566.aspx">http://blogs.msdn.com/b/michkap/archive/2005/04/19/409566.aspx</a>
---
jlf: I reached this page because the flag MB_COMPOSITE is not working!
This page brings the answer: the Microsoft doc has this note
    Note  For UTF-8 or code page 54936 (GB18030, starting with Windows Vista),
    dwFlags must be set to either 0 or MB_ERR_INVALID_CHARS. Otherwise, the function
    fails with ERROR_INVALID_FLAGS.
Uh?


<a target="_blank" rel="noopener noreferrer" href="http://archives.miloush.net/michkap/archive/2005/02/26/381020.html">http://archives.miloush.net/michkap/archive/2005/02/26/381020.html</a>
What the &amp;%#$ does MB_USEGLYPHCHARS do?
by Michael S. Kaplan, published on 2005/02/26 15:26 -05:00, original URI: <a target="_blank" rel="noopener noreferrer" href="http://blogs.msdn.com/b/michkap/archive/2005/02/26/381020.aspx">http://blogs.msdn.com/b/michkap/archive/2005/02/26/381020.aspx</a>


<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/apps/design/globalizing/use-utf8-code-page">https://learn.microsoft.com/en-us/windows/apps/design/globalizing/use-utf8-code-page</a>
Use UTF-8 code pages in Windows apps


<a target="_blank" rel="noopener noreferrer" href="https://mastodon.gamedev.place/@AshleyGullen/111109299141510319">https://mastodon.gamedev.place/@AshleyGullen/111109299141510319</a>
what it takes to pass a file path to a Windows API in C++


<a target="_blank" rel="noopener noreferrer" href="https://github.com/neacsum/utf8">https://github.com/neacsum/utf8</a>
This library simplifies usage of UTF-8 encoded strings under Win32
Related articles:
<a target="_blank" rel="noopener noreferrer" href="https://www.codeproject.com//Articles/5252037/Doing-UTF-8-in-Windows">https://www.codeproject.com//Articles/5252037/Doing-UTF-8-in-Windows</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.codeproject.com/Articles/5259868/Doing-UTF-8-in-Windows-Part-2-Tolower-or-Not-to-Lo">https://www.codeproject.com/Articles/5259868/Doing-UTF-8-in-Windows-Part-2-Tolower-or-Not-to-Lo</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.codeproject.com/Tips/5263944/UTF-8-in-Windows-INI-Files">https://www.codeproject.com/Tips/5263944/UTF-8-in-Windows-INI-Files</a>
---
Reddit review: <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/cpp/comments/174ee8q/doing_utf8_in_windows/">https://www.reddit.com/r/cpp/comments/174ee8q/doing_utf8_in_windows/</a>
---
This article about UTF-8 in Windows that does not discuss how to use a manifest
to get UTF-8 process ANSI codepage, directs people back to the 1990's.
Or pre-2019, at any rate.
---
Something else to note, if you're in the habit of keeping UTF-8 strings in
`std::string`, is that the Visual C++ version of `std::filesystem::path`
initialized from a `std::string` will use the default codepage for the process
to convert the path to UTF-16. That will result in interesting failures on
systems whose default codepage is MBCS. All without a single Windows API to be
seen in your source.
The solution to this is to upgrade to C++20 and use `std::u8string`, or to keep
filenames in `std::wstring` if you don't want to deal with the odd and
occasionally surprising limitations of `std::u8string`.


<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/windows/win32/sbscs/application-manifests#activeCodePage">https://learn.microsoft.com/en-us/windows/win32/sbscs/application-manifests#activeCodePage</a>
Application manifests - activeCodePage
---
On Windows 10, this element forces a process to use UTF-8 as the process code page.
On Windows 10, the only valid value for activeCodePage is UTF-8.
Starting in Windows 11, this element also allows selection of either the legacy
non-UTF-8 code page, or code pages for a specific locale for legacy application
compatibility. Modern applications are strongly encouraged to use Unicode.
On Windows 11, activeCodePage may also be set to the value Legacy or a locale
name such as en-US or ja-JP.


<a target="_blank" rel="noopener noreferrer" href="https://devblogs.microsoft.com/oldnewthing/20210527-00/?p=105255">https://devblogs.microsoft.com/oldnewthing/20210527-00/?p=105255</a>
How can I convert between IANA time zones and Windows registry-based time zones?
A copy of ICU has been included with Windows since Windows 10 Version 1703 (build 15063).
All you have to do is include icu.h, and you‚Äôre off to the races.
An advantage of using the version that comes with Windows is that it is actively
maintained and updated by the Windows team. If you need to run on older systems,
you can build your own copy from their fork of the ICU repo,
<a target="_blank" rel="noopener noreferrer" href="https://github.com/microsoft/icu">https://github.com/microsoft/icu</a>
but the job of servicing the project is now on you.


<hr><h2 id="Language_comparison">Language comparison</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://blog.kdheepak.com/my-unicode-cheat-sheet">https://blog.kdheepak.com/my-unicode-cheat-sheet</a>
Vim, Python, Julia and Rust.


<hr><h2 id="Regular_expressions">Regular expressions</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://regex101.com/">https://regex101.com/</a>
Testing a regular expression.
There is even a debugger!


<a target="_blank" rel="noopener noreferrer" href="https://www.regular-expressions.info/unicode.html">https://www.regular-expressions.info/unicode.html</a>
\X matches a grapheme


<a target="_blank" rel="noopener noreferrer" href="https://www.regular-expressions.info/posixbrackets.html">https://www.regular-expressions.info/posixbrackets.html</a>
POSIX Bracket Expressions
jlf: see the table in the section Character Classes


<a target="_blank" rel="noopener noreferrer" href="https://pypi.org/project/regex/">https://pypi.org/project/regex/</a>
&gt;&gt;&gt; a = "‡§¨‡§ø‡§ï‡•ç‡§∞‡§Æ ‡§Æ‡•á‡§∞‡•ã ‡§®‡§æ‡§Æ ‡§π‡•ã"
&gt;&gt;&gt; regex.findall(r'\X', a)
['‡§¨‡§ø', '‡§ï‡•ç', '‡§∞', '‡§Æ', ' ', '‡§Æ‡•á', '‡§∞‡•ã', ' ', '‡§®‡§æ', '‡§Æ', ' ', '‡§π‡•ã']
---
<a target="_blank" rel="noopener noreferrer" href="https://regex101.com/r/eD0eZ9/1">https://regex101.com/r/eD0eZ9/1</a>
---
jlf: the results above are correct extended grapheme clusters, but tailored
grapheme clusters will group '‡§ï‡•ç' '‡§∞' in one cluster ‡§ï‡•ç‡§∞


<a target="_blank" rel="noopener noreferrer" href="https://blog.burntsushi.net/ripgrep/">https://blog.burntsushi.net/ripgrep/</a>
ripgrep is faster than {grep, ag, git grep, ucg, pt, sift}
search for "unicode" and read...


<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/dotnet/standard/base-types/character-classes-in-regular-expressions">https://learn.microsoft.com/en-us/dotnet/standard/base-types/character-classes-in-regular-expressions</a>
Character classes in regular expressions


<a target="_blank" rel="noopener noreferrer" href="https://github.com/micromatch/posix-character-classes">https://github.com/micromatch/posix-character-classes</a>
POSIX character classes for creating regular expressions.
jlf: careful, not official. Looks similar to the table at
<a target="_blank" rel="noopener noreferrer" href="https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions">https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions</a>
    POSIX class	Equivalent to	Matches
    [:alnum:]	[A-Za-z0-9]                         digits, uppercase and lowercase letters
    [:alpha:]	[A-Za-z]                            upper- and lowercase letters
    [:ascii:]	[\x00-\x7F]                         ASCII characters
    [:blank:]	[ \t]                               space and TAB characters only
    [:cntrl:]	[\x00-\x1F\x7F]                     Control characters
    [:digit:]	[0-9]                               digits
    [:graph:]	[^ [:cntrl:]]                       graphic characters (all characters which have graphic representation)
    [:lower:]	[a-z]                               lowercase letters
    [:print:]	[[:graph:] ]                        graphic characters and space
    [:punct:]	[-!"#$%&amp;'()*+,./:;&lt;=&gt;?@[]^_`{|}~]   all punctuation characters (all graphic characters except letters and digits)
    [:space:]	[ \t\n\r\f\v]                       all blank (whitespace) characters, including spaces, tabs, new lines, carriage returns, form feeds, and vertical tabs
    [:upper:]	[A-Z]                               uppercase letters
    [:word:]	[A-Za-z0-9_]                        word characters
    [:xdigit:]	[0-9A-Fa-f]                         hexadecimal digits


<a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/icu/posix.html">https://unicode-org.github.io/icu/userguide/icu/posix.html</a>
C/POSIX Migration
    Character classes, point 7:
    For more about the problems with POSIX character classes in a Unicode context
    see Annex C: Compatibility Properties in Unicode Technical Standard #18: Unicode Regular Expressions
    <a target="_blank" rel="noopener noreferrer" href="http://www.unicode.org/reports/tr18/#Compatibility_Properties">http://www.unicode.org/reports/tr18/#Compatibility_Properties</a>
    and see the mailing list archives for the unicode list (on unicode.org).
    See also the ICU design document about C/POSIX character classes
    <a target="_blank" rel="noopener noreferrer" href="https://htmlpreview.github.io/?https://github.com/unicode-org/icu-docs/blob/main/design/posix_classes.html">https://htmlpreview.github.io/?https://github.com/unicode-org/icu-docs/blob/main/design/posix_classes.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/50570322/regex-pattern-matching-in-right-to-left-languages">https://stackoverflow.com/questions/50570322/regex-pattern-matching-in-right-to-left-languages</a>
Regex pattern matching in right-to-left languages
---
jlf: only one answer. Why control characters?
What I understand is that the bytes are in the spelling order of the characters.
The "/"
ooRexx returns the same sequence of bytes under macOS.
---
    /Store/ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™/ÿπÿ±ÿπ
     2F53746F72652F   D8B9D8B1D985D986D8AAD8ACD8A7D8AA   2F   D8B9D8B1D8B9
    |--------------| |--------------------------------| |--| |------------|
      "/Store/"                   ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™               /  i    ÿπÿ±ÿπ

    /Store/ÿπÿ±ÿπ/ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™
     2F53746F72652F   D8B9D8B1D8B9   2F   D8B9D8B1D985D986D8AAD8ACD8A7D8AA
    |--------------| |------------| |--| |--------------------------------|
      "/Store/"           ÿπÿ±ÿπ        /  i              ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™

    /Store/ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™/whatever
    2F53746F72652F D8B9D8B1D985D986D8AAD8ACD8A7D8AA  2F  7768617465766572
    |------------| |------------------------------| |--| |--------------|
      "/Store/"                ÿπÿ±ŸÖŸÜÿ™ÿ¨ÿßÿ™              /        whatever


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/20641297/unicode-characters-in-regex">https://stackoverflow.com/questions/20641297/unicode-characters-in-regex</a>
Unicode characters in Regex


<hr><h2 id="Test_cases__test_cases__tests_files">Test cases, test-cases, tests files</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/lemire/unicode_lipsum">https://github.com/lemire/unicode_lipsum</a>


<hr><h2 id="font_bold__italic__strikethrough__underline__backwards__upside_down">font bold, italic, strikethrough, underline, backwards, upside down</h2><hr>
I remember I saw an open-sourced implementation, but forgot to note it.
The URLs below are not providing a link to an open-sourced implementation, to
remove sooner or later.
<a target="_blank" rel="noopener noreferrer" href="https://convertcase.net/unicode-text-converter/">https://convertcase.net/unicode-text-converter/</a>
<a target="_blank" rel="noopener noreferrer" href="https://yaytext.com/">https://yaytext.com/</a>
<a target="_blank" rel="noopener noreferrer" href="https://capitalizemytitle.com/">https://capitalizemytitle.com/</a>
<a target="_blank" rel="noopener noreferrer" href="https://capitalizemytitle.com/fancy-text-generator/">https://capitalizemytitle.com/fancy-text-generator/</a>
<a target="_blank" rel="noopener noreferrer" href="http://slothsoft.net/UnicodeMapper/">http://slothsoft.net/UnicodeMapper/</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.fontgenerator.org/">https://www.fontgenerator.org/</a>
<a target="_blank" rel="noopener noreferrer" href="https://peterwunder.de/projects/prettify/">https://peterwunder.de/projects/prettify/</a>
<a target="_blank" rel="noopener noreferrer" href="https://texteditor.com/">https://texteditor.com/</a>


<a target="_blank" rel="noopener noreferrer" href="https://gwern.net/utext">https://gwern.net/utext</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=38016735">https://news.ycombinator.com/item?id=38016735</a>
Utext: Rich Unicode Documents (gwern.net)
An esoteric document proposal: abuse Unicode to create the fanciest possible ‚Äòplain text‚Äô documents.


<a target="_blank" rel="noopener noreferrer" href="https://fonts.google.com/noto">https://fonts.google.com/noto</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/mail-arch/unicode-ml/y2019-m01/0121.html">https://www.unicode.org/mail-arch/unicode-ml/y2019-m01/0121.html</a>
Encoding italic (was: A last missing link)


<hr><h2 id="youtube">youtube</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/playlist?list=PLMc927ywQmTNQrscw7yvaJbAbMJDIjeBh">https://www.youtube.com/playlist?list=PLMc927ywQmTNQrscw7yvaJbAbMJDIjeBh</a>
Videos from Unicode's Overview of Internationalization and Unicode Projects


<hr><h2 id="xxx_lang">xxx lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://rosettacode.org/wiki/Unicode_strings">https://rosettacode.org/wiki/Unicode_strings</a>

<a target="_blank" rel="noopener noreferrer" href="https://langdev.stackexchange.com/questions/1493/how-have-modern-language-designs-dealt-with-unicode-strings">https://langdev.stackexchange.com/questions/1493/how-have-modern-language-designs-dealt-with-unicode-strings</a>
How have modern language designs dealt with Unicode strings?
Asked 2023-06-13
Answer for
- Swift
- Rust
- Python 3
- Treat it as a (mostly) library issue
jlf: the Swift part is interesting, the rest is bof.
    In order to speed up repeated accesses to utf16, UTF-8 strings may put a breadcrumbs pointer after the null terminator:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/1532fb188c55f29de7bf8aaee94896557b3a3db1/stdlib/public/core/StringStorage.swift#L157">https://github.com/apple/swift/blob/1532fb188c55f29de7bf8aaee94896557b3a3db1/stdlib/public/core/StringStorage.swift#L157</a>

    The breadcrumbs are a list of the UTF-8 offsets of every 64th UTF-16 code unit:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/483087a47dfb56e78fcc20ef2b43085ebfb48ea0/stdlib/public/core/StringBreadcrumbs.swift">https://github.com/apple/swift/blob/483087a47dfb56e78fcc20ef2b43085ebfb48ea0/stdlib/public/core/StringBreadcrumbs.swift</a>

    A string stores whether it has breadcrumbs in an unused bit in its capacity field:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/1532fb188c55f29de7bf8aaee94896557b3a3db1/stdlib/public/core/StringStorage.swift#L45">https://github.com/apple/swift/blob/1532fb188c55f29de7bf8aaee94896557b3a3db1/stdlib/public/core/StringStorage.swift#L45</a>


<a target="_blank" rel="noopener noreferrer" href="http://xahlee.info/comp/unicode_essays_index.html">http://xahlee.info/comp/unicode_essays_index.html</a>
Unicode for Programers
jlf: this page contains several URL for programming languages. Short articles
but there is maybe something to learn. [later] After review, no so many things
to learn, the articles are very very short...


<hr><h2 id="Ada_lang">Ada lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://docs.adacore.com/live/wave/xmlada/html/xmlada_ug/unicode.html">https://docs.adacore.com/live/wave/xmlada/html/xmlada_ug/unicode.html</a>


<a target="_blank" rel="noopener noreferrer" href="http://www.dmitry-kazakov.de/ada/strings_edit.htm">http://www.dmitry-kazakov.de/ada/strings_edit.htm</a>


UXStrings Ada Unicode Extended Strings
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/ada/comments/t4hpip/ann_uxstrings_package_available_uxs_20220226/">https://www.reddit.com/r/ada/comments/t4hpip/ann_uxstrings_package_available_uxs_20220226/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/Blady-Com/UXStrings">https://github.com/Blady-Com/UXStrings</a>
---
2023.10.14 <a target="_blank" rel="noopener noreferrer" href="https://groups.google.com/g/comp.lang.ada/c/rWqDxiOwa1g">https://groups.google.com/g/comp.lang.ada/c/rWqDxiOwa1g</a>
[ANN] Release of UXStrings 0.6.0
- Add string convenient subprograms [2]: Contains, Ends_With,Starts_With,
[2] <a target="_blank" rel="noopener noreferrer" href="https://github.com/Blady-Com/UXStrings/blob/master/src/uxstrings3.ads#L346">https://github.com/Blady-Com/UXStrings/blob/master/src/uxstrings3.ads#L346</a>
jlf: see <a target="_blank" rel="noopener noreferrer" href="https://github.com/Blady-Com/UXStrings/blob/master/src/uxstrings3.adb">https://github.com/Blady-Com/UXStrings/blob/master/src/uxstrings3.adb</a>
After a quick look, I still don't know which kind of position is managed.
There is a parameter Case_Sensitivity, but I never see it used with a position
(that's the tricky part)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/AdaForge/Thematics/wiki/Unicode-and-String-manipulations">https://github.com/AdaForge/Thematics/wiki/Unicode-and-String-manipulations</a>
Unicode and String manipulations in UTF-8, UTF-16, ...


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/48829940/utf-8-on-windows-with-ada">https://stackoverflow.com/questions/48829940/utf-8-on-windows-with-ada</a>
UTF-8 on Windows with Ada


<a target="_blank" rel="noopener noreferrer" href="https://github.com/AdaCore/VSS/">https://github.com/AdaCore/VSS/</a>
High level string and text processing library


<a target="_blank" rel="noopener noreferrer" href="https://blog.adacore.com/vss-cursors-iterators-and-markers">https://blog.adacore.com/vss-cursors-iterators-and-markers</a>
VSS (Virtual String Subsystem): Cursors, Iterators and Markers
jlf: bof...


<hr><h2 id="Awk_lang">Awk lang</h2><hr>
Brian Kernighan adds Unicode support to Awk
<a target="_blank" rel="noopener noreferrer" href="https://github.com/onetrueawk/awk/commit/9ebe940cf3c652b0e373634d2aa4a00b8395b636">https://github.com/onetrueawk/awk/commit/9ebe940cf3c652b0e373634d2aa4a00b8395b636</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/onetrueawk/awk/tree/unicode-support">https://github.com/onetrueawk/awk/tree/unicode-support</a>
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=32534173">https://news.ycombinator.com/item?id=32534173</a>


<hr><h2 id="C___lang__cpp_lang__Boost">C++ lang, cpp lang, Boost</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://en.cppreference.com/w/cpp/language/string_literal">https://en.cppreference.com/w/cpp/language/string_literal</a>
String literal
(referenced by Adrian)
Some examples:
<a target="_blank" rel="noopener noreferrer" href="https://learn.microsoft.com/en-us/cpp/cpp/string-and-character-literals-cpp">https://learn.microsoft.com/en-us/cpp/cpp/string-and-character-literals-cpp</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=iQWtiYNK3kQ">https://www.youtube.com/watch?v=iQWtiYNK3kQ</a>
A Crash Course in Unicode for C++ Developers - Steve Downey - [CppNow 2021]
jlf: good video for pronunciation
  57:16 Algorithms
1:12:27 The future for C++ (you can stop here, not very interesting)


02/06/2021
<a target="_blank" rel="noopener noreferrer" href="http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1238r1.html">http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2019/p1238r1.html</a>
    SG16 initial Unicode direction and guidance for C++20 and beyond.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sg16-unicode/sg16">https://github.com/sg16-unicode/sg16</a>
    SG16 is an ISO/IEC JTC1/SC22/WG21 C++ study group tasked with improving Unicode and text processing support within the C++ standard.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sg16-unicode/sg16-meetings">https://github.com/sg16-unicode/sg16-meetings</a>
    Summaries of SG16 virtual meetings
<a target="_blank" rel="noopener noreferrer" href="https://lists.isocpp.org/mailman/listinfo.cgi/sg16">https://lists.isocpp.org/mailman/listinfo.cgi/sg16</a>
    SG16 mailing list


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p1629r1.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2020/p1629r1.html</a>
P1629R1
Transcoding the üåê - Standard Text Encoding
Published Proposal, 2020-03-02
---
jlf: referenced by Zach Laine in P2728R0.
[P1629R1] from JeanHeyd Meneide is a much more ambitious proposal that aims to
standardize a general-purpose text encoding conversion mechanism. This proposal
is not at odds with P1629; the two proposals have largely orthogonal aims. This
proposal only concerns itself with UTF interconversions, which is all that is
required for Unicode support. P1629 is concerned with those conversions, plus a
lot more. Accepting both proposals would not cause problems; in fact, the APIs
proposed here could be used to implement parts of the P1629 design.


01/06/2021
Zach Laine
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=944GjKxwMBo">https://www.youtube.com/watch?v=944GjKxwMBo</a>
<a target="_blank" rel="noopener noreferrer" href="https://tzlaine.github.io/text/doc/html/boost_text__proposed_/the_text_layer.html">https://tzlaine.github.io/text/doc/html/boost_text__proposed_/the_text_layer.html</a>
<a target="_blank" rel="noopener noreferrer" href="https://tzlaine.github.io/text/doc/html/index.html">https://tzlaine.github.io/text/doc/html/index.html</a>
The Text Layer
<a target="_blank" rel="noopener noreferrer" href="https://tzlaine.github.io/text/doc/html/">https://tzlaine.github.io/text/doc/html/</a>
Chapter 1. Boost.Text (Proposed) - 2018
<a target="_blank" rel="noopener noreferrer" href="https://github.com/tzlaine/text">https://github.com/tzlaine/text</a>
    last commit :
        master                          26/09/2020
        boost_serialization             24/10/2019
        coroutines                      25/08/2020
        experimental                    13/11/2019
        gh-pages                        04/09/2020
        optimization                    27/10/2019
        rope_free_fn_reimplementation   26/07/2020
No longer working on this project ?
---
Restart working on 22/03/2022
Zach's library was last discussed at the 2023-05-10 SG16 meeting; see
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sg16-unicode/sg16-meetings#may-10th-2023.">https://github.com/sg16-unicode/sg16-meetings#may-10th-2023.</a>
---
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=AoLl\_ZZqyOk">https://www.youtube.com/watch?v=AoLl\_ZZqyOk</a>
Applying the Lessons of std::ranges to Unicode in the C++ Standard Library - Zach Laine CppNow 2023


<a target="_blank" rel="noopener noreferrer" href="https://isocpp.org/files/papers/P2728R0.html">https://isocpp.org/files/papers/P2728R0.html</a> (see more recent version below)
Unicode in the Library, Part 1: UTF Transcoding
Document #: P2728R0
Date:       2022-11-20
Reply-to:   Zach Laine  &lt;whatwasthataddress@gmail.com&gt;
---
New version:
<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2728r5.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2728r5.html</a>
Document #: P2728R5
Date:       2023-07-05
---
latest published version:
<a target="_blank" rel="noopener noreferrer" href="https://wg21.link/p2728">https://wg21.link/p2728</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2729r0.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2729r0.html</a>
Unicode in the Library, Part 2: Normalization
Document #: P2729R0
Date:       2022-11-20
Reply-to:   Zach Laine  &lt;whatwasthataddress@gmail.com&gt;


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2773r0.pdf">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2773r0.pdf</a>
paper D2773R0 by Corentin Jabot


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p1949r7.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2021/p1949r7.html</a>
C++ Identifier Syntax using Unicode Standard Annex 31
Document #: P1949R7
Date:       2021-04-12
---
Adopt Unicode Annex 31 as part of C++ 23.
- That C++ identifiers match the pattern (XID_Start + _ ) + XID_Continue*.
- That portable source is required to be normalized as NFC.
- That using unassigned code points be ill-formed.
This proposal also recommends adoption of Unicode normalization form C (NFC)
for identifiers to ensure that when compared, identifiers intended to be the
same will compare as equal. Legacy encodings are generally naturally in NFC when
converted to Unicode. Most tools will, by default, produce NFC text.
Some scripts require the use of characters as joiners that are not allowed by
base UAX #31, these will no longer be available as identifiers in C++.
As a side-effect of adopting the identifier characters from UAX #31, using emoji
in or as identifiers becomes ill-formed.


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2528r0.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2528r0.html</a>
C++ Identifier Security using Unicode Standard Annex 39
Document #: P2538R0
Date:       2022-01-22


14/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/non-unicode-in-cpp/">https://hsivonen.fi/non-unicode-in-cpp/</a>
Same contents in sg16 mailing list + feedbacks
<a target="_blank" rel="noopener noreferrer" href="https://lists.isocpp.org/sg16/2019/04/0309.php">https://lists.isocpp.org/sg16/2019/04/0309.php</a>


03/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=27695412">https://news.ycombinator.com/item?id=27695412</a>
Any Encoding, Ever ‚Äì ztd.text and Unicode for C++


14/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/non-unicode-in-cpp/">https://hsivonen.fi/non-unicode-in-cpp/</a>
It‚Äôs Time to Stop Adding New Features for Non-Unicode Execution Encodings in C++
The Microsoft Code Page 932 Issue


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/58878651/what-is-the-printf-formatting-character-for-char8-t/58895428#58895428.">https://stackoverflow.com/questions/58878651/what-is-the-printf-formatting-character-for-char8-t/58895428#58895428.</a>
What is the printf() formatting character for char8_t *?
jlf: todo read it? not sure yet if it's useful to read.
Referenced from <a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/2020-April/008579.html">https://corp.unicode.org/pipermail/unicode/2020-April/008579.html</a>
Basic Unicode character/string support absent even in modern C++


<a target="_blank" rel="noopener noreferrer" href="https://github.com/nemtrif/utfcpp/">https://github.com/nemtrif/utfcpp/</a>
referenced from <a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/2020-April/008582.html">https://corp.unicode.org/pipermail/unicode/2020-April/008582.html</a>
Basic Unicode character/string support absent even in modern C++


<a target="_blank" rel="noopener noreferrer" href="https://www.boost.org/doc/libs/1_80_0/libs/locale/doc/html/index.html">https://www.boost.org/doc/libs/1_80_0/libs/locale/doc/html/index.html</a>
Boost.Locale
Boost.Locale uses the-state-of-the-art Unicode and Localization library: ICU - International Components for Unicode.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/uni-algo/uni-algo">https://github.com/uni-algo/uni-algo</a>
Unicode Algorithms Implementation for C/C++
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/cpp/comments/xspvn4/unialgo_v050_modern_unicode_library/">https://www.reddit.com/r/cpp/comments/xspvn4/unialgo_v050_modern_unicode_library/</a>
uni-algo v0.5.0: Modern Unicode Library
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/cpp/comments/vtgckq/new_unicode_library/">https://www.reddit.com/r/cpp/comments/vtgckq/new_unicode_library/</a>
Older post with more infos
<a target="_blank" rel="noopener noreferrer" href="https://github.com/uni-algo/uni-algo-single-include">https://github.com/uni-algo/uni-algo-single-include</a>
Single include version for Unicode Algorithms Implementation
This repository contains single include version of uni-algo library.


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/cpp/comments/14t2lzm/unialgo_v100_modern_unicode_library/">https://www.reddit.com/r/cpp/comments/14t2lzm/unialgo_v100_modern_unicode_library/</a>
uni-algo v1.0.0: Modern Unicode Library
---
jlf: see the critics of Zach Laine's library... mg152 has good arguments.
---
jlf: this library is referenced in the comments
<a target="_blank" rel="noopener noreferrer" href="https://github.com/hikogui/hikogui/tree/main/src/hikogui/unicode">https://github.com/hikogui/hikogui/tree/main/src/hikogui/unicode</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/hikogui/hikogui/tree/main/tools/ucd">https://github.com/hikogui/hikogui/tree/main/tools/ucd</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/hikogui/hikogui/tree/main/src/hikogui/unicode">https://github.com/hikogui/hikogui/tree/main/src/hikogui/unicode</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/hikogui/hikogui">https://github.com/hikogui/hikogui</a>
Modern accelerated GUI
jlf: the point is not the GUI, but the tools to parse Unicode UCD.
See <a target="_blank" rel="noopener noreferrer" href="https://github.com/hikogui/hikogui/tree/main/tools">https://github.com/hikogui/hikogui/tree/main/tools</a>
---
Comment of the author in <a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/cpp/comments/vtgckq/new_unicode_library/">https://www.reddit.com/r/cpp/comments/vtgckq/new_unicode_library/</a>
I recently discovered a way to compress the unicode-data-set, while still being
able to do quick lookups, with a single associative indirection.
Basically you chunk the data in groups of 32 entries. Then you de-duplicate
these chunks and make a index table (about 64kbyte) that points to the chunks.
This works because a code-point is only 21 bits, which you can split in 16 bit
msb and 5 bit lsb. This means that the index table has less than 64k uint16_t
entries.
My data is including the index around 700 KByte. With the following data:
    general category: 5 bit
    grapheme cluster break: 4
    line break class: 6
    word break property: 5
    sentence break property: 4
    east asian width: 3
    bidi class:5
    bidi bracket type: 2
    bidi mirroring glyph: 16
    ccc: 8
    script: 8
    decomposition type: 5
    decomposition index: 21 (decomposition table not included in the 700kbyte)
    composition index: 14 (composition table not included in the 700kbyte)
Of the 128 bits per entry, 22 bits are currently unused. It is also possible to
compress a single entry. For example ccc is always zero for non-composing
code-points, so it could share those bits with properties that are only allowed
for non-composing code-points.


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=38424689">https://news.ycombinator.com/item?id=38424689</a>
Bjarne Stroustrup Quotes (stroustrup.com)
---
Interesting discussion about strings (not limited to C++): search for "string".


<a target="_blank" rel="noopener noreferrer" href="https://www.sandordargo.com/blog/2023/11/29/cpp23-unicode-support">https://www.sandordargo.com/blog/2023/11/29/cpp23-unicode-support</a>
C++23: Growing unicode support
---
The standardization committee has accepted (at least) four papers which clearly
show a growing Unicode support in C++23.
- C++ Identifier Syntax using Unicode Standard Annex 31
- Remove non-encodable wide character literals and multicharacter wide character literals
- Delimited escape sequences
- Named universal character escapes
        U'\N{LATIN CAPITAL LETTER A WITH MACRON}' // Equivalent to U'\u0100'
        u8"\N{LATIN CAPITAL LETTER A WITH MACRON}\N{COMBINING GRAVE ACCENT}" // Equivalent to u8"\u0100\u0300"
    One of the concerns was the sheer size of the Unicode name database that
    contains the codes (e.g. U+0100) and the names (e.g. {LATIN CAPITAL LETTER A
    WITH MACRON}). It‚Äôs around 1.5 MiB which can significantly impact the size
    of compiler distributions. The authors proved that a non-naive
    implementation can be around 300 KiB or even less.

    jlf: next point sounds discutable, no?
    Another open question was how to accept the Unicode-assigned names.
    Is {latin capital letter a with macron} just as good as
    {LATIN CAPITAL LETTER A WITH MACRON}?
    Or what about {LATIN_CAPITAL_LETTER_A_WITH_MACRON}?
    While the Unicode consortium standardized an algorithm called UAX44-LM2 for
    that purpose and it‚Äôs quite permissive, language implementors barely follow
    it. C++ is going to require an exact match with the database therefore the
    answer to the previous question is no, {latin capital letter a with macron}
    is not the same as {LATIN CAPITAL LETTER A WITH MACRON}. On the other hand,
    if there will be a strong need, the requirements can be relaxed in a later
    version.


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2071r2.html">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2022/p2071r2.html</a>
Named universal character escapes
---
jlf: they don't want to support UAX44-LM2
jlf: todo, read the section "Design considerations"


<hr><h2 id="cRexx_lang">cRexx lang</h2><hr>
cRexx uses this library:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/sheredom/utf8.h">https://github.com/sheredom/utf8.h</a>
---
Codepoint Case
Various functions provided will do case insensitive compares, or transform utf8
strings from one case to another. Given the vastness of unicode, and the authors
lack of understanding beyond latin codepoints on whether case means anything,
the following categories are the only ones that will be checked in case insensitive code:
    ASCII
    Latin-1 Supplement
    Latin Extended-A
    Latin Extended-B
    Greek and Coptic
    Cyrillic


<hr><h2 id="DotNet__CoreFx">DotNet, CoreFx</h2><hr>
28/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/corefxlab/issues/2368">https://github.com/dotnet/corefxlab/issues/2368</a>
Scenarios and Design Philosophy - UTF-8 string support

    <a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/GrabYourPitchforks/901684d0aa1d2440eb378d847cfc8607">https://gist.github.com/GrabYourPitchforks/901684d0aa1d2440eb378d847cfc8607</a> (jlf: replaced by the following URL)
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/corefx/issues/34094">https://github.com/dotnet/corefx/issues/34094</a> (go directly to next URL)
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/runtime/issues/28204">https://github.com/dotnet/runtime/issues/28204</a>
    Motivations and driving principles behind the Utf8Char proposal

    <a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/runtime/issues/933">https://github.com/dotnet/runtime/issues/933</a>
    The NuGet package generally follows the proposal in dotnet/corefxlab#2350, which
    is where most of the discussion has taken place. It's a bit aggravating that the
    discussion is split across so many different forums, I know. :(

        ceztko
        I noticed dotnet/corefxlab#2350 just got closed. Did the discussion moved
        somewhere else about more UTF8 first citizen support efforts?

        @ceztko The corefxlab repo was archived, so open issues were closed to
        support that effort. That thread also got so large that it was difficult
        to follow. @krwq is working on restructuring the conversation so that we
        can continue the discussion in a better forum.

        jlf
        Not clear where the discussion is continued...
        This URL just show some tags, one of them is "Future".
        <a target="_blank" rel="noopener noreferrer" href="https://github.com/orgs/dotnet/projects/7#card-33368432">https://github.com/orgs/dotnet/projects/7#card-33368432</a>


    <a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/corefxlab/issues/2350">https://github.com/dotnet/corefxlab/issues/2350</a>
    Utf8String design discussion - last edited 14-Sep-19
    Tons of comments, with this conclusion:
    The discussion in this issue is too long and github has troubles rendering it.
    I think we should close this issue and start a new one in dotnet/runtime.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/runtime/tree/main">https://github.com/dotnet/runtime/tree/main</a>
.Net runtime
jlf: could be useful
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dotnet/runtime/blob/main/src/libraries/System.Console/src/System/Console.cs">https://github.com/dotnet/runtime/blob/main/src/libraries/System.Console/src/System/Console.cs</a>


<hr><h2 id="Dafny_lang">Dafny lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://corp.unicode.org/pipermail/unicode/2021-May/009434.html">https://corp.unicode.org/pipermail/unicode/2021-May/009434.html</a>
Dafny natively supports expressing statements about sets
and contract programming and a toy implementation turned out to be a fairly
rote translation of the Unicode spec.  Dafny is also transpilation focused,
so the primary interface must be highly functional and encoding neutral.


<hr><h2 id="Dart_lang">Dart lang</h2><hr>
Dart SDK uses ICU4X?
jlf: to investigate...
---
On Fuchsia, the Dart SDK uses createTimeZone() with metazone names obtained from the OS (usage site).
ICU4X currently only supports this stuff with BCP-47 ids. We should have a way to go from metazone names to BCP-47 ids.
I suspect this is already part of the plan but I'm not sure if there's a specific issue filed (@nordzilla?)
---
In the link you posted, it shows "America/New_York", which is an IANA time zone name, not a metazone name.
Did you mean to ask about IANA-to-BCP47 mapping? That would be #2909


<a target="_blank" rel="noopener noreferrer" href="https://github.com/dart-lang/sdk/blob/main/sdk/lib/core/string.dart">https://github.com/dart-lang/sdk/blob/main/sdk/lib/core/string.dart</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dart-lang/sdk/blob/e995cb5f7cd67d39c1ee4bdbe95c8241db36725f/pkg/analyzer/lib/source/source_range.dart">https://github.com/dart-lang/sdk/blob/e995cb5f7cd67d39c1ee4bdbe95c8241db36725f/pkg/analyzer/lib/source/source_range.dart</a>

<a target="_blank" rel="noopener noreferrer" href="https://github.com/dart-lang/">https://github.com/dart-lang/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dart-lang/language">https://github.com/dart-lang/language</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dart-lang/sdk">https://github.com/dart-lang/sdk</a>

<a target="_blank" rel="noopener noreferrer" href="https://dart.dev/guides/language/language-tour#strings">https://dart.dev/guides/language/language-tour#strings</a>
A Dart string (String object) holds a sequence of UTF-16 code units.


<a target="_blank" rel="noopener noreferrer" href="https://dart.dev/guides/language/language-tour#runes-and-grapheme-clusters">https://dart.dev/guides/language/language-tour#runes-and-grapheme-clusters</a>
In Dart, runes expose the Unicode code points of a string.
You can use the characters package to view or manipulate user-perceived
characters, also known as Unicode (extended) grapheme clusters.


<a target="_blank" rel="noopener noreferrer" href="https://dart.dev/guides/libraries/library-tour#strings-and-regular-expressions">https://dart.dev/guides/libraries/library-tour#strings-and-regular-expressions</a>


<a target="_blank" rel="noopener noreferrer" href="https://pub.dev/packages/characters">https://pub.dev/packages/characters</a>
Characters are strings viewed as sequences of user-perceived characters,
also known as Unicode (extended) grapheme clusters.
The Characters class allows access to the individual characters of a string,
and a way to navigate back and forth between them using a CharacterRange.

<a target="_blank" rel="noopener noreferrer" href="https://medium.com/dartlang/dart-string-manipulation-done-right-5abd0668ba3e">https://medium.com/dartlang/dart-string-manipulation-done-right-5abd0668ba3e</a>
Like many other programming languages designed before emojis started to dominate
our daily communications and the rise of multilingual support in commercial apps,
Dart represents a string as a sequence of UTF-16 code units.
---
jlf: they say that the Dart users are not aware of the Characters package.
They try to improve the situation in the Flutter framework, but they are not
very happy of the situation:
Those mitigations can help, but they are limited to string manipulations
performed in the context of a Flutter project. We need to carefully measure
their effectiveness after they become available. A more complete solution at the
Dart language level will likely require migration of at least some existing code,
although a few options (for example, static extension types) might make breaking
changes manageable.
More technical investigation is needed to fully understand the trade-offs.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/robertbastian/icu4x/tree/dart/ffi/capi/dart/package">https://github.com/robertbastian/icu4x/tree/dart/ffi/capi/dart/package</a>
jlf: A fork with DART FFI


<hr><h2 id="Elixir_lang">Elixir lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://elixir-lang.org/">https://elixir-lang.org/</a>
"Elixir" |&gt; String.graphemes() |&gt; Enum.frequencies()        %{"E" =&gt; 1, "i" =&gt; 2, "l" =&gt; 1, "r" =&gt; 1, "x" =&gt; 1}
---
"Elixir"~text~reduce(by: "characters", initial: .stem~new~~put(0)){accu[item] += 1}=
    a Stem (5 items)
    'E' :  1
    'i' :  2
    'l' :  1
    'r' :  1
    'x' :  1


<a target="_blank" rel="noopener noreferrer" href="https://hexdocs.pm/elixir/String.html">https://hexdocs.pm/elixir/String.html</a>
Strings in Elixir are UTF-8 encoded binaries.
Works at grapheme level.
The functions in this module rely on the Unicode Standard, but do not contain
any of the locale specific behaviour.
To act according to the Unicode Standard, many functions in this module run in
linear time, as they need to traverse the whole string considering the proper
Unicode code points.
For example, String.length/1 will take longer as the input grows.
On the other hand, Kernel.byte_size/1 always runs in constant time (i.e.
regardless of the input size).
---
Interesting: they manage correctly the upper/lower without using a locale.
upcase(string, mode \\ :default)
Converts all characters in the given string to uppercase according to mode.
mode may be :default, :ascii, :greek or :turkic.
    The :default mode considers all non-conditional transformations outlined in the Unicode standard.
    :ascii uppercases only the letters a to z.
    :greek includes the context sensitive mappings found in Greek.
    :turkic properly handles the letter i with the dotless variant.


<a target="_blank" rel="noopener noreferrer" href="https://hexdocs.pm/elixir/unicode-syntax.html">https://hexdocs.pm/elixir/unicode-syntax.html</a>
Strings are UTF-8 encoded.
Charlists are lists of Unicode code points. In such cases, the contents are kept
as written by developers, without any transformation.
Elixir allows Unicode characters in its variables, atoms, and calls.
From now on, we will refer to those terms as identifiers.
The characters allowed in identifiers are the ones specified by Unicode.
Elixir normalizes all characters to be the in the NFC form.
Mixed-script identifiers are not supported for security reasons.
    –∞dmin
    "–∞dmin"~text~unicodecharacters==
    an Array (shape [5], 5 items)
     1 : ( "–∞"   U+0430 Ll 1 "CYRILLIC SMALL LETTER A" )
     2 : ( "d"   U+0064 Ll 1 "LATIN SMALL LETTER D" )
     3 : ( "m"   U+006D Ll 1 "LATIN SMALL LETTER M" )
     4 : ( "i"   U+0069 Ll 1 "LATIN SMALL LETTER I" )
     5 : ( "n"   U+006E Ll 1 "LATIN SMALL LETTER N" )
The character must either be all in Cyrillic or all in Latin.
The only mixed-scripts that Elixir allows, according to the Highly Restrictive
Unicode recommendations, are:
    Latin and Han with Bopomofo
    Latin and Japanese
    Latin and Korean
Elixir will also warn on confusable identifiers in the same file.
For example, Elixir will emit a warning if you use both variables –∞ (Cyrillic)
and –∞ (Latin) in your code.
Elixir implements the requirements outlined in the Unicode Annex #31
(<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr31/)">https://www.unicode.org/reports/tr31/)</a>
Elixir does not allow the use of ZWJ or ZWNJ in identifiers and therefore does
not implement R1a.
Bidirectional control characters are also not supported.
R1b is guaranteed for backwards compatibility purposes.
Elixir supports only code points \t (0009), \n (000A), \r (000D) and \s (0020)
as whitespace and therefore does not follow requirement R3.
R3 requires a wider variety of whitespace and syntax characters to be supported.


<hr><h2 id="Factor_lang">Factor lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="http://docs.factorcode.org/content/article-unicode.html">http://docs.factorcode.org/content/article-unicode.html</a>

<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-1.html">http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-1.html</a>
JLF : bof...

<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-2.html">http://useless-factor.blogspot.fr/2007/02/doing-unicode-right-part-2.html</a>

<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-3.html">http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-3.html</a>

<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-4.html">http://useless-factor.blogspot.fr/2007/08/unicode-implementers-guide-part-4.html</a>
grapheme breaking

<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/08/r-597-rs-unicode-library-is-broken.html">http://useless-factor.blogspot.fr/2007/08/r-597-rs-unicode-library-is-broken.html</a>


<a target="_blank" rel="noopener noreferrer" href="http://useless-factor.blogspot.fr/2007/02/more-string-parsing.html">http://useless-factor.blogspot.fr/2007/02/more-string-parsing.html</a>
UTF-8/16 encoder/decoder

    I used a design pattern known as a sentinel, which helps me cross-cut pointcutting concerns
    by instantiating objects which encapsulate the state of the parser. I never mutate these,
    and the program is purely functional except for the use of make (which could trivially be
    changed into a less efficient map [ ] subset, sacrificing efficiency and some terseness
    but making it functional).

    TUPLE: new ;
    TUPLE: double val ;
    TUPLE: quad2 val ;
    TUPLE: quad3 val ;

    : bad-char CHAR: ? ;

    GENERIC: (utf16le) ( char state -- state )
    M: new (utf16le)
        drop &lt;double&gt; ;
    M: double (utf16le)
        over -3 shift BIN: 11011 = [
            over BIN: 100 bitand 0 =
            [ double-val swap BIN: 11 bitand 8 shift bitor &lt;quad2&gt; ]
            [ 2drop bad-char , &lt;new&gt; ] if
        ] [ double-val swap 8 shift bitor , &lt;new&gt; ] if ;
    M: quad2 (utf16le)
        quad2-val 10 shift bitor &lt;quad3&gt; ;
    M: quad3 (utf16le)
        over -2 shift BIN: 110111 = [
            swap BIN: 11 bitand 8 shift
            swap quad3-val bitor HEX: 10000 + , &lt;new&gt;
        ] [ 2drop bad-char , &lt;new&gt; ] if ;

    : utf16le ( state string -- state string )
        [ [ swap (utf16le) ] each ] { } make ;


<a target="_blank" rel="noopener noreferrer" href="https://re.factorcode.org/2023/05/unicode.html">https://re.factorcode.org/2023/05/unicode.html</a>
jlf: very basic, but may be useful to write little tests


<a target="_blank" rel="noopener noreferrer" href="https://re.factorcode.org/2023/05/case-conversion.html">https://re.factorcode.org/2023/05/case-conversion.html</a>
snake_case
camelCase
kebab-case
PascalCase
Ada_Case
Train-Case
COBOL-CASE
MACRO_CASE
UPPER CASE
lower case
Title Case
Sentence case
dot.case


<hr><h2 id="Fortran_lang">Fortran lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://fortran-lang.discourse.group/t/using-unicode-characters-in-fortran/2764">https://fortran-lang.discourse.group/t/using-unicode-characters-in-fortran/2764</a>
jlf: hum... it's a blind support of UTF-8, as we do with current Rexx.
There is no support of Unicode.
In the unicode_len.f90 example:
    chars = 'Fortran is üí™, üòé, üî•!'
    if (len(chars) /= 28) error stop
28 is the lentgh in bytes...
In the unicode_index.f90 example:
    chars = 'üìê: 4.0¬∑tan‚Åª¬π(1.0) = œÄ'
    i = index(chars, 'n')
    if (i /= 14) error stop
    i = index(chars, '¬π')
    if (i /= 18) error stop
14 and 18 are byte positions...


<hr><h2 id="GO_lang">GO lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://go.dev/">https://go.dev/</a>


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/ref/spec#Conversions_to_and_from_a_string_type">https://go.dev/ref/spec#Conversions_to_and_from_a_string_type</a>
jlf: worth reading, they cover all the possible conversions between bytes, rune
and string.


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/play/">https://go.dev/play/</a>
The Go Playground


<a target="_blank" rel="noopener noreferrer" href="https://github.com/traefik/yaegi">https://github.com/traefik/yaegi</a>
Another Elegant Go Interpreter
---
rlwrap yaegi


<a target="_blank" rel="noopener noreferrer" href="https://yourbasic.org/golang/">https://yourbasic.org/golang/</a>
Tutorial, a selection related to strings
---
    []byte("No√´l")                                              // [78 111 195 171 108]

    // 1. Using the string() constructor
    string([]byte{78, 111, 195, 171, 108})                      // No√´l

    // 2. Go provides a package called bytes with a function called NewBuffer(), which
    //    creates a new Buffer and then uses the String() method to get the string output.
    bytes.NewBuffer([]byte{78, 111, 195, 171, 108}).String()    // No√´l

    // 3. Using fmt.Sprintf() function
    fmt.Sprintf("%s", []byte{78, 111, 195, 171, 108})           // No√´l

    // String building
    fmt.Sprintf("Size: %d MB.", 85)                             // Size: 85 MB.

    // High-performance string concatenation
    var b strings.Builder
    b.Grow(32) // preallocate memory when the maximum size of the string is known
    for i, p := range []int{2, 3, 5, 7, 11, 13} {
        fmt.Fprintf(&amp;b, "%d:%d, ", i+1, p)
    }
    s := b.String()   // no copying
    s = s[:b.Len()-2] // no copying (removes trailing ", ")
    fmt.Println(s)    // 1:2, 2:3, 3:5, 4:7, 5:11, 6:13

    // Convert string to runes
    // For an invalid UTF-8 sequence, the rune value will be 0xFFFD for each invalid byte.
    []rune("No√´l")        // [78 111 235 108]

    // Convert runes to string
    // When you convert a slice of runes to a string, you get a new string that
    // is the concatenation of the runes converted to UTF-8 encoded strings.
    // Values outside the range of valid Unicode code points are converted to
    // \uFFFD, the Unicode replacement character ÔøΩ.
    string([]rune{'\u004E', '\u006F', '\u00EB', '\u006C'})  // No√´l

    // String iteration by runes
    //    the range loop iterates over Unicode code points.
    //    The index is the first byte of a UTF-8-encoded code point;
    //    the second value, of type rune, is the value of the code point.
    //    For an invalid UTF-8 sequence, the second value will be 0xFFFD,
    //    and the iteration will advance a single byte.
    for i, ch := range "Êó•Êú¨Ë™û" {
        fmt.Printf("%#U starts at byte position %d\n", ch, i)
    }
    // Output:
    U+004E 'N' starts at byte position 0
    U+006F 'o' starts at byte position 1
    U+00EB '√´' starts at byte position 2
    U+006C 'l' starts at byte position 4

    // String iteration by bytes
    const s = "No√´l"
    for i := 0; i &lt; len(s); i++ {
        fmt.Printf("%x ", s[i])
    }
    // Output: 4e 6f c3 ab 6c


<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/strings">https://pkg.go.dev/strings</a>
Package strings implements simple functions to manipulate UTF-8 encoded strings.
jlf: BIFs


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/blog/slices">https://go.dev/blog/slices</a>
Arrays, slices (and strings): The mechanics of 'append'
Rob Pike
26 September 2013
---
jlf: prerequisite to understand how strings are managed
Next blog also helps (no relation with Unicode, but...)
<a target="_blank" rel="noopener noreferrer" href="https://teivah.medium.com/slice-length-vs-capacity-in-go-af71a754b7d8">https://teivah.medium.com/slice-length-vs-capacity-in-go-af71a754b7d8</a>


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/blog/strings">https://go.dev/blog/strings</a>
Strings, bytes, runes and characters in Go
Rob Pike
23 October 2013
---
In Go, a string is in effect a read-only slice of bytes.
A string holds arbitrary bytes. It is not required to hold Unicode text, UTF-8 text,
or any other predefined format.
    const sample = "\xbd\xb2\x3d\xbc\x20\xe2\x8c\x98"
Indexing a string accesses individual bytes, not characters.
    for i := 0; i &lt; len(sample); i++ {
        fmt.Printf("%x ", sample[i])        # bd b2 3d bc 20 e2 8c 98
    }
A shorter way to generate presentable output for a messy string is to use the %x
(hexadecimal) format verb of fmt.Printf. It just dumps out the sequential bytes
of the string as hexadecimal digits, two per byte.
    fmt.Printf("%x\n", sample)              # bdb23dbc20e28c98
    fmt.Printf("% x\n", sample)             # bd b2 3d bc 20 e2 8c 98
The %q (quoted) verb will escape any non-printable byte sequences in a string so
the output is unambiguous.
    fmt.Printf("%q\n", sample)              # "\xbd\xb2=\xbc ‚åò"
    fmt.Printf("%+q\n", sample)             # "\xbd\xb2=\xbc \u2318"
The Go language defines the word rune as an alias for the type int32, so programs
can be clear when an integer value represents a code point.
A for range loop decodes one UTF-8-encoded rune on each iteration. Each time around
the loop, the index of the loop is the starting position of the current rune, measured
in bytes, and the code point is its value.
    const nihongo = "Êó•Êú¨Ë™û"
    for index, runeValue := range nihongo {
        fmt.Printf("%#U starts at byte position %d\n", runeValue, index)
    }
The output shows how each code point occupies multiple bytes:
    U+65E5 'Êó•' starts at byte position 0
    U+672C 'Êú¨' starts at byte position 3
    U+8A9E 'Ë™û' starts at byte position 6


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/pkg/unicode/utf8/">https://go.dev/pkg/unicode/utf8/</a>
Unicode/utf8 package


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/blog/normalization">https://go.dev/blog/normalization</a>
Text normalization in Go
Marcel van Lohuizen
26 November 2013
---
To write your text as NFC, use the <a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text/unicode/norm">https://pkg.go.dev/golang.org/x/text/unicode/norm</a>
package to wrap your io.Writer of choice:
    wc := norm.NFC.Writer(w)
    defer wc.Close()
    // write as before...
If you have a small string and want to do a quick conversion, you can use this simpler form:
    norm.NFC.Bytes(b)


<a target="_blank" rel="noopener noreferrer" href="https://cs.opensource.google/go/x/text">https://cs.opensource.google/go/x/text</a>
This repository holds supplementary Go libraries for text processing, many involving Unicode.


<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text/collate">https://pkg.go.dev/golang.org/x/text/collate</a>
The collate package, which can sort strings in a language-specific way, works
correctly even with unnormalized strings


<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text/encoding">https://pkg.go.dev/golang.org/x/text/encoding</a>
Package encoding defines an interface for character encodings, such as Shift JIS
and Windows 1252, that can convert to and from UTF-8.
Encoding implementations are provided in other packages, such as
    golang.org/x/text/encoding/charmap
    golang.org/x/text/encoding/japanese.
A Decoder converts bytes to UTF-8. It implements transform.Transformer.
    Transforming source bytes that are not of that encoding will not result in
    an error per se. Each byte that cannot be transcoded will be represented in
    the output by the UTF-8 encoding of '\uFFFD', the replacement rune.
    ---
    jlf: strange... I was expecting a more conservative conversion, since the
    core language supports any bytes in a string.
An Encoder converts bytes from UTF-8. It implements transform.Transformer.
    Each rune that cannot be transcoded will result in an error. In this case,
    the transform will consume all source byte up to, not including the offending
    rune. Transforming source bytes that are not valid UTF-8 will be replaced by
    `\uFFFD`.
    ---
    jlf: the previous description seems contradictory.
    "up to, not including the offending rune"
    "not valid UTF-8 will be replaced by `\uFFFD`"

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/charmap">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/charmap</a>
Package charmap provides simple character encodings such as IBM Code Page 437
and Windows 1252.
    CodePage037 is the IBM Code Page 037 encoding.
    CodePage1047 is the IBM Code Page 1047 encoding.
    CodePage1140 is the IBM Code Page 1140 encoding.
    CodePage437 is the IBM Code Page 437 encoding.
    CodePage850 is the IBM Code Page 850 encoding.
    CodePage852 is the IBM Code Page 852 encoding.
    CodePage855 is the IBM Code Page 855 encoding.
    CodePage858 is the Windows Code Page 858 encoding.
    CodePage860 is the IBM Code Page 860 encoding.
    CodePage862 is the IBM Code Page 862 encoding.
    CodePage863 is the IBM Code Page 863 encoding.
    CodePage865 is the IBM Code Page 865 encoding.
    CodePage866 is the IBM Code Page 866 encoding.
    ISO8859_1 is the ISO 8859-1 encoding.
    ISO8859_10 is the ISO 8859-10 encoding.
    ISO8859_13 is the ISO 8859-13 encoding.
    ISO8859_14 is the ISO 8859-14 encoding.
    ISO8859_15 is the ISO 8859-15 encoding.
    ISO8859_16 is the ISO 8859-16 encoding.
    ISO8859_2 is the ISO 8859-2 encoding.
    ISO8859_3 is the ISO 8859-3 encoding.
    ISO8859_4 is the ISO 8859-4 encoding.
    ISO8859_5 is the ISO 8859-5 encoding.
    ISO8859_6 is the ISO 8859-6 encoding.
    ISO8859_7 is the ISO 8859-7 encoding.
    ISO8859_8 is the ISO 8859-8 encoding.
    ISO8859_9 is the ISO 8859-9 encoding.
    KOI8R is the KOI8-R encoding.
    KOI8U is the KOI8-U encoding.
    Macintosh is the Macintosh encoding.
    MacintoshCyrillic is the Macintosh Cyrillic encoding.
    Windows1250 is the Windows 1250 encoding.
    Windows1251 is the Windows 1251 encoding.
    Windows1252 is the Windows 1252 encoding.
    Windows1253 is the Windows 1253 encoding.
    Windows1254 is the Windows 1254 encoding.
    Windows1255 is the Windows 1255 encoding.
    Windows1256 is the Windows 1256 encoding.
    Windows1257 is the Windows 1257 encoding.
    Windows1258 is the Windows 1258 encoding.
    Windows874 is the Windows 874 encoding.

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/japanese">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/japanese</a>
Package japanese provides Japanese encodings such as EUC-JP and Shift JIS.
    EUCJP is the EUC-JP encoding.
    ISO2022JP is the ISO-2022-JP encoding.
    ShiftJIS is the Shift JIS encoding, also known as Code Page 932 and Windows-31J.

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/korean">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/korean</a>
Package korean provides Korean encodings such as EUC-KR.
    EUCKR is the EUC-KR encoding, also known as Code Page 949.

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/simplifiedchinese">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/simplifiedchinese</a>
Package simplifiedchinese provides Simplified Chinese encodings such as GBK.
    HZGB2312 is the HZ-GB2312 encoding.

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/traditionalchinese">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/traditionalchinese</a>
Package traditionalchinese provides Traditional Chinese encodings such as Big5.
    Big5 is the Big5 encoding, also known as Code Page 950.

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/unicode">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/unicode</a>
Package unicode provides Unicode encodings such as UTF-16.
    UTF8 is the UTF-8 encoding. It neither removes nor adds byte order marks.
    UTF8BOM is an UTF-8 encoding where the decoder strips a leading byte order mark while the encoder adds one.
    UTF16 returns a UTF-16 Encoding for the given default endianness and byte order mark (BOM) policy.
        func UTF16(e Endianness, b BOMPolicy) encoding.Encoding

<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/unicode/utf32">https://pkg.go.dev/golang.org/x/text@v0.10.0/encoding/unicode/utf32</a>
Package utf32 provides the UTF-32 Unicode encoding.
    UTF32 returns a UTF-32 Encoding for the given default endianness and byte order mark (BOM) policy.
        func UTF32(e Endianness, b BOMPolicy) encoding.Encoding


<a target="_blank" rel="noopener noreferrer" href="https://go.dev/blog/matchlang">https://go.dev/blog/matchlang</a>
Language and Locale Matching in Go
The Go package <a target="_blank" rel="noopener noreferrer" href="https://golang.org/x/text/language">https://golang.org/x/text/language</a> implements the BCP 47 standard
for language tags and adds support for deciding which language to use based on
data published in the Unicode Common Locale Data Repository (CLDR).


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2882">https://github.com/unicode-org/icu4x/issues/2882</a>
<a target="_blank" rel="noopener noreferrer" href="https://cs.opensource.google/go/x/text">https://cs.opensource.google/go/x/text</a>
The golang x-text library has re-implemented most of ICU from scratch,
and some of their algorithms and data structures might be interesting
for the icu4x project
(afaik x-text was not just a port of the ICU codebase to another language,
but an actual re-implementation).
You might want to have a look at their code, or talk to @mpvl who wrote most of it.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/golang/go/blob/master/src/cmd/compile/internal/syntax/scanner.go">https://github.com/golang/go/blob/master/src/cmd/compile/internal/syntax/scanner.go</a>
Implementation of Golang‚Äôs lexer
Identifier is made up of letters and digits (where first is always a letter) and
letter is an arbitrary Unicode code point.

    package main
    import "fmt"
    func Èö®Ê©üÂêçÁ®±() {
        fmt.Println("It works!")
    }
    func main() {
        Èö®Ê©üÂêçÁ®±()
        ≈∫d≈∫b≈Ço := 1
        fmt.Println(≈∫d≈∫b≈Ço)
    }


<a target="_blank" rel="noopener noreferrer" href="https://henvic.dev/posts/go-utf8/">https://henvic.dev/posts/go-utf8/</a>
UTF-8 strings with Go: len(s) isn't enough
jlf: in his initial post, the guy was not aware of graphemes
and it's after a feedback on Reddit that he addded stuff about graphemes.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/rivo/uniseg">https://github.com/rivo/uniseg</a>
Unicode Text Segmentation, Word Wrapping, and String Width Calculation in Go


<a target="_blank" rel="noopener noreferrer" href="https://pkg.go.dev/github.com/rivo/uniseg#hdr-Monospace_Width">https://pkg.go.dev/github.com/rivo/uniseg#hdr-Monospace_Width</a>
Monospace Width
Monospace width, as referred to in this package, is the width of a string in a monospace font.
This package differs from wcswidth() in a number of ways, presumably to generate more visually pleasing results.
Note that whether these widths appear correct depends on your application's render engine,
to which extent it conforms to the Unicode Standard, and its choice of font.
---
Rules implemented by uniseg:
    we assume that every code point has a width of 1, with the following exceptions:

    - Code points with grapheme cluster break properties Control, CR, LF, Extend, and ZWJ have a width of 0.
    - U+2E3A, Two-Em Dash, has a width of 3.
    - U+2E3B, Three-Em Dash, has a width of 4.
    - Characters with the East-Asian Width properties "Fullwidth" (F) and "Wide" (W)
      have a width of 2. (Properties "Ambiguous" (A) and "Neutral" (N) both have a width of 1.)
    - Code points with grapheme cluster break property Regional Indicator have a width of 2.
    - Code points with grapheme cluster break property Extended Pictographic have
      a width of 2, unless their Emoji Presentation flag is "No", in which case the width is 1.

    - For Hangul grapheme clusters composed of conjoining Jamo and for Regional Indicators
      (flags), all code points except the first one have a width of 0.
    - For grapheme clusters starting with an Extended Pictographic, any additional
      code point will force a total width of 2, except if the Variation Selector-15
      (U+FE0E) is included, in which case the total width is always 1.
    - Grapheme clusters ending with Variation Selector-16 (U+FE0F) have a width of 2.
---
jlf: mouais, in conclusion there is no guarantee that the result will be good.
---
uniseg.StringWidth("üá©üá™üè≥Ô∏è‚Äçüåà!")       -- uniseg returns 5
utf8proc:
"üá©üá™üè≥Ô∏è‚Äçüåà!"~text~unicodeCharacters~each("charWidth")=     -- [ 1, 1, 1, 0, 0, 2, 1]
"üá©üá™üè≥Ô∏è‚Äçüåà!"~text~unicodeCharacters==
    an Array (shape [7], 7 items)
     1 : ( "üá©"   U+1F1E9 So 1 "REGIONAL INDICATOR SYMBOL LETTER D" )
     2 : ( "üá™"   U+1F1EA So 1 "REGIONAL INDICATOR SYMBOL LETTER E" )
     3 : ( "üè≥"   U+1F3F3 So 1 "WAVING WHITE FLAG" )
     4 : ( "Ô∏è"    U+FE0F Mn 0 "VARIATION SELECTOR-16", "VS16" )
     5 : ( "‚Äç"    U+200D Cf 0 "ZERO WIDTH JOINER", "ZWJ" )
     6 : ( "üåà"  U+1F308 So 2 "RAINBOW" )
     7 : ( "!"   U+0021 Po 1 "EXCLAMATION MARK" )


<hr><h2 id="jRuby_lang">jRuby lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/RubyString.java">https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/RubyString.java</a>
jlf: big file, more than 7000 lines.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/RubyEncoding.java">https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/RubyEncoding.java</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/jruby/jruby/blob/master/lib/ruby/stdlib/unicode_normalize/normalize.rb">https://github.com/jruby/jruby/blob/master/lib/ruby/stdlib/unicode_normalize/normalize.rb</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jruby/jruby/blob/master/spec/ruby/core/string/unicode_normalize_spec.rb">https://github.com/jruby/jruby/blob/master/spec/ruby/core/string/unicode_normalize_spec.rb</a>


<hr><h2 id="Java_lang">Java lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/en/java/javase/">https://docs.oracle.com/en/java/javase/</a>


<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/en/java/javase/20/docs/api/java.base/java/text/BreakIterator.html">https://docs.oracle.com/en/java/javase/20/docs/api/java.base/java/text/BreakIterator.html</a>
java.text.BreakIterator
The default implementation of the character boundary analysis conforms to the
Unicode Consortium's Extended Grapheme Cluster breaks. For more detail, refer to
Grapheme Cluster Boundaries section in the Unicode Standard Annex #29.


<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/en/java/javase/20/intl/internationalization-overview.html">https://docs.oracle.com/en/java/javase/20/intl/internationalization-overview.html</a>
Internationalization Overview


<a target="_blank" rel="noopener noreferrer" href="https://codeahoy.com/2016/05/08/the-char-type-in-java-is-broken/">https://codeahoy.com/2016/05/08/the-char-type-in-java-is-broken/</a>
Java has supported Unicode since its first release and strings are internally
represented using UTF-16 encoding. UTF-16 is a variable length encoding scheme.
For characters that can fit into the 16 bits space, it uses 2 bytes to represent
them. For all other characters, it uses 4 bytes.
For a character that requires more than 16 bits, like these emojis üë¶üë©, the
char methods like someString.charAt(0) or someString.substring(0,1) will break
and give you only half the code point.

<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/javase/tutorial/i18n/text/unicode.html">https://docs.oracle.com/javase/tutorial/i18n/text/unicode.html</a>
    When the specification for the Java language was created, the Unicode standard
    was accepted and the char primitive was defined as a 16-bit data type, with
    characters in the hexadecimal range from 0x0000 to 0xFFFF.

    Because 16-bit encoding supports 216 (65,536) characters, which is insufficient
    to define all characters in use throughout the world, the Unicode standard was
    extended to 0x10FFFF, which supports over one million characters. The definition
    of a character in the Java programming language could not be changed from 16
    bits to 32 bits without causing millions of Java applications to no longer run
    properly. To correct the definition, a scheme was developed to handle characters
    that could not be encoded in 16 bits.

    The characters with values that are outside of the 16-bit range, and within the
    range from 0x10000 to 0x10FFFF, are called supplementary characters and are
    defined as a pair of char values.


<a target="_blank" rel="noopener noreferrer" href="https://openjdk.org/jeps/400">https://openjdk.org/jeps/400</a>
JEP 400: UTF-8 by Default
A quick way to see the default charset of the current JDK is with the following command:
    java -XshowSettings:properties -version 2&gt;&amp;1 | grep file.encoding
As envisaged by the specification of Charset.defaultCharset(), the JDK will allow
the default charset to be configured to something other than UTF-8.
    java -Dfile.encoding=COMPAT
        the default charset will be the charset chosen by the algorithm in JDK 17 and earlier,
        based on the user's operating system, locale, and other factors.
        The value of file.encoding will be set to the name of that charset.
    java -Dfile.encoding=UTF-8
        the default charset will be UTF-8.
        This no-op value is defined in order to preserve the behavior of existing command lines.
    The treatment of values other than "COMPAT" and "UTF-8" are not specified.
    They are not supported, but if such a value worked in JDK 17 then it will likely continue to work in JDK 18.


<a target="_blank" rel="noopener noreferrer" href="https://www.baeldung.com/java-remove-accents-from-text">https://www.baeldung.com/java-remove-accents-from-text</a>
Remove Accents and Diacritics From a String in Java
- We will perform the compatibility decomposition represented as the Java enum NFKD.
  because it decomposes more ligatures than the canonical method (for example, ligature ‚ÄúÔ¨Å‚Äù).
- We will remove all characters matching the Unicode Mark category using the \p{M} regex expression.
Test:
    assertEquals("\\u0066 \\u0069", StringNormalizer.unicodeValueOfNormalizedString("Ô¨Å"));
    assertEquals("\\u0061 \\u0304", StringNormalizer.unicodeValueOfNormalizedString("ƒÅ"));
    assertEquals("\\u0069 \\u0308", StringNormalizer.unicodeValueOfNormalizedString("√Ø"));
    assertEquals("\\u006e \\u0301", StringNormalizer.unicodeValueOfNormalizedString("≈Ñ"));
Compare Strings Including Accents Using Collator.
Java provides four strength values for a Collator:
    PRIMARY: comparison omitting case and accents
    SECONDARY: comparison omitting case but including accents and diacritics
    TERTIARY: comparison including case and accents
    IDENTICAL: all differences are significant


<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/io/DataInput.html#modified-utf-8">https://docs.oracle.com/en/java/javase/16/docs/api/java.base/java/io/DataInput.html#modified-utf-8</a>
Implementations of the DataInput and DataOutput interfaces represent Unicode strings in a format
that is a slight modification of UTF-8.
- Characters in the range '\u0001' to '\u007F' are represented by a single byte.
- The null character '\u0000' and characters in the range '\u0080' to '\u07FF' are represented by a pair of bytes.
- Characters in the range '\u0800' to '\uFFFF' are represented by three bytes.
The differences between this format and the standard UTF-8 format are the following:
- The null byte '\u0000' is encoded in 2-byte format rather than 1-byte,
  so that the encoded strings never have embedded nulls.
- Only the 1-byte, 2-byte, and 3-byte formats are used.
- Supplementary characters are represented in the form of surrogate pairs.


Decomposition of ligature
In Java, you'll need to use the Normalizer class and the NFKC form:
---
String ff ="\uFB00";
String normalized = Normalizer.normalize(ff, Form.NFKC);
System.out.println(ff + " = " + normalized);
---
This will print
Ô¨Ä = ff


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/9699071/what-is-the-javas-internal-represention-for-string-modified-utf-8-utf-16">https://stackoverflow.com/questions/9699071/what-is-the-javas-internal-represention-for-string-modified-utf-8-utf-16</a>
    You can confirm the following by looking at the source code of the relevant version of the java.lang.String class in OpenJDK.
    Prior to Java 9, the standard in-memory representation for a Java String is UTF-16 code-units held in a char[].
        private final char value[];
    With Java 6 update 21 and later, there was a non-standard option (-XX:UseCompressedStrings) to enable compressed strings. This feature was removed in Java 7.
    For Java 9 and later, the implementation of String has been changed to use a compact representation by default.
        private final byte[] value;
        private final byte coder;      // LATIN1 (0) or UTF16 (1)

    <a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/en/java/javase/20/docs/specs/man/java.html#advanced-runtime-options-for-java">https://docs.oracle.com/en/java/javase/20/docs/specs/man/java.html#advanced-runtime-options-for-java</a>
    -XX:-CompactStrings
    Disables the Compact Strings feature.
    By default, this option is enabled.
    When this option is enabled, Java Strings containing only single-byte characters are internally represented
    and stored as single-byte-per-character Strings using ISO-8859-1 / Latin-1 encoding.
    This reduces, by 50%, the amount of space required for Strings containing only single-byte characters.
    For Java Strings containing at least one multibyte character:
    these are represented and stored as 2 bytes per character using UTF-16 encoding.
    Disabling the Compact Strings feature forces the use of UTF-16 encoding as the internal representation for all Java Strings.

    As of 2023, see JEP 254: Compact Strings <a target="_blank" rel="noopener noreferrer" href="https://openjdk.org/jeps/254">https://openjdk.org/jeps/254</a>
    <a target="_blank" rel="noopener noreferrer" href="https://howtodoinjava.com/java9/compact-strings/">https://howtodoinjava.com/java9/compact-strings/</a>


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/44178432/difference-between-compact-strings-and-compressed-strings-in-java-9">https://stackoverflow.com/questions/44178432/difference-between-compact-strings-and-compressed-strings-in-java-9</a>
    In Java 9 on the other hand, compact strings are fully integrated into the JDK source.
    String is always backed by byte[], where characters use one byte if they are Latin-1 and otherwise two.
    Most operations do a check to see which is the case, e.g. charAt:

        public char charAt(int index) {
            if (isLatin1()) {
                return StringLatin1.charAt(value, index);
            } else {
                return StringUTF16.charAt(value, index);
            }
        }

    Compact strings are enabled by default and can be partially disabled - "partially"
    because they are still backed by a byte[] and operations returning chars must still
    put them together from two separate bytes

    public int length() {
        return value.length &gt;&gt; coder();
    }
    If our String is Latin1 only, coder is going to be zero, so length of value (the byte array) is the size of chars.
    For non-Latin1 divide by two.


<a target="_blank" rel="noopener noreferrer" href="https://www.baeldung.com/java-string-encode-utf-8">https://www.baeldung.com/java-string-encode-utf-8</a>
Encoding With Core Java
    // First, we get the String bytes, and then we create a new one using the retrieved bytes and the desired charset:
    String rawString = "Entwickeln Sie mit Vergn√ºgen";
    byte[] bytes = rawString.getBytes(StandardCharsets.UTF_8);
    String utf8EncodedString = new String(bytes, StandardCharsets.UTF_8);
    assertEquals(rawString, utf8EncodedString);
Encoding With Java 7 StandardCharsets
    // First, we'll encode the String into bytes, and second, we'll decode it into a UTF-8 String:
    String rawString = "Entwickeln Sie mit Vergn√ºgen";
    ByteBuffer buffer = StandardCharsets.UTF_8.encode(rawString);
    String utf8EncodedString = StandardCharsets.UTF_8.decode(buffer).toString();
    assertEquals(rawString, utf8EncodedString);


<a target="_blank" rel="noopener noreferrer" href="https://www.baeldung.com/java-string-to-byte-array">https://www.baeldung.com/java-string-to-byte-array</a>
Convert String to Byte Array and Reverse in Java
    Converting a String to Byte Array
        A String is stored as an array of Unicode characters in Java.
        To convert it to a byte array, we translate the sequence of characters into a sequence of bytes.
        For this translation, we use an instance of Charset.
        This class specifies a mapping between a sequence of chars and a sequence of bytes.
        We refer to the above process as encoding.

        Using String.getBytes()
            The String class provides three overloaded getBytes methods to encode a String into a byte array:
                - getBytes() ‚Äì encodes using platform's default charset
                    ---
                    String inputString = "Hello World!";
                    byte[] byteArrray = inputString.getBytes();
                    ---
                    The above method is platform-dependent, as it uses the platform's default charset. We can get this charset by calling Charset.defaultCharset().
                - getBytes (String charsetName) ‚Äì encodes using the named charset
                - getBytes (Charset charset) ‚Äì encodes using the provided charset

        Using Charset.encode()
            The Charset class provides encode(), a convenient method that encodes Unicode characters into bytes.
            This method always replaces invalid input and unmappable-characters using the charset's default replacement byte array.
                ---
                String inputString = "Hello ‡®∏‡©∞‡®∏‡®æ‡®∞!";
                Charset charset = StandardCharsets.US_ASCII;
                byte[] byteArrray = charset.encode(inputString).array();
                ---

        CharsetEncoder
            CharsetEncoder transforms Unicode characters into a sequence of bytes for a given charset.
            Moreover, it provides fine-grained control over the encoding process.
                ---
                String inputString = "Hello ‡®∏‡©∞‡®∏‡®æ‡®∞!";
                CharsetEncoder encoder = StandardCharsets.US_ASCII.newEncoder();
                encoder.onMalformedInput(CodingErrorAction.IGNORE)
                  .onUnmappableCharacter(CodingErrorAction.REPLACE)
                  .replaceWith(new byte[] { 0 });
                byte[] byteArrray = encoder.encode(CharBuffer.wrap(inputString)).array();
                ---

    Converting a Byte Array to String
        We refer to the process of converting a byte array to a String as decoding.
        Similar to encoding, this process requires a Charset.
        However, we can't just use any charset for decoding a byte array.
        In particular, we should use the charset that encoded the String into the byte array.


<a target="_blank" rel="noopener noreferrer" href="https://retrocomputing.stackexchange.com/questions/26535/why-do-java-classfiles-and-jni-use-a-frankensteins-monster-encoding-crossin">https://retrocomputing.stackexchange.com/questions/26535/why-do-java-classfiles-and-jni-use-a-frankensteins-monster-encoding-crossin</a>
Why do Java classfiles (and JNI) use a "Frankenstein's monster" encoding crossing UTF-8 and UTF-16?
jlf: interesting for the history. If I understand correctly, Java uses the CESU-8
encoding to store strings in classfiles and JNI payloads.


<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/CESU-8">https://en.wikipedia.org/wiki/CESU-8</a>
CESU-8 = Compatibility Encoding Scheme for UTF-16: 8-Bit
- A Unicode code point from the Basic Multilingual Plane (BMP), i.e. a code point
  in the range U+0000 to U+FFFF, is encoded in the same way as in UTF-8
- A Unicode supplementary character, i.e. a code point in the range U+10000 to
  U+10FFFF, is first represented as a surrogate pair, like in UTF-16, and then each
  surrogate code point is encoded in UTF-8. Therefore, CESU-8 needs six bytes (3
  bytes per surrogate) for each Unicode supplementary character while UTF-8 needs only four.
CESU-8 is not an official part of the Unicode Standard, because Unicode Technical
Reports are informative documents only. It should be used exclusively for internal
processing and never for external data exchange.
Supporting CESU-8 in HTML documents is prohibited by the W3C and WHATWG HTML
standards, as it would present a cross-site scripting vulnerability.


What is the level of support of surrogates?
    Java.lang.Character.isSurrogatePair()
    Java.lang.Character.toCodePoint(char high, char low) : int
    String.codePointAt()
    Character.codePointAt()


<a target="_blank" rel="noopener noreferrer" href="http://hauchee.blogspot.com/2015/05/surrogate-characters-mechanism.html">http://hauchee.blogspot.com/2015/05/surrogate-characters-mechanism.html</a>
    Neither String or StringBuilder working properly. To avoid the issue above, use
    java.text.BreakIterator to determine the correct position.
    jlf: the code below show how to pass from logical position to real position.
        public static void main(String[] args) {
            String text = "a\uD834\uDD60s\uD834\uDD60\uD834\uDD60¬©‚ÇÇ"; // text: aÌÖ†sÌÖ†ÌÖ†¬©‚ÇÇ
            int startIndex = 2;
            int endIndex = 5;

            BreakIterator charIterator = BreakIterator.getCharacterInstance();
            System.out.println(
                    subString(charIterator, text, startIndex, endIndex)); // output: sÌÖ†ÌÖ†
        }

        private static String subString(BreakIterator charIterator,
                String target, int start, int end) {
            int realStart = 0;
            int realEnd = 0;
            charIterator.setText(target);
            int boundary = charIterator.first();
            int i = 0;
            while (boundary != BreakIterator.DONE) {
                if (i == start) {
                    realStart = boundary;
                }
                if (i == end) {
                    realEnd = boundary;
                    break;
                }
                boundary = charIterator.next();
                i++;
            }
            return target.substring(realStart, realEnd);
        }


<a target="_blank" rel="noopener noreferrer" href="https://github.com/s-u/rJava/issues/51">https://github.com/s-u/rJava/issues/51</a>
    R to Java interface
    Error on UTF-16 surrogate pairs
    Java uses UTF-16 internally and encodes Unicode characters above U+FFFF with
    surrogate pairs. When strings containing such characters are converted to UTF-8
    by rJava they are encoded as a pair of 3 byte sequences rather than as the correct
    4 byte sequence. This is not valid UTF-8 and will result in "invalid multibyte string" errors.
    <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/utf_bom.html#utf8-4">https://www.unicode.org/faq/utf_bom.html#utf8-4</a>


<a target="_blank" rel="noopener noreferrer" href="https://bugs.openjdk.org/browse/JDK-8291660">https://bugs.openjdk.org/browse/JDK-8291660</a>


<a target="_blank" rel="noopener noreferrer" href="https://youtrack.jetbrains.com/issue/IDEA-197555">https://youtrack.jetbrains.com/issue/IDEA-197555</a>
\b{g} not supported in regex
In the docs for java.util.regex.Pattern (<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/javase/10/docs/api/java/util/regex/Pattern.html):">https://docs.oracle.com/javase/10/docs/api/java/util/regex/Pattern.html):</a>
\b{g} is listed under the ‚ÄúBoundary matchers‚Äù section: ‚Äú\b{g} A Unicode extended grapheme cluster boundary‚Äù


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/LanguageTechnology/comments/af0ice/seeking_lightweight_java_graphemetophoneme_g2p/">https://www.reddit.com/r/LanguageTechnology/comments/af0ice/seeking_lightweight_java_graphemetophoneme_g2p/</a>
Seeking lightweight Java grapheme-to-phoneme (G2P) model
Succeeded at getting jg2p working. It's doing pretty well in terms of
pronunciation quality but the model is very large for an Android app and takes
forever to load.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/steveash/jg2p/">https://github.com/steveash/jg2p/</a>
jg2p
Java implementation of a general grapheme to phoneme toolkit using a pipeline of
CRFs, a log-loss re-ranker, and a joint "graphone" language model.


<a target="_blank" rel="noopener noreferrer" href="https://horstmann.com/unblog/2023-10-03/index.html">https://horstmann.com/unblog/2023-10-03/index.html</a>
Stop Using char in Java. And Code Points
jlf: moderately interesting...
jlf: idem for the related HN comments <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=37822967">https://news.ycombinator.com/item?id=37822967</a>
Since Java 20, there is a way of iterating over the grapheme clusters of a string,
using the BreakIterator class from Java 1.1.
    String s = "Ciao üáÆüáπ!";
    BreakIterator iter = BreakIterator.getCharacterInstance();
    iter.setText(s);
    int start = boundary.first();
    int end = boundary.next();
    while (end != BreakIterator.DONE) {
       String gc = s.substring(start, end);
       start = end;
       end = boundary.next();
       process(gc);
    }
Here is a much simpler way, clearly not as efficient. I was stunned to find out
that this worked since Java 9!
    s.split("\\b{g}"); // An array withments "C", "i", "a", "o", " ", "üáÆüáπ", "!"
Or, to get a stream:
    Pattern.compile("\\X").matcher(s).results().map(MatchResult::group)


<hr><h2 id="JavaScript_lang">JavaScript lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://certitude.consulting/blog/en/invisible-backdoor/">https://certitude.consulting/blog/en/invisible-backdoor/</a>
THE INVISIBLE JAVASCRIPT BACKDOOR


<a target="_blank" rel="noopener noreferrer" href="https://www.npmjs.com/package/tty-strings">https://www.npmjs.com/package/tty-strings</a>
A one stop shop for working with text displayed in the terminal.
The goal of this project is to alleviate the headache of working with Javascript's
internal representation of unicode characters, particularly within the context of
displaying text in the terminal for command line applications.
---
jlf tag: character width


<a target="_blank" rel="noopener noreferrer" href="https://github.com/foliojs/linebreak">https://github.com/foliojs/linebreak</a>
A JS implementation of the Unicode Line Breaking Algorithm (UAX #14)
It is used by PDFKit (<a target="_blank" rel="noopener noreferrer" href="https://github.com/foliojs/pdfkit)">https://github.com/foliojs/pdfkit)</a> for line wrapping text
in PDF documents.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/codebox/homoglyph">https://github.com/codebox/homoglyph</a>
A big list of homoglyphs and some code to detect them


<hr><h2 id="Julia_lang">Julia lang</h2><hr>
Remember: search in issues with "utf8proc in:title,body"


<a target="_blank" rel="noopener noreferrer" href="https://bkamins.github.io/julialang/2020/08/13/strings.html">https://bkamins.github.io/julialang/2020/08/13/strings.html</a>
The String, or There and Back Again


<a target="_blank" rel="noopener noreferrer" href="https://docs.julialang.org/en/v1/manual/strings/">https://docs.julialang.org/en/v1/manual/strings/</a>
    You can input any Unicode character in single quotes using \u followed by up to
    four hexadecimal digits or \U followed by up to eight hexadecimal digits
    (the longest valid value only requires six):

    julia&gt; '\u0'
    '\0': ASCII/Unicode U+0000 (category Cc: Other, control)

    julia&gt; '\u78'
    'x': ASCII/Unicode U+0078 (category Ll: Letter, lowercase)

    julia&gt; '\u2200'
    '‚àÄ': Unicode U+2200 (category Sm: Symbol, math)

    julia&gt; '\U10ffff'
    '\U10ffff': Unicode U+10FFFF (category Cn: Other, not assigned)

    julia&gt; s = "\u2200 x \u2203 y"
    "‚àÄ x ‚àÉ y"


<a target="_blank" rel="noopener noreferrer" href="https://docs.julialang.org/en/v1/base/strings/">https://docs.julialang.org/en/v1/base/strings/</a>
jlf: search for "√ü" in this page with Chrome, you will see it matches with "ss"
It doesn't match the Œ≤ here: isless("Œ≤", "Œ±")
"Œ≤"~text~characters=    -- ( "Œ≤"   U+03B2 Ll 1 "GREEK SMALL LETTER BETA" )


<a target="_blank" rel="noopener noreferrer" href="https://juliapackages.com/p/strs">https://juliapackages.com/p/strs</a>
jlf: the string implemention of Scott P Jones
Seems quiet since last year...

    This uses Swift-style \ escape sequences, such as \u{xxxx} for Unicode constants,
    instead of \uXXXX and \UXXXXXXXX, which have the advantage of not having to worry
    about some digit or letter A-F or a-f occurring after the last hex digit of the Unicode constant.

    It also means that $, a very common character for LaTeX strings or output of currencies,
     does not need to be in a string quoted as '$'

    It uses \(expr) for interpolation like Swift, instead of $name or $(expr), which
    also has the advantage of not having to worry about the next character in the
    string someday being allowed in a name.

    It allows for embedding Unicode characters using a variety of easy to remember
    names, instead of hex codes: \:emojiname: \&lt;latexname&gt; \N{unicodename} \&amp;htmlname;
    Examples of this are:
    f"\&lt;dagger&gt; \&amp;yen; \N{ACCOUNT OF} \:snake:", which returns the string: "‚Ä† ¬• ‚ÑÄ üêç "


<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/stupid-question-on-unicode/27674/7">https://discourse.julialang.org/t/stupid-question-on-unicode/27674/7</a>
    Discussion about escape sequence


<a target="_blank" rel="noopener noreferrer" href="https://docs.julialang.org/en/v1/stdlib/Unicode/">https://docs.julialang.org/en/v1/stdlib/Unicode/</a>
Unicode.julia_chartransform(c::Union{Char,Integer})
Unicode.isassigned(c) -&gt; Bool
isequal_normalized(s1::AbstractString, s2::AbstractString; casefold=false, stripmark=false, chartransform=identity)
Unicode.normalize(s::AbstractString; keywords...)
    boolean keywords options (which all default to false except for compose)
        - compose=false: do not perform canonical composition
        - decompose=true: do canonical decomposition instead of canonical composition (compose=true is ignored if present)
        - compat=true: compatibility equivalents are canonicalized
        - casefold=true: perform Unicode case folding, e.g. for case-insensitive string comparison
        - newline2lf=true, newline2ls=true, or newline2ps=true: convert various newline sequences (LF, CRLF, CR, NEL) into a linefeed (LF), line-separation (LS), or paragraph-separation (PS) character, respectively
        - stripmark=true: strip diacritical marks (e.g. accents)
        - stripignore=true: strip Unicode's "default ignorable" characters (e.g. the soft hyphen or the left-to-right marker)
        - stripcc=true: strip control characters; horizontal tabs and form feeds are converted to spaces; newlines are also converted to spaces unless a newline-conversion flag was specified
        - rejectna=true: throw an error if unassigned code points are found
        - stable=true: enforce Unicode versioning stability (never introduce characters missing from earlier Unicode versions)
Unicode.normalize(s::AbstractString, normalform::Symbol)
    normalform can be :NFC, :NFD, :NFKC, or :NFKD.


utf8proc doesn't support language-sensitive case-folding
Julia, which uses utf8proc, has decided to remain locale-independent.
See <a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/issues/7848">https://github.com/JuliaLang/julia/issues/7848</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/pull/42493">https://github.com/JuliaLang/julia/pull/42493</a>
This PR adds a function isequal_normalized to the Unicode stdlib to check whether
two strings are canonically equivalent (optionally casefolding and/or stripping combining marks).


<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/13">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/13</a>
    julia&gt; '\ub5'
    '¬µ': Unicode U+00b5 (category Ll: Letter, lowercase)

    julia&gt; '\uff'
    '√ø': Unicode U+00ff (category Ll: Letter, lowercase)

    julia&gt; Base.Unicode.uppercase("√ø")[1]
    '≈∏': Unicode U+0178 (category Lu: Letter, uppercase)

    julia&gt; Base.Unicode.uppercase("¬µ")[1]
    'Œú': Unicode U+039c (category Lu: Letter, uppercase)


jlf: I find the next thead interesting from a social point of view...
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/40">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/40</a>
Yet another Stefan Karpinski against Scott P Jones...
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/42">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/42</a>
    jlf: helping Scott P Jones
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/46">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/46</a>
    Referencing <a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/pull/25021">https://github.com/JuliaLang/julia/pull/25021</a>
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/72">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/72</a>
    jlf: Stefan Karpinski not happy
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/79">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/79</a>
    jlf: Stefan Karpinski not happy
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/88">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/88</a>
    jlf: Scott P Jones not happy
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/130">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/130</a>
    Using a hybrid encoding like Python 3‚Äôs strings or @ScottPJones‚Äôs UniStr
    means that not only do you need to look at every byte of incoming data, but
    you also have to transcode it in general. This is a total performance
    nightmare for dealing with large text files.
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/133">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/133</a>
    jlf: Interesting points of Stefan Karpinski regarding the validation of strings.
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/138">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/138</a>
    jlf: not sure if Scott P Jones says that graphemes are no needed...
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/144">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/144</a>
    jlf: r√©volte!
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/148">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/148</a>
    jlf: "This is a plea for the thread to stop."
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/154">https://discourse.julialang.org/t/problems-with-deprecations-of-islower-lowercase-isupper-uppercase/7797/154</a>
    jlf: very upset guy


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/pull/25021">https://github.com/JuliaLang/julia/pull/25021</a>
Move Unicode-related functions to new Unicode stdlib package
jlf: nothing interesting in the comments, but this is this PR that Scott P Jones
describes as a bomb.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/pull/19469#issuecomment-264810748">https://github.com/JuliaLang/julia/pull/19469#issuecomment-264810748</a>
AFAICT does the currently implemented lowercase also not follow the spec.
I do not know anything about Turkish but the following behaviour in Greek
    julia&gt; lowercase("OŒîŒ•Œ£Œ£ŒïŒ•Œ£")
    "oŒ¥œÖœÉœÉŒµœÖœÉ" # wrong
    "oŒ¥œÖœÉœÉŒµœÖœÇ" # would be correct
is wrong, i.e. the lowercase sigma at the end is the non-final form œÉ but should be the final form œÇ instead.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/issues/54">https://github.com/JuliaStrings/utf8proc/issues/54</a>
Feature request: Full Case Folding #54
opened in 2015, still opened in 2022
jlf: related to utf8proc
---
<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/issues/54#issuecomment-141545196">https://github.com/JuliaStrings/utf8proc/issues/54#issuecomment-141545196</a>
our case is to make a perfect search in MAPS.ME :)
In general, we need to preprocess a lot of raw strings added by community of OpenStreetMap,
and match these strings effectively on mobile device, for any language and any input.
This includes stripping out all diacritics, full case folding, and even some special
conversions which are not covered in Unicode standard but are important for users trying to find something.
    I've already mentioned √ü=&gt;ss conversion,
    there are also non-standard ≈Å=&gt;L, –π=&gt;–∏,
    famous turkish ƒ∞ and ƒ± conversions,
all very important if you don't have a ≈Å key on your keyboard, for example,
and trying to enter it as L (and find some Polish street for example).
Now we have our own highly-optimized implementation for NFKD and Case Folding.
---
jlf: made a search in <a target="_blank" rel="noopener noreferrer" href="https://github.com/mapsme/omim,">https://github.com/mapsme/omim,</a> but could not find where
they handle ≈Å=&gt;L
Found NormalizeAndSimplifyString, but it doesn't simplify ≈Å=&gt;L.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/pull/102">https://github.com/JuliaStrings/utf8proc/pull/102</a>
Fixes allowing for ‚ÄúFull‚Äù folding and NFKC_CaseFold compliance. #102
---
jlf: this is the creation of the function NFKC_Casefold in utf8proc
---
<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/pull/133">https://github.com/JuliaStrings/utf8proc/pull/133</a>
Case folding fixes #133
Updated version of #102:
    Restores the original behavior of IGNORE so that this PR is non-breaking, adds new STRIPNA flag.
    Renames the new function to utf8proc_NFKC_Casefold instead of utf8proc_NFKC_CF
    Adds a minimal test.
    Updates the utf8proc_data.c file.
jlf: this explains why the the options in utf8proc are like that.
jlf: "NFKC_CF" seems the name to search to get useful infos about utf8proc_NFKC_Casefold.
    <a target="_blank" rel="noopener noreferrer" href="https://unicode-org.github.io/icu/userguide/transforms/normalization/">https://unicode-org.github.io/icu/userguide/transforms/normalization/</a>
        NFKC_Casefold: ICU 4.4 supports the combination of NFKC, case folding
        and removing ignorable characters which was introduced with Unicode 5.2.
    <a target="_blank" rel="noopener noreferrer" href="https://docs.tibco.com/pub/enterprise-runtime-for-R/5.0.0/doc/html/Language_Reference/terrUtils/normalizeUnicode.html">https://docs.tibco.com/pub/enterprise-runtime-for-R/5.0.0/doc/html/Language_Reference/terrUtils/normalizeUnicode.html</a>
        normalizeUnicode(x, form = "NCF")
        form: a character string specifying the type of Unicode normalization to be used.
              Should be one of the strings "NFC", "NFD", "NFKC", "NFKD", "NFKC_CF" or "NFKC_Casefold".
        The forms "NFKC_CF" or "NFKC_Casefold" (which are equivalent) are described in <a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/reports/tr31/.">https://www.unicode.org/reports/tr31/.</a>
    <a target="_blank" rel="noopener noreferrer" href="https://www.lanqiao.cn/library/elasticsearch-definitive-guide-cn/220_Token_normalization/40_Case_folding">https://www.lanqiao.cn/library/elasticsearch-definitive-guide-cn/220_Token_normalization/40_Case_folding</a>
        Case folding is the act of converting words into a (usually lowercase) form that does not necessarily result in the correct spelling, but does allow case-insensitive comparisons.
        jlf: they say "The default normalization form that the icu_normalizer token filter uses is nfkc_cf"


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/issues/52408">https://github.com/JuliaLang/julia/issues/52408</a>
isequal_normalized("◊ë÷º÷∞", Unicode.normalize("◊ë÷º÷∞")) == false
---
jlf: see the comments and new code
---
This strings is really not well supported by bbedit!
"◊ë÷º÷∞"~text~unicodeCharacters==
    an Array (shape [3], 3 items)
     1 : ( "◊ë"   U+05D1 Lo 1 "HEBREW LETTER BET" )
     2 : ( "÷º"    U+05BC Mn 0 "HEBREW POINT DAGESH OR MAPIQ" )
     3 : ( "÷∞"    U+05B0 Mn 0 "HEBREW POINT SHEVA" )
"◊ë÷º÷∞"~text~c2x=     -- D791 D6BC D6B0
"◊ë÷º÷∞"~text~nfc~c2x=     -- D791 D6B0 D6BC


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/issues/257">https://github.com/JuliaStrings/utf8proc/issues/257</a>
normalization does not commute with case-folding?
    julia&gt; using Unicode: normalize
    julia&gt; s = "J\uf72\uec8\u345\u315\u5bf\u5bb\U1d16d\u5b0\u334\u35c"
    "J"
    julia&gt; normalize(s, casefold=true) == normalize(normalize(s), casefold=true)
    false
    julia&gt; normalize(normalize(s, casefold=true)) == normalize(normalize(s), casefold=true)
    false
Not sure if this is a bug or just a weird behavior of Unicode.
---
I get something similar in Python 3:
    &gt;&gt;&gt; import unicodedata
    &gt;&gt;&gt; s = "J\u0f72\u0ec8\u0345\u0315\u05bf\u05bb\U0001d16d\u05b0\u0334\u035c"
    &gt;&gt;&gt; unicodedata.normalize("NFC", s.casefold()) == unicodedata.normalize("NFC", s).casefold()
    False
    &gt;&gt;&gt; unicodedata.normalize("NFC", s.casefold()) == unicodedata.normalize("NFC", unicodedata.normalize("NFC", s).casefold())
    False
So I guess this is a weird quirk of Unicode?
---
Executor idem:
    s = "J\u0f72\u0ec8\u0345\u0315\u05bf\u05bb\U0001d16d\u05b0\u0334\u035c"~text~unescape

    s~nfc(casefold:.true) == s~nfc~nfc(casefold:.true)=         -- 0
    s~nfc(casefold:.true)~c2x=                                  -- 6A E0BB88 E0BDB2 CEB9 CCB4 D6B0 D6BB D6BF F09D85AD CC95 CD9C
    s~nfc~nfc(casefold:.true)~c2x=                              -- 6A CCB4 D6B0 D6BB D6BF E0BB88 E0BDB2 F09D85AD CC95 CD9C CEB9

    s~nfc(casefold:.true)~nfc == s~nfc~nfc(casefold:.true)=     -- 0
    s~nfc(casefold:.true)~nfc~c2x=                              -- 6A E0BB88 E0BDB2 CEB9 CCB4 D6B0 D6BB D6BF F09D85AD CC95 CD9C
    s~nfc~nfc(casefold:.true)~c2x=                              -- 6A CCB4 D6B0 D6BB D6BF E0BB88 E0BDB2 F09D85AD CC95 CD9C CEB9


<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaStrings/utf8proc/issues/101#issuecomment-1876151702">https://github.com/JuliaStrings/utf8proc/issues/101#issuecomment-1876151702</a>
jlf: maybe this example of Julia code could be useful for Executor?
function _isequal_normalized!
&gt; I agree that a fast case-folded/normalized comparison function that requires
&gt; no buffers seems possible to write and could be useful, even for Julia;
Note that such a function was implemented in Julia, and could be ported to C:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/JuliaLang/julia/blob/0f6c72c71bc947282ae18715c09f93a22828aab7/stdlib/Unicode/src/Unicode.jl#L268-L340">https://github.com/JuliaLang/julia/blob/0f6c72c71bc947282ae18715c09f93a22828aab7/stdlib/Unicode/src/Unicode.jl#L268-L340</a>


<hr><h2 id="Kotlin_lang">Kotlin lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/">https://kotlinlang.org/api/latest/jvm/stdlib/kotlin.text/</a>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/JetBrains/kotlin/tree/master/libraries/stdlib/jvm/src/kotlin/text">https://github.com/JetBrains/kotlin/tree/master/libraries/stdlib/jvm/src/kotlin/text</a>


<hr><h2 id="Lisp_lang">Lisp lang</h2><hr>
14/09/2021
<a target="_blank" rel="noopener noreferrer" href="https://www.gnu.org/software/emacs/manual/html_node/elisp/Character-Properties.html">https://www.gnu.org/software/emacs/manual/html_node/elisp/Character-Properties.html</a>
    name
    Corresponds to the Name Unicode property.
    The value is a string consisting of upper-case Latin letters A to Z, digits,
    spaces, and hyphen ‚Äò-‚Äô characters. For unassigned codepoints, the value is nil.

    general-category
    Corresponds to the General_Category Unicode property.
    The value is a symbol whose name is a 2-letter abbreviation of the character‚Äôs
    classification. For unassigned codepoints, the value is Cn.

    canonical-combining-class
    Corresponds to the Canonical_Combining_Class Unicode property.
    The value is an integer. For unassigned codepoints, the value is zero.

    bidi-class
    Corresponds to the Unicode Bidi_Class property.
    The value is a symbol whose name is the Unicode directional type of the
    character. Emacs uses this property when it reorders bidirectional text for
    display (see Bidirectional Display). For unassigned codepoints, the value
    depends on the code blocks to which the codepoint belongs: most unassigned
    codepoints get the value of L (strong L), but some get values of AL (Arabic
    letter) or R (strong R).

    decomposition
    Corresponds to the Unicode properties Decomposition_Type and Decomposition_Value.
    The value is a list, whose first element may be a symbol representing a
    compatibility formatting tag, such as small18; the other elements are
    characters that give the compatibility decomposition sequence of this
    character. For characters that don‚Äôt have decomposition sequences, and for
    unassigned codepoints, the value is a list with a single member, the
    character itself.

    decimal-digit-value
    Corresponds to the Unicode Numeric_Value property for characters whose
    Numeric_Type is ‚ÄòDecimal‚Äô. The value is an integer, or nil if the character
    has no decimal digit value. For unassigned codepoints, the value is nil,
    which means NaN, or ‚Äúnot a number‚Äù.

    digit-value
    Corresponds to the Unicode Numeric_Value property for characters whose
    Numeric_Type is ‚ÄòDigit‚Äô. The value is an integer. Examples of such characters
    include compatibility subscript and superscript digits, for which the value
    is the corresponding number. For characters that don‚Äôt have any numeric value,
    and for unassigned codepoints, the value is nil, which means NaN.

    numeric-value
    Corresponds to the Unicode Numeric_Value property for characters whose
    Numeric_Type is ‚ÄòNumeric‚Äô. The value of this property is a number. Examples
    of characters that have this property include fractions, subscripts,
    superscripts, Roman numerals, currency numerators, and encircled numbers.
    For example, the value of this property for the character U+2155 VULGAR
    FRACTION ONE FIFTH is 0.2. For characters that don‚Äôt have any numeric value,
    and for unassigned codepoints, the value is nil, which means NaN.

    mirrored
    Corresponds to the Unicode Bidi_Mirrored property.
    The value of this property is a symbol, either Y or N. For unassigned
    codepoints, the value is N.

    mirroring
    Corresponds to the Unicode Bidi_Mirroring_Glyph property.
    The value of this property is a character whose glyph represents the mirror
    image of the character‚Äôs glyph, or nil if there‚Äôs no defined mirroring glyph.
    All the characters whose mirrored property is N have nil as their mirroring
    property; however, some characters whose mirrored property is Y also have nil
    for mirroring, because no appropriate characters exist with mirrored glyphs.
    Emacs uses this property to display mirror images of characters when
    appropriate (see Bidirectional Display). For unassigned codepoints, the
    value is nil.

    paired-bracket
    Corresponds to the Unicode Bidi_Paired_Bracket property.
    The value of this property is the codepoint of a character‚Äôs paired bracket,
    or nil if the character is not a bracket character. This establishes a
    mapping between characters that are treated as bracket pairs by the Unicode
    Bidirectional Algorithm; Emacs uses this property when it decides how to
    reorder for display parentheses, braces, and other similar characters (see
    Bidirectional Display).

    bracket-type
    Corresponds to the Unicode Bidi_Paired_Bracket_Type property.
    For characters whose paired-bracket property is non-nil, the value of this
    property is a symbol, either o (for opening bracket characters) or c (for
    closing bracket characters). For characters whose paired-bracket property is
    nil, the value is the symbol n (None). Like paired-bracket, this property is
    used for bidirectional display.

    old-name
    Corresponds to the Unicode Unicode_1_Name property.
    The value is a string. For unassigned codepoints, and characters that have
    no value for this property, the value is nil.

    iso-10646-comment
    Corresponds to the Unicode ISO_Comment property.
    The value is either a string or nil. For unassigned codepoints, the value
    is nil.

    uppercase
    Corresponds to the Unicode Simple_Uppercase_Mapping property.
    The value of this property is a single character. For unassigned codepoints,
    the value is nil, which means the character itself.

    lowercase
    Corresponds to the Unicode Simple_Lowercase_Mapping property.
    The value of this property is a single character. For unassigned codepoints,
    the value is nil, which means the character itself.

    titlecase
    Corresponds to the Unicode Simple_Titlecase_Mapping property.
    Title case is a special form of a character used when the first character of
    a word needs to be capitalized. The value of this property is a single
    character. For unassigned codepoints, the value is nil, which means the
    character itself.

    special-uppercase
    Corresponds to Unicode language- and context-independent special upper-casing
    rules. The value of this property is a string (which may be empty). For
    example mapping for U+00DF LATIN SMALL LETTER SHARP S is "SS". For characters
    with no special mapping, the value is nil which means uppercase property
    needs to be consulted instead.

    special-lowercase
    Corresponds to Unicode language- and context-independent special lower-casing
    rules. The value of this property is a string (which may be empty). For
    example mapping for U+0130 LATIN CAPITAL LETTER I WITH DOT ABOVE the value is
    "i\u0307" (i.e. 2-character string consisting of LATIN SMALL LETTER I followed
    by U+0307 COMBINING DOT ABOVE). For characters with no special mapping, the
    value is nil which means lowercase property needs to be consulted instead.

    special-titlecase
    Corresponds to Unicode unconditional special title-casing rules.
    The value of this property is a string (which may be empty). For example
    mapping for U+FB01 LATIN SMALL LIGATURE FI the value is "Fi". For characters
    with no special mapping, the value is nil which means titlecase property
    needs to be consulted instead.


<hr><h2 id="Mathematica_lang">Mathematica lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=yiwLBvirm7A">https://www.youtube.com/watch?v=yiwLBvirm7A</a>
Live CEOing Ep 426: Language Design in Wolfram Language [Unicode Characters &amp; WFR Suggestions]
At the begining, there are a few minutes about character properties.


<a target="_blank" rel="noopener noreferrer" href="https://writings.stephenwolfram.com/2022/06/launching-version-13-1-of-wolfram-language-mathematica/#emojis-and-more-multilingual-support">https://writings.stephenwolfram.com/2022/06/launching-version-13-1-of-wolfram-language-mathematica/#emojis-and-more-multilingual-support</a>
Launching Version 13.1 of Wolfram Language &amp; Mathematica üôÄü§†ü•≥
Emojis! And More Multilingual Support
Original 16-bit Unicode is ‚Äúplane 0‚Äù. Now there are up to 16 additional planes.
Not quite 32-bit characters, but given the way computers work, the approach now
is to allow characters to be represented by 32-bit objects. It‚Äôs far from trivial
to do that uniformly and efficiently. And for us it‚Äôs been a long process to
upgrade everything in our system‚Äîfrom string manipulation to notebook rendering‚Äî
to handle full 32-bit characters. And that‚Äôs finally been achieved in Version 13.1.
---
You can have wolf and ram variables:
    In:= Expand[(üê∫ + üêè)^8]
    In:= Expand[(\|01f43a + \|01f40f)^8]
               8       7            6    2         5   3         4   4         3   5         2    6           7      8
    Out= üêè  + 8 üêè  üê∫ + 28 üêè  üê∫  + 56 üêè  üê∫  + 70 üêè  üê∫  + 56 üêè  üê∫  + 28 üêè  üê∫  + 8 üêè üê∫  + üê∫

The üêè sorts before the üê∫ because it happens to have a numerically smaller character code:
    In:= ToCharacterCode["üê∫üêè"]
    In:= ToCharacterCode["\|01f43a\|01f40f"]
    Out= {128058, 128015}
---
In:= Grid[Outer[StringJoin[#1, "\:200d", #2] &amp; , {"üë©", "üë®"}, {"üî¨", "üè´", "üéì", "üç≥", "üöÄ", "üîß"}]]
In:= Grid[Outer[StringJoin[#1, "\:200d", #2] &amp; , {"\|01f469", "\|01f468"}, {"\|01f52c", "\|01f3eb", "\|01f393", "\|01f373", "\|01f680", "\|01f527"}]]
Out= üë©‚Äçüî¨   üë©‚Äçüè´   üë©‚Äçüéì   üë©‚Äçüç≥   üë©‚ÄçüöÄ   üë©‚Äçüîß
     üë®‚Äçüî¨   üë®‚Äçüè´   üë®‚Äçüéì   üë®‚Äçüç≥   üë®‚ÄçüöÄ   üë®‚Äçüîß
---
No outer product in Executor, only element-wise operators
("üë©", "üë®")~each{(item || .Unicode["zero width joiner"]~text) || ("üî¨", "üè´", "üéì", "üç≥", "üöÄ", "üîß")}==
    an Array (shape [2], 2 items)
     1 : [T'üë©‚Äçüî¨',T'üë©‚Äçüè´',T'üë©‚Äçüéì',T'üë©‚Äçüç≥',T'üë©‚ÄçüöÄ',T'üë©‚Äçüîß']
     2 : [T'üë®‚Äçüî¨',T'üë®‚Äçüè´',T'üë®‚Äçüéì',T'üë®‚Äçüç≥',T'üë®‚ÄçüöÄ',T'üë®‚Äçüîß']
---
In:= CharacterRange[74000, 74050]
Out= {íÑê, íÑë, íÑí, íÑì, íÑî, íÑï, íÑñ, íÑó, íÑò, íÑô, íÑö, íÑõ, íÑú, íÑù, íÑû, íÑü, íÑ†, íÑ°, íÑ¢, íÑ£,
 &gt;    íÑ§, íÑ•, íÑ¶, íÑß, íÑ®, íÑ©, íÑ™, íÑ´, íÑ¨, íÑ≠, íÑÆ, íÑØ, íÑ∞, íÑ±, íÑ≤, íÑ≥, íÑ¥, íÑµ, íÑ∂, íÑ∑,
 &gt;    íÑ∏, íÑπ, íÑ∫, íÑª, íÑº, íÑΩ, íÑæ, íÑø, íÖÄ, íÖÅ, íÖÇ}
In:= FromCharacterCode[{2361, 2367}]
Out= ‡§π‡§ø
In:= Characters["‡§π‡§ø"]
In:= Characters["\:0939\:093f"]
Out= {‡§π, ‡§ø}
In:= Characters["o\:0308"]
Out= {o, Ãà}
In:= CharacterNormalize["o\:0308", "NFC"]
Out= √∂
In:= ToCharacterCode[%]
Out= {246}


<hr><h2 id="netrexx_lang">netrexx lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://groups.io/g/netrexx/topic/93734685">https://groups.io/g/netrexx/topic/93734685</a>
Unicode Examples


(this not NetRexx, but this answer is useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/63410278/code-point-and-utf-16-code-units-are-the-same-thing">https://stackoverflow.com/questions/63410278/code-point-and-utf-16-code-units-are-the-same-thing</a>
    code point and UTF-16 code units are the same thing?

    No, they are different.
    I know, MDN uses the rarely used "code units" term, which confuses people a lot.

    Code points are the number given to a Unicode element (character).
    This is independent to the encoding, and it can be as high as 0x‚Äã10FFFF.
    UTF-32 code units are equivalent to Unicode code points (if you are using the correct endianess).

    Code units in UTF-16 are units of 16bit data.
    UTF-16 uses 1 or 2 code units to describe a code point, depending on its value.

    Code points below (or equal) to 0xFFFF (the old limit/expectation of Unicode
    that such numbers were enough to encode all characters) use just 1 code unit,
    and its value is the same as the code point.

    Unicode expanded the code point space, so now code points between 0x010000..0x10FFFF require 2 code units
    (and we use "surrogates" to encode such characters), 4 bytes total.

    So, code points are not the same as code units.
    For UTF-16, code units are 16bit long, and code points could be 1 or 2 code units.


(this is JavaScript, but this answer is useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://exploringjs.com/impatient-js/ch_unicode.html#:~:text=Code%20units%20are%20numbers%20that,has%208%2Dbit%20code%20units.">https://exploringjs.com/impatient-js/ch_unicode.html#:~:text=Code%20units%20are%20numbers%20that,has%208%2Dbit%20code%20units.</a>
    each UTF-16 code unit is always either a leading surrogate, a trailing surrogate, or encodes a BMP code point
    BMP = Basic Multilingual Plane (0x0000‚Äì0xFFFF)


(this is JavaScript, but this answer is useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://www.w3schools.com/jsref/jsref_codepointat.asp#:~:text=Difference%20Between%20charCodeAt()%20and%20codePointAt()&text=charCodeAt()%20returns%20a%20number,value%20greather%200xFFFF%20(65535).">https://www.w3schools.com/jsref/jsref_codepointat.asp#:~:text=Difference%20Between%20charCodeAt()%20and%20codePointAt()&text=charCodeAt()%20returns%20a%20number,value%20greather%200xFFFF%20(65535).</a>
Difference Between charCodeAt() and codePointAt()
    charCodeAt() is UTF-16, codePointAt() is Unicode.
    charCodeAt() returns a number between 0 and 65535.
    Both methods return an integer representing the UTF-16 code of a character,
    but only codePointAt() can return the full value of a Unicode value greather 0xFFFF (65535).


(this is Unicode, but this answer is useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://www.unicode.org/faq/utf_bom.html#:~:text=Surrogates%20are%20code%20points%20from,DC0016%20to%20DFFF16.">https://www.unicode.org/faq/utf_bom.html#:~:text=Surrogates%20are%20code%20points%20from,DC0016%20to%20DFFF16.</a>
    What are surrogates?
        Surrogates are code points from two special ranges of Unicode values, reserved
        for use as the leading, and trailing values of paired code units in UTF-16.
        Leading surrogates, also called high surrogates, are encoded from D800 to DBFF,
        and trailing surrogates, or low surrogates, from DC00 to DFFF.
        They are called surrogates, since they do not represent characters directly, but only as a pair.

    What is the difference between UCS-2 and UTF-16?
        UCS-2 is obsolete terminology which refers to a Unicode implementation up to Unicode 1.1,
        before surrogate code points and UTF-16 were added to Version 2.0 of the standard. This term should now be avoided.
        UCS-2 does not describe a data format distinct from UTF-16, because both use exactly the same 16-bit code unit representations.
        However, UCS-2 does not interpret surrogate code points, and thus cannot be used to conformantly represent supplementary characters.
        Sometimes in the past an implementation has been labeled ‚ÄúUCS-2‚Äù to indicate that it does not support supplementary characters
        and doesn‚Äôt interpret pairs of surrogate code points as characters.
        Such an implementation would not handle processing of character properties,
        code point boundaries, collation, etc. for supplementary characters, nor would it
        be able to support most emoji, for example. [AF]


(this is Unicode, but this answer is useful for NetRexx)
Unicode standard
    How the word "surrogate" is used
        surrogate pair
        surrogate code unit
        surrogate code point
        leading surrogate
        trailing surrogate
        high-surrogate code point
        high-surrogate code unit
        low-surrogate code point
        low-surrogate code unit

    Surrogates
        D71
        High-surrogate code point: A Unicode code point in the range U+D800 to U+DBFF.

        D72
        High-surrogate code unit: A 16-bit code unit in the range D800 to DBFF, used in
        UTF-16 as the leading code unit of a surrogate pair.

        D73
        Low-surrogate code point: A Unicode code point in the range U+DC00 to U+DFFF.

        D74
        Low-surrogate code unit: A 16-bit code unit in the range DC00 to DFFF,
        used in UTF-16 as the trailing code unit of a surrogate pair.

    UTF-16
        In the UTF-16 encoding form, non-surrogate code points in the range U+0000..U+FFFF
        are represented as a single 16-bit code unit; code points in the supplementary planes,
        in the range U+10000..U+10FFFF, are represented as pairs of 16-bit code units.
        These pairs of special code units are known as surrogate pairs. The values of the code units
        used for surro- gate pairs are completely disjunct from the code units used for the
        single code unit representations, thus maintaining non-overlap for all code point representations in UTF-16.

    Code Points Unassigned to Abstract Characters
        C1
        A process shall not interpret a high-surrogate code point or a low-surrogate code point as an abstract character.
        ‚Ä¢ The high-surrogate and low-surrogate code points are designated for surrogate code units in the UTF-16 character encoding form.
          They are unassigned to any abstract character.


(this is Java, but this answer is useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/39955169/which-encoding-does-java-uses-utf-8-or-utf-16/39957184#39957184">https://stackoverflow.com/questions/39955169/which-encoding-does-java-uses-utf-8-or-utf-16/39957184#39957184</a>
Which encoding does Java uses UTF-8 or UTF-16?
    Note that
    new String(bytes, StandardCharsets.UTF_16);
    does not "convert it to UTF-16 explicitly".
    This string constructor takes a sequence of bytes, which is supposed to be in
    the encoding that you have given in the second argument, and converts it to the
    UTF-16 representation of whatever characters those bytes represent in that encoding.

    You can't tell Java how to internally store strings.
    It always stores them as UTF-16.
    The constructor String(byte[],Charset) tells Java to create a UTF-16 string from
    an array of bytes that is supposed to be in the given character set.
    The method getBytes(Charset) tells Java to give you a sequence of bytes that
    represent the string in the given encoding (charset).
    And the method getBytes() without an argument does the same - but uses your
    platform's default character set for the conversion.

    Edit: in fact, Java 9 introduced just such a change in internal representation
    of strings, where, by default, strings whose characters all fall in the ISO-8859-1
    range are internally represented in ISO-8859-1, whereas strings with at least one
    character outside that range are internally represented in UTF-16 as before.
    So indeed, getBytes(StandardCharsets.UTF_16) no longer returns the internal representation.


(this thread contains answers useful for NetRexx)
<a target="_blank" rel="noopener noreferrer" href="https://lucumr.pocoo.org/2014/1/9/ucs-vs-utf8/">https://lucumr.pocoo.org/2014/1/9/ucs-vs-utf8/</a>
UCS vs UTF-8 as Internal String Encoding
jlf: Very good introduction 100% applicable to NetRexx

<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9618306">https://news.ycombinator.com/item?id=9618306</a>
jlf: comments about the blog above.
Interesting comments about the need or non-need to have direct access to code
units or "characters" in constant time.
---
Unicode provides 3 classes of grapheme clusters (legacy, extended and tailored)
at least one of which (tailored) is locale-dependent (`ch` is a single tailored
grapheme cluster under the Slovak locale, because it's the ch digraph).
---
A text-editing control is thinking in terms of "grapheme clusters", not in terms
of codepoints.
jlf: not true. BBEdit works at codepoint level.
üë©‚Äçüë®‚Äçüë©‚ÄçüëßüéÖ
2 graphemes, 8 codepoints, 29 bytes
c2x = 'F09F91A9 E2808D F09F91A8 E2808D F09F91A9 E2808D F09F91A7 F09F8E85'
c2u = 'U+1F469 U+200D U+1F468 U+200D U+1F469 U+200D U+1F467 U+1F385'
c2g = 'F09F91A9E2808DF09F91A8E2808DF09F91A9E2808DF09F91A7 F09F8E85'
In BBEdit, I see 8 "characters" and can move the cursor between each "character".
The ZERO WIDTH JOINER codepoints are visible.
In VSCode, I see 2 "characters".
---
What is the practicality of an unbounded number of possible graphemes?
The standard itself doesn't mention any bounds but there is Unicode Standard Annex #15 -
Unicode Normalization Forms which defines the Stream-Safe Text Format.
    UAX15-D3. Stream-Safe Text Format: A Unicode string is said to be in
    Stream-Safe Text Format if it would not contain any sequences of
    non-starters longer than 30 characters in length when normalized
    to NFKD.
---
This sub-part of the thread is exactly what we are discussing for NetRexx
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9620112">https://news.ycombinator.com/item?id=9620112</a>
---
jlf: next description is exactly what I do in the Executor prototype.
What you really want is constant-time dereferencing of designators for semantically
meaningful substrings. But no language AFAIK actually has that. The fundamental
problem is that most languages have painted themselves into a corner by carving
into stone the fact that strings can be dereferenced by integers. Once you've
done that, you're pretty much screwed. It's not that you can't make it work,
it's just that it requires an awful lot of machinery. You basically need to build
an index for every string you construct, and that can get very expensive.
Fixed-width representations are a less-than-perfect-but-still-not-entirely-unreasonable
engineering solution to this problem.


(this is Python, but this link could be useful for NetRexx c2x and x2c)
<a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/struct.html">https://docs.python.org/3/library/struct.html</a>
Interpret bytes as packed binary data


<hr><h2 id="Oracle">Oracle</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/database/121/NLSPG/ch5lingsort.htm#NLSPG288">https://docs.oracle.com/database/121/NLSPG/ch5lingsort.htm#NLSPG288</a>
Database Globalization Support Guide
5 Linguistic Sorting and Matching
Complex! Did not read in details, maybe I should...


<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/database/121/NLSPG/ch6unicode.htm#NLSPG323">https://docs.oracle.com/database/121/NLSPG/ch6unicode.htm#NLSPG323</a>
Database Globalization Support Guide
6 Supporting Multilingual Databases with Unicode


<a target="_blank" rel="noopener noreferrer" href="https://docs.oracle.com/database/121/NLSPG/ch7progrunicode.htm#NLSPG346">https://docs.oracle.com/database/121/NLSPG/ch7progrunicode.htm#NLSPG346</a>
Database Globalization Support Guide
7 Programming with Unicode


<hr><h2 id="Perl_lang__Perl_6_has_been_renamed_to_Raku_">Perl lang (Perl 6 has been renamed to Raku)</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/18/example-post-3/">https://swigunicode.wordpress.com/2021/10/18/example-post-3/</a>
    SWIG and Perl: Unicode C Library
    Part 1. Small Intro to SWIG

    <a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/22/part-2-c-header-file/">https://swigunicode.wordpress.com/2021/10/22/part-2-c-header-file/</a>
    Part 2. C Header File

    <a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/24/part-3-c-source-file/">https://swigunicode.wordpress.com/2021/10/24/part-3-c-source-file/</a>
    Part 3. C Source File

    <a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/25/part-4-perl-source-file/">https://swigunicode.wordpress.com/2021/10/25/part-4-perl-source-file/</a>
    Part 4. Perl Source File

    <a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/26/part-5-build-and-run-scripts/">https://swigunicode.wordpress.com/2021/10/26/part-5-build-and-run-scripts/</a>
    Part 5. Build and Run Scripts

    <a target="_blank" rel="noopener noreferrer" href="https://swigunicode.wordpress.com/2021/10/27/part-6-swig-interface-file/">https://swigunicode.wordpress.com/2021/10/27/part-6-swig-interface-file/</a>
    Part 6. SWIG Interface File


<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/667684/">https://lwn.net/Articles/667684/</a>
An article about NFG.
Unless one specifies otherwise, Perl 6 normalizes a text string to NFC when it's not NFG.


<hr><h2 id="PHP_lang">PHP lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/nicolas-grekas/Patchwork-UTF8">https://github.com/nicolas-grekas/Patchwork-UTF8</a>
Extensive, portable and performant handling of UTF-8 and grapheme clusters for PHP


<a target="_blank" rel="noopener noreferrer" href="https://kunststube.net/encoding/">https://kunststube.net/encoding/</a>
jlf: First, a general introduction to encoding. Then a focus on PHP.


<a target="_blank" rel="noopener noreferrer" href="https://www.php.net/manual/en/function.iconv.php">https://www.php.net/manual/en/function.iconv.php</a>
iconv ‚Äî Convert a string from one character encoding to another
iconv(string $from_encoding, string $to_encoding, string $string): string|false


<a target="_blank" rel="noopener noreferrer" href="https://www.php.net/manual/en/book.mbstring.php">https://www.php.net/manual/en/book.mbstring.php</a>
Multibyte String
replicates all important string functions in a multi-byte aware fashion.
Because the mb_ functions now have to actually think about what they're doing,
they need to know what encoding they're working on. Therefore every mb_ function
accepts an $encoding parameter as well. Alternatively, this can be set globally
for all mb_ functions using mb_internal_encoding.


<a target="_blank" rel="noopener noreferrer" href="https://www.php.net/manual/en/mbstring.overload.php">https://www.php.net/manual/en/mbstring.overload.php</a>
Warning
This feature has been DEPRECATED as of PHP 7.2.0, and REMOVED as of PHP 8.0.0.
Relying on this feature is highly discouraged.
---
mbstring supports a 'function overloading' feature which enables you to add
multibyte awareness to such an application without code modification by
overloading multibyte counterparts on the standard string functions. For
example, mb_substr() is called instead of substr() if function overloading is
enabled. This feature makes it easy to port applications that only support
single-byte encodings to a multibyte environment in many cases.
---
jlf: the few user's comments are all negative.
     hum... this is one of the choices we foresee for Rexx. Bad idea?
Example: "In short, only use mbstring.func_overload if you are 100% certain that
nothing on your site relies on manipulating binary data in PHP."


Search PHP souces: grapheme
<a target="_blank" rel="noopener noreferrer" href="https://heap.space/search?project=PHP-8.2&full=grapheme&defs=&refs=&path=&hist=&type=">https://heap.space/search?project=PHP-8.2&full=grapheme&defs=&refs=&path=&hist=&type=</a>


<a target="_blank" rel="noopener noreferrer" href="https://news-web.php.net/group.php?group=php.i18n">https://news-web.php.net/group.php?group=php.i18n</a>
php.i18n
Most recent: 08 Feb 2018 (?)


<a target="_blank" rel="noopener noreferrer" href="https://news-web.php.net/php.i18n/1439">https://news-web.php.net/php.i18n/1439</a>
Unicode support with UString abstraction layer
21/03/2012 by Umberto Salsi
jlf: no URL


<a target="_blank" rel="noopener noreferrer" href="https://wiki.php.net/rfc/ustring">https://wiki.php.net/rfc/ustring</a>
    UString is much quicker than mbstring thanks to the use of ICU.
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/PHP/comments/2jvvol/rfc_ustring/">https://www.reddit.com/r/PHP/comments/2jvvol/rfc_ustring/</a>
    Low enthusiasm on reddit...
<a target="_blank" rel="noopener noreferrer" href="https://github.com/krakjoe/ustring">https://github.com/krakjoe/ustring</a>
    UnicodeString for PHP7
    dead, last commit on Mar 17, 2016


<a target="_blank" rel="noopener noreferrer" href="https://github.com/nicolas-grekas/Patchwork-UTF8">https://github.com/nicolas-grekas/Patchwork-UTF8</a>
Patchwork-UTF8
Extensive, portable and performant handling of UTF-8 and grapheme clusters for PHP
Dead? last commit was on May 18, 2016


<a target="_blank" rel="noopener noreferrer" href="https://blog.internet-formation.fr/2022/08/nettoyer-et-remplacer-les-homographes-et-homoglyphes-dun-texte-en-php/">https://blog.internet-formation.fr/2022/08/nettoyer-et-remplacer-les-homographes-et-homoglyphes-dun-texte-en-php/</a>
Nettoyer et remplacer les homographes (et homoglyphes) d‚Äôun texte en PHP


<hr><h2 id="Python_lang">Python lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/dabeaz-course/practical-python/blob/master/Notes/01_Introduction/04_Strings.md">https://github.com/dabeaz-course/practical-python/blob/master/Notes/01_Introduction/04_Strings.md</a>
Practical Python Programming. A course by David Beazley
jlf:good introduction to Python strings.


<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Nfqh6lr3frQ">https://www.youtube.com/watch?v=Nfqh6lr3frQ</a>
The Guts of Unicode in Python
Benjamin Peterson
This talk will examine how Python's internal Unicode representation has changed
from its introduction through the latest major changes in Python 3.3.
jlf: not too long (28 min), good overview.


10/08/2021
List of Python PEPS related to string.
<a target="_blank" rel="noopener noreferrer" href="https://www.python.org/dev/peps/">https://www.python.org/dev/peps/</a>
    Other Informational PEPs
        I	257	Docstring Conventions	Goodger, GvR
        I	287	reStructuredText Docstring Format	Goodger

    Accepted PEPs (accepted; may not be implemented yet)
        SA	675	Arbitrary Literal String Type
        SA	686	Make UTF-8 mode default
        SA	701	Syntactic formalization of f-strings

    Open PEPs (under consideration)

    Finished PEPs (done, with a stable interface)
        SF	100	Python Unicode Integration	Lemburg
        SF	260	Simplify xrange()	GvR
        SF	261	Support for "wide" Unicode characters	Prescod
        SF	263	Defining Python Source Code Encodings	Lemburg, von L√∂wis
        SF	277	Unicode file name support for Windows NT	Hodgson
        SF	278	Universal Newline Support	Jansen
        SF	292	Simpler String Substitutions	Warsaw
        SF	331	Locale-Independent Float/String Conversions	Reis
        SF	383	Non-decodable Bytes in System Character Interfaces	von L√∂wis
        SF	393	Flexible String Representation	v. L√∂wis
        SF	414	Explicit Unicode Literal for Python 3.3	Ronacher, Coghlan
        SF	498	Literal String Interpolation	Smith
        SF	515	Underscores in Numeric Literals	Brandl, Storchaka
        SF	528	Change Windows console encoding to UTF-8	Dower
        SF	529	Change Windows filesystem encoding to UTF-8	Dower
        SF	538	Coercing the legacy C locale to a UTF-8 based locale	Coghlan
        SF	540	Add a new UTF-8 Mode	Stinner
        SF	597	Add optional EncodingWarning	Naoki
        SF	616	String methods to remove prefixes and suffixes	Sweeney
        SF	623	Remove wstr from Unicode	Naoki
        SF	624	Remove Py_UNICODE encoder APIs	Naoki
        SF	3101	Advanced String Formatting	Talin
        SF	3112	Bytes literals in Python 3000	Orendorff
        SF	3120	Using UTF-8 as the default source encoding	von L√∂wis
        SF	3127	Integer Literal Support and Syntax	Maupin
        SF	3131	Supporting Non-ASCII Identifiers	von L√∂wis
        SF	3137	Immutable Bytes and Mutable Buffer	GvR
        SF	3138	String representation in Python 3000	Ishimoto

    Deferred PEPs (postponed pending further research or updates)
        SD	501	General purpose string interpolation	Coghlan
        SD	536	Final Grammar for Literal String Interpolation	Angerer
        SD	558	Defined semantics for locals()	Coghlan

    Abandoned, Withdrawn, and Rejected PEPs
        SS	215	String Interpolation	Yee
        IR	216	Docstring Format	Zadka
        SR	224	Attribute Docstrings	Lemburg
        SR	256	Docstring Processing System Framework	Goodger
        SR	295	Interpretation of multiline string constants	Koltsov
        SR	332	Byte vectors and String/Unicode Unification	Montanaro
        SR	349	Allow str() to return unicode strings	Schemenauer
        IR	502	String Interpolation - Extended Discussion	Miller
        SR	3126	Remove Implicit String Concatenation	Jewett, Hettinger


15/07/2021
review
<a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/howto/unicode.html">https://docs.python.org/3/howto/unicode.html</a>

    Escape sequences in string literals
        "\N{GREEK CAPITAL LETTER DELTA}"        # Using the character name  '\u0394'
        "\u0394"                                # Using a 16-bit hex value  '\u0394'
        "\U00000394"                            # Using a 32-bit hex value  '\u0394'

    One can create a string using the decode() method of bytes.
    This method takes an encoding argument, such as UTF-8, and optionally an errors argument.
    The errors argument specifies the response when the input string can‚Äôt be converted
    according to the encoding‚Äôs rules. Legal values for this argument are
        'strict'            (raise a UnicodeDecodeError exception),
        'replace'           (use U+FFFD, REPLACEMENT CHARACTER),
        'ignore'            (just leave the character out of the Unicode result),
        'backslashreplace'  (inserts a \xNN escape sequence).
    Examples:
        b'\x80abc'.decode("utf-8", "strict")                # UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 0
        b'\x80abc'.decode("utf-8", "replace")               # '\ufffdabc'
        b'\x80abc'.decode("utf-8", "backslashreplace")      # '\\x80abc'
        b'\x80abc'.decode("utf-8", "ignore")                # 'abc'

    Encodings are specified as strings containing the encoding‚Äôs name.
    Python comes with roughly 100 different encodings:
        <a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/codecs.html#standard-encodings">https://docs.python.org/3/library/codecs.html#standard-encodings</a>

    One-character Unicode strings can also be created with the chr() built-in function,
    which takes integers and returns a Unicode string of length 1 that contains
    the corresponding code point:
        chr(57344)      # '\ue000'
    The reverse operation is the built-in ord() function that takes a one-character
    Unicode string and returns the code point value:
        ord('\ue000')   # 57344

    The opposite method of bytes.decode() is str.encode(), which returns a bytes
    representation of the Unicode string, encoded in the requested encoding.
    The errors parameter is the same as the parameter of the decode() method
    but supports a few more possible handlers.
        'strict'            (raise a UnicodeDecodeError exception),
        'replace'           inserts a question mark instead of the unencodable character,
        'ignore'            (just leave the character out of the Unicode result),
        'backslashreplace'  (inserts a \uNNNN escape sequence)
        'xmlcharrefreplace' (inserts an XML character reference),
        'namereplace'       (inserts a \N{...} escape sequence).

    Unicode code points can be written using the \u escape sequence, which is
    followed by four hex digits giving the code point. The \U escape sequence
    is similar, but expects eight hex digits, not four
        &gt;&gt;&gt; s = "a\xac\u1234\u20ac\U00008000"
        ... #     ^^^^ two-digit hex escape
        ... #         ^^^^^^ four-digit Unicode escape
        ... #                     ^^^^^^^^^^ eight-digit Unicode escape
        &gt;&gt;&gt; [ord(c) for c in s]
        [97, 172, 4660, 8364, 32768]

    Python supports writing source code in UTF-8 by default, but you can use almost
    any encoding if you declare the encoding being used. This is done by including
    a special comment as either the first or second line of the source file:
        #!/usr/bin/env python
        # -*- coding: latin-1 -*-
        u = 'abcd√©'
    <a target="_blank" rel="noopener noreferrer" href="https://www.python.org/dev/peps/pep-0263/">https://www.python.org/dev/peps/pep-0263/</a>
    PEP 263 -- Defining Python Source Code Encodings

    Comparing Strings
    The casefold() string method converts a string to a case-insensitive
    form following an algorithm described by the Unicode Standard. This
    algorithm has special handling for characters such as the German letter ‚Äò√ü‚Äô
    (code point U+00DF), which becomes the pair of lowercase letters ‚Äòss‚Äô.
        &gt;&gt;&gt; street = 'G√ºrzenichstra√üe'
        &gt;&gt;&gt; street.casefold()
        'g√ºrzenichstrasse'
    The unicodedata module‚Äôs normalize() function converts strings to one of
    several normal forms: ‚ÄòNFC‚Äô, ‚ÄòNFKC‚Äô, ‚ÄòNFD‚Äô, and ‚ÄòNFKD‚Äô.
        def compare_strs(s1, s2):
            def NFD(s):
                return unicodedata.normalize('NFD', s)
            return NFD(s1) == NFD(s2)
    The Unicode Standard also specifies how to do caseless comparisons:
        def compare_caseless(s1, s2):
            def NFD(s):
                return unicodedata.normalize('NFD', s)
            return NFD(NFD(s1).casefold()) == NFD(NFD(s2).casefold())
    Why is NFD() invoked twice? Because there are a few characters that make
    casefold() return a non-normalized string, so the result needs to be
    normalized again. See section 3.13 of the Unicode Standard

    <a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/unicodedata.html">https://docs.python.org/3/library/unicodedata.html</a>
        unicodedata.lookup(name)
            Look up character by name.
            If a character with the given name is found, return the corresponding character.
            If not found, KeyError is raised.
            Changed in version 3.3: Support for name aliases 1 and named sequences 2 has been added.
        unicodedata.name(chr[, default])
            Returns the name assigned to the character chr as a string.
        unicodedata.decimal(chr[, default])
            Returns the decimal value assigned to the character chr as integer.
        unicodedata.digit(chr[, default])
            Returns the digit value assigned to the character chr as integer.
        unicodedata.numeric(chr[, default])
            Returns the numeric value assigned to the character chr as float.
        unicodedata.category(chr)
            Returns the general category assigned to the character chr as string.
        unicodedata.bidirectional(chr)
            Returns the bidirectional class assigned to the character chr as string.
        unicodedata.combining(chr)
            Returns the canonical combining class assigned to the character chr as integer.
            Returns 0 if no combining class is defined.
        unicodedata.east_asian_width(chr)
            Returns the east asian width assigned to the character chr as string.
        unicodedata.mirrored(chr)
            Returns the mirrored property assigned to the character chr as integer.
            Returns 1 if the character has been identified as a ‚Äúmirrored‚Äù character in bidirectional text, 0 otherwise.
        unicodedata.decomposition(chr)
            Returns the character decomposition mapping assigned to the character chr as string.
            An empty string is returned in case no such mapping is defined.
        unicodedata.normalize(form, unistr)
            Return the normal form form for the Unicode string unistr.
            Valid values for form are ‚ÄòNFC‚Äô, ‚ÄòNFKC‚Äô, ‚ÄòNFD‚Äô, and ‚ÄòNFKD‚Äô.
        unicodedata.is_normalized(form, unistr)
            Return whether the Unicode string unistr is in the normal form form.
            Valid values for form are ‚ÄòNFC‚Äô, ‚ÄòNFKC‚Äô, ‚ÄòNFD‚Äô, and ‚ÄòNFKD‚Äô.
        unicodedata.unidata_version
            The version of the Unicode database used in this module.
        unicodedata.ucd_3_2_0
            This is an object that has the same methods as the entire module,
            but uses the Unicode database version 3.2 instead

    <a target="_blank" rel="noopener noreferrer" href="https://www.python.org/dev/peps/pep-0393/">https://www.python.org/dev/peps/pep-0393/</a>
    PEP 393 -- Flexible String Representation
        When creating new strings, it was common in Python to start of with a
        heuristical buffer size, and then grow or shrink if the heuristics failed.
        With this PEP, this is now less practical, as you need not only a heuristics
        for the length of the string, but also for the maximum character.

        In order to avoid heuristics, you need to make two passes over the input:
        once to determine the output length, and the maximum character; then
        allocate the target string with PyUnicode_New and iterate over the input
        a second time to produce the final output. While this may sound expensive,
        it could actually be cheaper than having to copy the result again as in
        the following approach.

        If you take the heuristical route, avoid allocating a string meant to be
        resized, as resizing strings won't work for their canonical representation.
        Instead, allocate a separate buffer to collect the characters, and then
        construct a unicode object from that using PyUnicode_FromKindAndData.
        One option is to use Py_UCS4 as the buffer element, assuming for the worst
        case in character ordinals. This will allow for pointer arithmetics, but
         may require a lot of memory. Alternatively, start with a 1-byte buffer,
         and increase the element size as you encounter larger characters.
         In any case, PyUnicode_FromKindAndData will scan over the buffer to
         verify the maximum character.


15/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/codecs.html">https://docs.python.org/3/library/codecs.html</a>
Codec registry and base classes
Most standard codecs are text encodings, which encode text to bytes, but there
are also codecs provided that encode text to text, and bytes to bytes.
errors string argument:
    strict
    ignore
    replace
    xmlcharrefreplace
    backslashreplace
    namereplace
    surrogateescape
    surrogatepass


15/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://discourse.julialang.org/t/a-python-rant-about-types/43294/22">https://discourse.julialang.org/t/a-python-rant-about-types/43294/22</a>
A Python rant about types
jlf: the main discussion is about invalid string data.
Stefan Karpinski describes the Julia strings:
    1. You can read and write any data, valid or not.
    2. It is interpreted as UTF-8 where possible and as invalid characters otherwise.
    3. You can simply check if strings or chars are valid UTF-8 or not.
    4. You can work with individual characters easily, even invalid ones.
    5. You can losslessly read and write any string data, valid or not, as strings or chars.
    6. You only get an error when you try to ask for the code point of an invalid char.
    Most Julia code that works with strings is automatically robust with respect to
    invalid UTF-8 data. Only code that needs to look at the code points of individual
    characters will fail on invalid data; in order to do that robustly, you simply
    need to check if the character is valid before taking its code point and handle
    that appropriately.
jlf: I think that all the Julia methods working at character level will raise an error,
not just when looking at the code point.
jlf: Stefan Karpinski explains why Python design is problematic.
Python 3 has to be able to represent any input string in terms of code points.
Needing to turn every string into a fixed-width sequence of code points puts them
in a tough position with respect to invalid strings where there is simply no
corresponding sequence of code points.


17/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://groups.google.com/g/python-ideas/c/wStIS1_NVJQ">https://groups.google.com/g/python-ideas/c/wStIS1_NVJQ</a>
Fix default encodings on Windows
jlf: did not read in details, too long, too many feedbacks.
Maybe some comments are interesting, so I save this URL.


<a target="_blank" rel="noopener noreferrer" href="https://djangocas.dev/blog/python-unicode-string-lowercase-casefold-caseless-match/">https://djangocas.dev/blog/python-unicode-string-lowercase-casefold-caseless-match/</a>
Interesting infos about caseless matching


<a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/dpk/8325992">https://gist.github.com/dpk/8325992</a>
PyICU cheat sheet


10/05/2023
<a target="_blank" rel="noopener noreferrer" href="https://github.com/python/cpython/issues/56938">https://github.com/python/cpython/issues/56938</a>
original URL before migration to github:
<a target="_blank" rel="noopener noreferrer" href="https://bugs.python.org/issue12729">https://bugs.python.org/issue12729</a>

    Python lib re cannot handle Unicode properly due to narrow/wide bug
    jlf: TODO not yet read, but seems interesting.
    I found this link thanks to <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=9618306">https://news.ycombinator.com/item?id=9618306</a> (referenced
    in the NetRexx section)


<a target="_blank" rel="noopener noreferrer" href="https://peps.python.org/pep-0414/">https://peps.python.org/pep-0414/</a>
PEP 414 ‚Äì Explicit Unicode Literal for Python 3.3
Specifically, the Python 3 definition for string literal prefixes will be expanded to allow:
    "u" | "U"
in addition to the currently supported:
    "r" | "R"
The following will all denote ordinary Python 3 strings:
    'text'
    "text"
    '''text'''
    """text"""
    u'text'
    u"text"
    u'''text'''
    u"""text"""
    U'text'
    U"text"
    U'''text'''
    U"""text"""


Types of string and their methods:
    string          "H"     "H"[0]  # "H"
    unicode string  u"H"    u"H"[0] # "H"
    byte string     b"H"    b"H"[0] # 72        string of 8-bit bytes
    raw string      r"H"    r"H"[0] # "H"       string literals with an uninterpreted backslash.
    f-string        f"H"    f"H"[0] # "H"       string with formatted expression substitution.

        dir(""), dir(f""), dir(r"")     dir(b"")
        -------------------------------------------------
        __add__                         __add__
                                        __bytes__
        __class__                       __class__
        __contains__                    __contains__
        __delattr__                     __delattr__
        __dir__                         __dir__
        __doc__                         __doc__
        __eq__                          __eq__
        __format__                      __format__
        __ge__                          __ge__
        __getattribute__                __getattribute__
        __getitem__                     __getitem__
        __getnewargs__                  __getnewargs__
        __getstate__                    __getstate__
        __gt__                          __gt__
        __hash__                        __hash__
        __init__                        __init__
        __init_subclass__               __init_subclass__
        __iter__                        __iter__
        __le__                          __le__
        __len__                         __len__
        __lt__                          __lt__
        __mod__                         __mod__
        __mul__                         __mul__
        __ne__                          __ne__
        __new__                         __new__
        __reduce__                      __reduce__
        __reduce_ex__                   __reduce_ex__
        __repr__                        __repr__
        __rmod__                        __rmod__
        __rmul__                        __rmul__
        __setattr__                     __setattr__
        __sizeof__                      __sizeof__
        __str__                         __str__
        __subclasshook__                __subclasshook__
        capitalize                      capitalize
        casefold
        center                          center
        count                           count
                                        decode
        encode
        endswith                        endswith
        expandtabs                      expandtabs
        find                            find
        format
        format_map
                                        fromhex
                                        hex
        index                           index
        isalnum                         isalnum
        isalpha                         isalpha
        isascii                         isascii
        isdecimal
        isdigit                         isdigit
        isidentifier
        islower                         islower
        isnumeric
        isprintable
        isspace                         isspace
        istitle                         istitle
        isupper                         isupper
        join                            join
        ljust                           ljust
        lower                           lower
        lstrip                          lstrip
        maketrans                       maketrans
        partition                       partition
        removeprefix                    removeprefix
        removesuffix                    removesuffix
        replace                         replace
        rfind                           rfind
        rindex                          rindex
        rjust                           rjust
        rpartition                      rpartition
        rsplit                          rsplit
        rstrip                          rstrip
        split                           split
        splitlines                      splitlines
        startswith                      startswith
        strip                           strip
        swapcase                        swapcase
        title                           title
        translate                       translate
        upper                           upper
        zfill                           zfill


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/72371202/unicodedecodeerror-utf-8-codec-cant-decode-byte-0x97-in-position-3118-inval">https://stackoverflow.com/questions/72371202/unicodedecodeerror-utf-8-codec-cant-decode-byte-0x97-in-position-3118-inval</a>
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x97 in position 3118: invalid start byte Simple text file [duplicate]
(jlf: just keeping a note for the example)
It seems like the file is not encoded in utf-8. Could you try open the file using
io.open with latin-1 encoding instead?
<a target="_blank" rel="noopener noreferrer" href="https://docs.python.org/3/library/functions.html#open">https://docs.python.org/3/library/functions.html#open</a>
    --- (example)
    from textblob import TextBlob
    import io

    with io.open("positive.txt", encoding='latin-1') as f:
        for line in f.read().split('\n'):
            # do what you want with line
    ---


<a target="_blank" rel="noopener noreferrer" href="https://github.com/life4/textdistance">https://github.com/life4/textdistance</a>
Compute distance between sequences.
30+ algorithms, pure python implementation, common interface, optional external libs usage.
Reimplemented in Rust by the same author: <a target="_blank" rel="noopener noreferrer" href="https://github.com/life4/textdistance.rs">https://github.com/life4/textdistance.rs</a>


Testing the JMB's example
"œÇ".upper()                                       # 'Œ£'
"œÉ".upper()                                       # 'Œ£'
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lower()                              # '·ΩÄŒ¥œÖœÉœÉŒµœçœÇ'    last Œ£ becomes œÇ
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£A".lower()                             # '·ΩÄŒ¥œÖœÉœÉŒµœçœÉa'   last Œ£ becomes œÉ
# Humm... the concatenation doesn't change œÇ to œÉ
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lower() + "A"                        # '·ΩÄŒ¥œÖœÉœÉŒµœçœÇA'
("·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lower() + "A").upper()              # '·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£A'
("·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lower() + "A").upper().lower()      # '·ΩÄŒ¥œÖœÉœÉŒµœçœÉa'


<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=33984308">https://news.ycombinator.com/item?id=33984308</a>
The History and rationale of the Python 3 Unicode model for the operating system (vstinner.github.io)
jlf: HN comments about this old blog
<a target="_blank" rel="noopener noreferrer" href="https://vstinner.github.io/python30-listdir-undecodable-filenames.html">https://vstinner.github.io/python30-listdir-undecodable-filenames.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/python/cpython/blob/main/Include/cpython/unicodeobject.h">https://github.com/python/cpython/blob/main/Include/cpython/unicodeobject.h</a>
(search "Unicode Type")
CPython source code of Unicode string
This URL comes from
<a target="_blank" rel="noopener noreferrer" href="https://blog.vito.nyc/posts/gil-balm/">https://blog.vito.nyc/posts/gil-balm/</a>
Fast string construction for CPython extensions


<a target="_blank" rel="noopener noreferrer" href="https://python.developpez.com/tutoriels/plonger-au-coeur-de-python/?page=chapitre-4-moins-strings">https://python.developpez.com/tutoriels/plonger-au-coeur-de-python/?page=chapitre-4-moins-strings</a>
jlf: todo read (french)
Translation from english, could not find the original article.


<hr><h2 id="R_lang">R lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://stringi.gagolewski.com/index.html">https://stringi.gagolewski.com/index.html</a>
stringi: Fast and Portable Character String Processing in R
stringi (pronounced ‚Äústringy‚Äù, IPA [strin…°i]) is THE R package for very fast, portable,
correct, consistent, and convenient string/text processing in any locale or character encoding.
Thanks to ICU, stringi fully supports a wide range of Unicode standards.
Paper (PDF): <a target="_blank" rel="noopener noreferrer" href="https://www.jstatsoft.org/index.php/jss/article/view/v103i02/4324">https://www.jstatsoft.org/index.php/jss/article/view/v103i02/4324</a>

<a target="_blank" rel="noopener noreferrer" href="https://github.com/gagolews/stringi">https://github.com/gagolews/stringi</a>
Fast and Portable Character String Processing in R (with the Unicode ICU)


<hr><h2 id="RAKU_lang_Rakudo_lang__Perl6__Perl_6__MOAR_VM_">RAKU lang Rakudo lang (Perl6, Perl 6, MOAR-VM)</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://raku-advent.blog/2022/12/23/sigils-2/">https://raku-advent.blog/2022/12/23/sigils-2/</a>
jlf: not related to unicode, but good for general culture.
A sigil is any non-alphabetic character that‚Äôs used at the front of a word, and
that conveys meta information about the word. For example, hashtags are a sigil:
the ‚Äâ#‚Äâ in ‚Äâ#nofilter‚Äâ is a sigil that communicates that ‚Äúnofilter‚Äù is a tag
(not a regular word of text). The Raku programming language uses sigils to mark
its variables; Raku has four sigils:
@‚Äâ  (normally associated with arrays),
    can only be used for types that implement the ‚ÄâPositional‚Äâ (‚Äúarray-like‚Äù) role
%‚Äâ  (normally associated with hashes),
    can only be used for types that implement the ‚ÄâAssociative‚Äâ (‚Äúhash-like‚Äù) role
&amp;‚Äâ  (normally associated with functions)
    can only be used for types that implement the ‚ÄâCallable‚Äâ (‚Äúfunction-like‚Äù) role
$‚Äâ (for other variables, such as numbers and strings).


<a target="_blank" rel="noopener noreferrer" href="https://dev.to/lizmat/series/24075">https://dev.to/lizmat/series/24075</a>
Migrating Perl to Raku Series' Articles
jlf: not related to unicode, but good for general culture.


<a target="_blank" rel="noopener noreferrer" href="http://docs.p6c.org/routine.html">http://docs.p6c.org/routine.html</a>
Raku Routines
This is a list of all built-in routines that are documented here as part of the Raku language.
jlf: not related to unicode, but good for general culture.


<a target="_blank" rel="noopener noreferrer" href="https://www.learningraku.com/2016/11/26/quick-tip-11-number-strings-and-numberstring-allomorphs/">https://www.learningraku.com/2016/11/26/quick-tip-11-number-strings-and-numberstring-allomorphs/</a>
Quick Tip #11: Number, Strings, and NumberString Allomorphs
jlf: maybe the same as ooRexx string numbers?
<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/type/Stringy">https://docs.raku.org/type/Stringy</a>          String or object that can act as a string   (role)
<a target="_blank" rel="noopener noreferrer" href="https://rakudocs.github.io/type/Allomorph">https://rakudocs.github.io/type/Allomorph</a>   Dual value number and string                (class)
<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/type/IntStr">https://docs.raku.org/type/IntStr</a>           Dual value integer and string               (class)
<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/type/RatStr">https://docs.raku.org/type/RatStr</a>           Dual value rational number and string       (class)


<a target="_blank" rel="noopener noreferrer" href="https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc">https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc</a>
MoarVM string documentation.
jlf: little intro, no detailled API.


<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/type/Str">https://docs.raku.org/type/Str</a>
class Str
Built-in class for strings. Objects of type Str are immutable.


<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/type/Uni">https://docs.raku.org/type/Uni</a>
class Uni
A string of Unicode codepoints
Unlike Str, which is made of Grapheme clusters, Uni is string strictly made of
Unicode codepoints. That is, base characters and combining characters are
separate elements of a Uni instance.
Uni presents itself with a list-like interface of integer Codepoints.
Typical usage of Uni is through one of its subclasses, NFC, NFD, NFKD and NFKC,
which represent strings in one of the Unicode Normalization Forms of the same name.


<a target="_blank" rel="noopener noreferrer" href="https://course.raku.org/essentials/strings/string-concatenation/">https://course.raku.org/essentials/strings/string-concatenation/</a>
String concatenation
jlf: strange... the concatenation is not described in the doc of Str.
In Raku, you concatenate strings using concatenation operator.
This operator is a tilde: ~.
    my $greeting = 'Hello, ';
    my $who = 'World!';
    say $greeting ~ $who;
Concatenation with assignment
    $str = $str ~ $another-str;
    $str ~= $another-str;


<a target="_blank" rel="noopener noreferrer" href="https://www.codesections.com/blog/raku-unicode/">https://www.codesections.com/blog/raku-unicode/</a>
A deep dive into Raku's Unicode support
Grepping for "Unicode Character Database" brings us to unicode_db.c.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/MoarVM/MoarVM/blob/master/src/strings/unicode_db.c">https://github.com/MoarVM/MoarVM/blob/master/src/strings/unicode_db.c</a>


29/05/2021
<a target="_blank" rel="noopener noreferrer" href="http://moarvm.com/releases.html">http://moarvm.com/releases.html</a>
    2017.07
        Greatly reduce the cases when string concatenation needs renormalization
        Use normalize_should_break to decide if concat needs normalization
        Rename should_break to MVM_unicode_normalize_should_break
        Fix memory leak in MVM_nfg_is_concat_stable
        If both last_a and first_b during concat are non-0 CCC, re-NFG
    --&gt; maybe to review : the last sentence seems to be an optimization of concatenation.
    2017.02
        Implement support for synthetic graphemes in MVM_unicode_string_compare
        Implement configurable collation_mode for MVM_unicode_string_compare
    2017.01
        Add a new unicmp_s op, which compares using the Unicode Collation Algorithm
        Add support for Grapheme_Cluster_Break=Prepend from Unicode 9.0
        Add a script to download the latest version of all of the Unicode data
    --&gt; should review this script
    2015.11
        NFG now uses Unicode Grapheme Cluster algorithm; "\r\n" is now one grapheme
    --&gt; ??? [later] ah, I had a bug! Was not analyzing an UTF-8 ASCII string... Now fixed:
        "0A0D"x~text~description= -- UTF-8 ASCII ( 2 graphemes, 2 codepoints, 2 bytes )
        "0D0A"x~text~description= -- UTF-8 ASCII ( 1 grapheme, 2 codepoints, 2 bytes )


29/05/2021
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=26591373">https://news.ycombinator.com/item?id=26591373</a>
String length functions for single emoji characters evaluate to greater than 1
--&gt; to check : MOAR VM really concatenate a 8bit string with a 32bit string using a string concatenation object ?

    You could do it the way Raku does. It's implementation defined. (Rakudo on MoarVM)
    The way MoarVM does it is that it does NFG, which is sort of like NFC except that it stores grapheme clusters as if they were negative codepoints.

    If a string is ASCII it uses an 8bit storage format, otherwise it uses a 32bit one.
    It also creates a tree of immutable string objects.
    If you do a substring operation it creates a substring object that points at an existing string object.
    If you combine two strings it creates a string concatenation object. Which is useful for combining an 8bit string with a 32bit one.
    All of that is completely opaque at the Raku level of course.

        my $str = "\c[FACE PALM, EMOJI MODIFIER FITZPATRICK TYPE-3, ZWJ, MALE SIGN, VARIATION SELECTOR-16]";

        say $str.chars;        # 1
        say $str.codes;        # 5
        say $str.encode('utf16').elems; # 7
        say $str.encode('utf16').bytes; # 14
        say $str.encode.elems; # 17
        say $str.encode.bytes; # 17
        say $str.codes * 4;    # 20
        #(utf32 encode/decode isn't implemented in MoarVM yet)

        say for $str.uninames;
        # FACE PALM
        # EMOJI MODIFIER FITZPATRICK TYPE-3
        # ZERO WIDTH JOINER
        # MALE SIGN
        # VARIATION SELECTOR-16
    The reason we have utf8-c8 encode/decode is because filenames, usernames, and passwords are not actually Unicode.
    (I have 4 files all named r√®sum√® in the same folder on my computer.)
    utf8-c8 uses the same synthetic codepoint system as grapheme clusters.


<a target="_blank" rel="noopener noreferrer" href="https://andrewshitov.com/2018/10/31/unicode-in-perl-6/">https://andrewshitov.com/2018/10/31/unicode-in-perl-6/</a>
Unicode in Raku


<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/language/unicode">https://docs.raku.org/language/unicode</a>
Raku applies normalization by default to all input and output except for file names,
which are read and written as UTF8-C8
UTF-8 Clean-8 is an encoder/decoder that primarily works as the UTF-8 one. However,
upon encountering a byte sequence that will either not decode as valid UTF-8, or
that would not round-trip due to normalization, it will use NFG synthetics to
keep track of the original bytes involved. This means that encoding back to UTF-8 Clean-8
will be able to recreate the bytes as they originally existed.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc">https://github.com/MoarVM/MoarVM/blob/master/docs/strings.asciidoc</a>
Strings in MoarVM
Strands
    Strands are a type of MVMString which instead of being a flat string with contiguous data,
    actually contains references to other strings. Strands are created during concatenation
    or substring operations. When two flat strings are concatenated together, a Strand with
    references to both string a and string b is created. If string a and string b were strands
    themselves, the references of string a and references of string b are copied one after another
    into the Strand.
Synthetic‚Äôs
    Synthetics are graphemes which contain multiple codepoints. In MoarVM these are stored
    and accessed using a trie, while the actual data itself stores the base character seprately
    and then the combiners are stored in an array.
    Currently the maximum number of combiners in a synthetic is 1024.
    MoarVM will throw an exception if you attempt to create a grapheme with more than 1024 codepoints in it.
Normalization
    MoarVM normalizes into NFG form all input text.
NFG
    Normalization Form Grapheme. Similar to NFC except graphemes which contain multiple codepoints
    are stored in Synthetic graphemes.


<a target="_blank" rel="noopener noreferrer" href="https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/">https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/</a>
Types
    Str type: graphemes
        say "‡§®‡§ø".codes;    # returns  2
        say "‡§®‡§ø".chars;    # returns  1
        say "\r\n".chars;    # returns 1
    NFC, NFD, NFKC, NFKD: types (jlf: types? really?)
    Uni: work with codepoints, no normalization (keep text as-is)
    Blob: family of types to work at the binary level
Unicode source code
    say 0 ‚àà ¬´42 -5 1¬ª.map(&amp;log ‚àò &amp;abs);
    say 0.1e0 + 0.2e0 ‚âÖ 0.3e0;
    say ÔΩ¢There is no \escape in here!ÔΩ£
"Texas" source code
    say 0 (elem) &lt;&lt;42 -5 1&gt;&gt;.map(&amp;log o &amp;abs);
    say 0.1e0 + 0.2e0 =~= 0.3e0;
    say Q[[[There is no \escape in here!]]]


<a target="_blank" rel="noopener noreferrer" href="https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/#comment-14302">https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/#comment-14302</a>
jlf: interesting critics about graphemes.
See also the comment after, which provides answers to the critics.


<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/667036/">https://lwn.net/Articles/667036/</a>
Unicode, Perl 6, and You
jlf: interesting opinions.
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Devanagari#Conjunct_consonants">https://en.wikipedia.org/wiki/Devanagari#Conjunct_consonants</a>
jlf: this is executable code (what is this notation &lt; ‡§∑‡§ø &gt; ?)
    &lt; ‡§∑‡§ø &gt; .NFC .say    # NFC:0x&lt;0937 093f&gt;
    &lt; ‡§∑‡§ø &gt; .NFKC .say   # NFD:0x&lt;0937 093f&gt;
    &lt; ‡§∑‡§ø &gt; .NFD .say    # NFKC:0x&lt;0937 093f&gt;
    &lt; ‡§∑‡§ø &gt; .NFKD .say   # NFKD:0x&lt;0937 093f&gt;
Particularly interesting, this subthread:
<a target="_blank" rel="noopener noreferrer" href="https://lwn.net/Articles/667669/">https://lwn.net/Articles/667669/</a>
Is the current Unicode design impractical?


jlf tests
    # Returns a list of Unicode codepoint numbers that describe the codepoints making up the string
    "a√•¬´".ords                                                  # (97 229 171)

    # Returns the codepoint number of the base characters of the first grapheme in the string
    "√•¬´".ord                                                    # 229

    "Bundesstra√üe im Freiland".lc                               # bundesstra√üe im freiland
    "Bundesstra√üe im Freiland".uc                               # BUNDESSTRASSE IM FREILAND
    "Bundesstra√üe im Freiland".fc                               # bundesstrasse im freiland
    "Bundesstra√üe im Freiland".index("Freiland")                # 16 (start at 0) (executor: 17)
    "Bundesstra√üe im Freiland".index("freiland", :ignorecase)   # 16

                                                                                    # Bundesstra√üe sss s√üs ss
                                                                                    # 01234567890123456789012
                                                                                    #      |    |  ||  ||  |
    "Bundesstra√üe sss s√üs ss".indices("ss")                                         # (5 13 21)
    "Bundesstra√üe sss s√üs ss".indices("ss", :overlap)                               # (5 13 14 21)
    "Bundesstra√üe sss s√üs ss".indices("ss", :ignorecase)                            # (5 10 13 18 21)
    "Bundesstra√üe sss s√üs ss".indices("ss", :ignorecase, :overlap)                  # (5 10 13 14 18 21)   not 17?

    "Bundesstra√üe sss s√üs ss".indices("s", :ignorecase, :overlap)                   # (5 6 13 14 15 17 19 21 22)
    "Bundesstra√üe sss s√üs ss".indices("sSs", :ignorecase, :overlap)                 # (13 17 18)
    "Bundesstra√üe sss s√üs ss".indices("sSsS", :ignorecase, :overlap)                # (17)

    "B√ºndesstra√üe sss s√üs ss".fc                                                    # bundesstrasse sss ssss ss
                                                                                    # 0123456789012345678901234
                                                                                    #      |    |   ||  |||  |
    "B√ºnd√´sstr√§√ü√´ sss s√üs ss".fc.indices("ss")                                      # (5 10 14 18 20 23)
    "B√ºnd√´sstr√§√ü√´ sss s√üs ss".fc.indices("ss", :overlap)                            # (5 10 14 15 18 19 20 23)

                                                                                    # stra√üss√ü√üssse
                                                                                    # 0123456789012
                                                                                    #     || ||||
    "stra√üss√ü√üssse".indices("Ss", :ignorecase)                                      # (4 7 9)
    "stra√üss√ü√üssse".indices("Ss", :ignorecase, :overlap)                            # (4 5 7 8 9 10)

    "T√ät\c[TAG SPACE]e".chars                                                       # 4, "t" + "TAG SPACE" is one grapheme
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc                             # T√ätÛ†Ä†e sss ssss ss tÛ†Ä†√™TE
                                                                                    # 012345678901234567890
                                                                                    # ^ ^  ||  |||  |  ^ ^
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".indices("ss")                  # (5 13)
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".indices("ss", :ignorecase)     # (5 10 13)
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc.indices("ss")               # (5 9 11 14)      11? why not 10? because no overlap
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc.indices("ss", :overlap)     # (5 6 9 10 11 14)

    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc.indices("te")               # ()
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc.indices("te", :ignorecase)  # (19)
    "T√ät\c[TAG SPACE]e sss s√üs ss t\c[TAG SPACE]√™TE".fc.indices("te", :ignoremark)  # (0 2 17 19)      so TAG SPACE is ignored when :ignoremark

    # Matching inside a grapheme
    "no√´lüë©‚Äçüë®‚Äçüë©‚ÄçüëßüéÖ".indices("üëßüéÖ")                                         # ()
    "no√´lüë©‚Äçüë®‚Äçüë©‚ÄçüëßüéÖ".indices("üë®‚Äçüë©")                                        # ()

    # Matching a ligature
                                                                                    # b√¢Ô¨Ñ√©
                                                                                    # 012 3
    "b√¢Ô¨Ñ√©".indices("√©")                                                            # (3)
    "b√¢Ô¨Ñ√©".indices("ffl")                                                          # ()
    "b√¢Ô¨Ñ√©".indices("ffl", :ignorecase)                                             # (2)


<a target="_blank" rel="noopener noreferrer" href="https://raku-advent.blog/2022/12/22/day-22-hes-making-a-list-part-1/">https://raku-advent.blog/2022/12/22/day-22-hes-making-a-list-part-1/</a>
Unicode‚Äôs CLDR (Common Linguistic Data Repository)
jlf: to read...


<a target="_blank" rel="noopener noreferrer" href="https://www.nu42.com/2015/12/perl6-newline-translation-broken.html">https://www.nu42.com/2015/12/perl6-newline-translation-broken.html</a>
Newline translation in Perl6 is broken
A. Sinan Unur
December 11, 2015
---
jlf:
Referenced from <a target="_blank" rel="noopener noreferrer" href="https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/#comment-14382">https://perl6advent.wordpress.com/2015/12/07/day-7-unicode-perl-6-and-you/#comment-14382</a>
I reference this URL in case \r\n versus \r is a problem for Rexx Unicodified.
For Unicode, \r\n is one grapheme. Maybe no relation with the failed test cases.
Was fixed like that:
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/Raku/old-issue-tracker/issues/4849#issuecomment-570873506">https://github.com/Raku/old-issue-tracker/issues/4849#issuecomment-570873506</a>
    * We do translation of \r\n graphemes to \n on all input read as text except
      sockets, independent of platform
    * We do translation of all \n graphemes to \r\n on text output to handles
      except sockets, on Windows only
    * \n is now, unless `use newline` is in force, always \x0A
    * We don't do any such translation when using .encode/.decode, and of course
      when reading/writing Bufs to files, providing an escape hatch from translation if needed


<a target="_blank" rel="noopener noreferrer" href="https://6guts.wordpress.com/2015/11/21/what-one-christmas-elf-has-been-up-to/">https://6guts.wordpress.com/2015/11/21/what-one-christmas-elf-has-been-up-to/</a>
jlf: referenced for the section NFG improvements.


<a target="_blank" rel="noopener noreferrer" href="https://6guts.wordpress.com/2015/10/15/last-week-unicode-case-fixes-and-much-more/">https://6guts.wordpress.com/2015/10/15/last-week-unicode-case-fixes-and-much-more/</a>
jlf: referenced for the section A case of Unicode.


Testing the JMB's example
"œÇ".uc                            # Œ£
"œÉ".uc                            # Œ£
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lc                   # ·ΩÄŒ¥œÖœÉœÉŒµœçœÇ     last Œ£ becomes œÇ
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£A".lc                  # ·ΩÄŒ¥œÖœÉœÉŒµœçœÉa    last Œ£ becomes œÉ
# Humm... the concatenation doesn't change œÇ to œÉ
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lc ~ "A"             # ·ΩÄŒ¥œÖœÉœÉŒµœçœÇA
("·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lc ~ "A").uc        # ·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£A
("·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lc ~ "A").uc.lc     # ·ΩÄŒ¥œÖœÉœÉŒµœçœÉa


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/39663846/how-can-i-make-perl-6-be-round-trip-safe-for-unicode-data">https://stackoverflow.com/questions/39663846/how-can-i-make-perl-6-be-round-trip-safe-for-unicode-data</a>
How can I make Perl 6 be round-trip safe for Unicode data?
Answer: UTF8-C8 isn't really a good solution (but is probably the only solution currently).
jlf: asked in 2016-09-23, maybe the situation is better today.


<a target="_blank" rel="noopener noreferrer" href="https://rosettacode.org/wiki/String_comparison#Raku">https://rosettacode.org/wiki/String_comparison#Raku</a>
String comparisons never do case folding because that's a very complicated subject
in the modern world of Unicode. (You can explicitly apply an appropriate case-folding
function to the arguments before doing the comparison, or for "equality" testing you
can do matching with a case-insensitive regex, assuming Unicode's language-neutral
case-folding rules are okay.)
---
Be aware that Raku applies normalization (Unicode NFC form (Normalization Form Canonical))
by default to all input and output except for file names See docs. Raku follows the Unicode spec.
Raku follows all of the Unicode spec, including parts that some people don't like.
There are some graphemes for which the Unicode consortium has specified that the
NFC form is a different (though usually visually identical) grapheme. Referred to
in Unicode standard annex #15 as Canonical Equivalence. Raku adheres to that spec.


<a target="_blank" rel="noopener noreferrer" href="https://docs.raku.org/language/traps#Traps_to_avoid">https://docs.raku.org/language/traps#Traps_to_avoid</a>
Some problems that might arise when dealing with strings


<a target="_blank" rel="noopener noreferrer" href="https://raku.guide/#_unicode">https://raku.guide/#_unicode</a>
Escape characters
    say "\x0061";
    say "\c[LATIN SMALL LETTER A]";
Numbers
    say (Ÿ§,Ÿ•,Ÿ¶,1,2,3).sort; # (1 2 3 4 5 6)
    say 1 + Ÿ©;              # 10
Raku has methods/operators that implement the Unicode Collation Algorithm.
    say 'a' unicmp 'B'; # Less
Raku provides a collate method that implements the Unicode Collation Algorithm.
    say ('a','b','c','D','E','F').sort;    # (D E F a b c)
    say ('a','b','c','D','E','F').collate; # (a b c D E F)


<hr><h2 id="Rexx_lang">Rexx lang</h2><hr>
11/08/2021
<a target="_blank" rel="noopener noreferrer" href="http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEIN">http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEIN</a>
Reads in a numeric value from a binary (ie, non-text) file.
value = VALUEIN(stream, position, length, options)
    Args
        stream is the name of the stream.
        It can include the full path to the stream (ie, any drive and directory names).
        If omitted, the default is to read from STDIN.

        position specifies at what character position (within the stream) to start
        reading from, where 1 means to start reading at the very first character
        in the stream. If omitted, the default is to resume reading at where a
        previous call to CHARIN() or VALUEIN() left off (ie, where you current
        read character position is).

        length is a 1 to read in the next binary byte (ie, 8-bit value), a 2 to
        read in the next binary short (ie, 16-bit value), or a 4 to read in the
        next binary long (ie, 32-bit value). If length is omitted, VALUEIN() defaults to reading a byte.

        options can be any of the following:
            M	The value is stored (in the stream) in Motorola (big endian) byte order,
                rather than Intel (little endian) byte order.
                The effects only long and short values.
            H	Read in the value as hexadecimal (rather than the default of base 10,
                or decimal, which is the base that REXX uses to express numbers).
                The value can later be converted with X2D().
            B	Read in the value as binary (base 2).
            -	The value is signed (as opposed to unsigned).
            V	stream is the actual data string from which to extract a value.
                You can now replace calls to SUBSTR and C2D with a single, faster call to VALUEIN.
            If omitted, options defaults to none of the above.
    Returns
        The value, if successful.
        If an error, an empty string is returned (unless the NOTREADY condition
        is trapped via CALL method. Then, a '0' is returned).

<a target="_blank" rel="noopener noreferrer" href="http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEOUT">http://nokix.sourceforge.net/help/learn_rexx/funcs5.htm#VALUEOUT</a>
Write out numeric values to a binary (ie, non-text) file (ie, in non-text format).
result = VALUEOUT(stream, values, position, size, options)
    Args
        stream is the name of the stream.
        It can include the full path to the stream (ie, any drive and directory names).
        If omitted, the default is to write to STDOUT (typically, display the data in the console window).

        position specifies at what character position (within the stream) to start writing the data,
        where 1 means to start writing at the very first character in the stream.
        If omitted, the default is to resume writing at where a previous call to
        CHAROUT() or VALUEOUT() left off (or where the "write character pointer" was set via STREAM's SEEK).

        values are the numeric values (ie, data) to write out.
        Each value is separated by one space.

        size is a 1 if each value is to be written as a byte (ie, 8-bit value),
        2 if each value is to be written as a short (16-bit value),
        or 4 if each value is to be written as a long (32-bit value). If omitted, size defaults to 1.

        options can be any of the following:
            M	Write out the values in Motorola (big endian) byte order,
                rather than Intel (little endian) byte order. The effects only long and short values.
            H	The values you supplied are specified in hexadecimal.
            B	The values you supplied are specified in binary (base 2).
            V	stream is the name of a variable, and the data will be overlaid
                onto that variable's value. You can now replace calls to D2C and
                OVERLAY with a single, faster call to VALUEOUT, especially when
                a variable has a large amount of non-text data.
            If omitted, options defaults to none of the above.
    Returns
        0 if the string was written out successfully.
        If an error, VALUEOUT() returns non-zero.


<a target="_blank" rel="noopener noreferrer" href="http://www.dg77.net/tekno/manuel/rexxendian.htm">http://www.dg77.net/tekno/manuel/rexxendian.htm</a>
Test de l‚Äôendianit√©
    /* Verifie l'endianit√© / check endiannity          */
    /* Pour traitement d'information encodees en UTF-8 */
    /* Adapter si on utilise un autre encodage         */
    CALL CONV8_16 ' '
    IF c2x(sortie) = '2000' THEN DO
        endian = 'LE' /* little endian  */
        blanx = '2000'
        END
    ELSE DO
        endian = 'BE' /* big endian  */
        blanx = '0020'
        END
    return endian blanx
    /* ********************************************************************** */
    /*           Conversion UTF-8 -&gt; UNICODE                                  */
    CONV8_16:
    parse arg entree
    sortie = ''
    ZONESORTIE.='NUL'; ZONESORTIE.0=0
    err = systounicode(entree, 'UTF8', , ZONESORTIE.)
    if err == 0 then sortie = ZONESORTIE.!TEXT
      else say 'probleme car., code ' err
    return


<a target="_blank" rel="noopener noreferrer" href="http://www.dg77.net/tekno/xhtml/codage.htm">http://www.dg77.net/tekno/xhtml/codage.htm</a>
Le codage des caract√®res
To read, some infos about the code pages could be useful.


Regina doc
    EXPORT(address, [string], [length] [,pad]) - (AREXX)
        Copies data from the (optional) string into a previously-allocated memory area, which must be
        specified as a 4-byte address. The length parameter specifies the maximum number of characters to
        be copied; the default is the length of the string. If the specified length is longer than the string, the
        remaining area is filled with the pad character or nulls('00'x). The returned value is the number
        of characters copied.
        Caution is advised in using this function. Any area of memory can be overwritten,possibly
        causing a system crash.
        See also STORAGE() and IMPORT().
        Note that the address specified is subject to a machine's endianess.
        EXPORT('0004 0000'x,'The answer') '10'

    IMPORT(address [,length]) - (AREXX)
        Creates a string by copying data from the specified 4-byte address. If the length parameter is not
        supplied,the copy terminates when a null byte is found.
        See also EXPORT()
        Note that the address specified is subject to a machine's endianess.
        IMPORT('0004 0000'x,10) 'The answer' /* maybe */


<hr><h2 id="Ruby_lang">Ruby lang</h2><hr>
jlf note:
still searching articles/blogs comparing the Ruby's approach (multi-encodings)
with languages that force the conversion to Unicode (be it utf-8 or Unicode scalars).


<a target="_blank" rel="noopener noreferrer" href="https://docs.ruby-lang.org/en/3.2/String.html">https://docs.ruby-lang.org/en/3.2/String.html</a>
class String
    ---
    jlf: focus on comparison.
    I did not find the definition of "compatible".

    Methods for Comparing
        ==, ===: Returns true if a given other string has the same content as self.
        eql?: Returns true if the content is the same as the given other string.
        &lt;=&gt;: Returns -1, 0, or 1 as a given other string is smaller than, equal to, or larger than self.
        casecmp: Ignoring case, returns -1, 0, or 1 as a given other string is smaller than, equal to, or larger than self.
        casecmp?: Returns true if the string is equal to a given string after Unicode case folding; false otherwise.

    Returns false if the two strings‚Äô encodings are not compatible:
        "\u{e4 f6 fc}" == ("\u{e4 f6 fc}")                          # =&gt; true
        "\u{e4 f6 fc}".encode("ISO-8859-1") == ("\u{e4 f6 fc}")     # =&gt; false

        "\u{e4 f6 fc}".eql?("\u{e4 f6 fc}")                         # =&gt; true
        "\u{e4 f6 fc}".encode("ISO-8859-1").eql?("\u{e4 f6 fc}")    # =&gt; false

        # "√§√∂√º"   "√Ñ√ñ√ú"
        "\u{e4 f6 fc}".casecmp("\u{c4 d6 dc}")                      # =&gt; 1
        "\u{e4 f6 fc}".encode("ISO-8859-1").casecmp("\u{c4 d6 dc}") # =&gt; nil


<a target="_blank" rel="noopener noreferrer" href="https://yehudakatz.com/2010/05/17/encodings-unabridged/">https://yehudakatz.com/2010/05/17/encodings-unabridged/</a>
Encodings, Unabridged
jlf: this article explains why the Ruby team consider that Unicode is not a
     good solution for CJK.


<a target="_blank" rel="noopener noreferrer" href="https://ruby-doc.org/current/Encoding.html">https://ruby-doc.org/current/Encoding.html</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/ruby/ruby/blob/master/encoding.c">https://github.com/ruby/ruby/blob/master/encoding.c</a>
jlf: search "compat"


<a target="_blank" rel="noopener noreferrer" href="https://docs.ruby-lang.org/en/master/encodings_rdoc.html">https://docs.ruby-lang.org/en/master/encodings_rdoc.html</a>
Encodings
---
jlf: Executor has a similar support of encodings, with less defaults and less
     supported encodings. Otherwise the technical solution is the same: all
     encodings are equal, there is no forced internal encoding, no forced
     conversion.
---
Default encodings:
    - Encoding.default_external:     the default external encoding
    - Encoding.default_internal:     the default internal encoding (may be nil)
    - locale:       the default encoding for a string from the environment
    - filesystem:   the default encoding for a string from the filesystem
String encoding
    A Ruby String object has an encoding that is an instance of class Encoding.
    The encoding may be retrieved by method String#encoding.
        's'.encoding # =&gt; #&lt;Encoding:UTF-8&gt;
    The default encoding for a string literal is the script encoding
The encoding for a string may be changed:
    s = "R\xC3\xA9sum\xC3\xA9"     # =&gt; "R√©sum√©"
    s.encoding                     # =&gt; #&lt;Encoding:UTF-8&gt;
    s.force_encoding('ISO-8859-1') # =&gt; "R\xC3\xA9sum\xC3\xA9"
    s.encoding                     # =&gt; #&lt;Encoding:ISO-8859-1&gt;
Stream Encodings
    Certain stream objects can have two encodings; these objects include instances of:
        IO.
        File.
        ARGF.
        StringIO.
    The two encodings are:
    - An external encoding, which identifies the encoding of the stream.
      The default external encoding is:
          - UTF-8 for a text stream.
          - ASCII-8BIT for a binary stream.
    - An internal encoding, which (if not nil) specifies the encoding to be used
      for the string constructed from the stream.
      The default internal encoding is nil (no conversion).
Script Encoding
    The default script encoding is UTF-8; a Ruby source file may set its script
    encoding with a magic comment on the first line of the file (or second line,
    if there is a shebang on the first).
    The comment must contain the word coding or encoding, followed by a colon,
    space and the Encoding name or alias:
        # encoding: ISO-8859-1
        __ENCODING__ #=&gt; #&lt;Encoding:ISO-8859-1&gt;
This example writes a string to a file, encoding it as ISO-8859-1, then reads
the file into a new string, encoding it as UTF-8:
    s = "R\u00E9sum\u00E9"
    path = 't.tmp'
    ext_enc = 'ISO-8859-1'
    int_enc = 'UTF-8'

    File.write(path, s, external_encoding: ext_enc)
    raw_text = File.binread(path)                                                               # "R\xE9sum\xE9"
    transcoded_text = File.read(path, external_encoding: ext_enc, internal_encoding: int_enc)   # "R√©sum√©"


<a target="_blank" rel="noopener noreferrer" href="https://www.justinweiss.com/articles/3-steps-to-fix-encoding-problems-in-ruby/">https://www.justinweiss.com/articles/3-steps-to-fix-encoding-problems-in-ruby/</a>
3 Steps to Fix Encoding Problems in Ruby
The major difference between encode and force_encoding is that encode might
change bytes, and force_encoding won‚Äôt.
In ASCII-8BIT, every character is represented by a single byte.
That is, str.chars.length == str.bytes.length.


<a target="_blank" rel="noopener noreferrer" href="https://www.cloudbees.com/blog/how-ruby-string-encoding-benefits-developers">https://www.cloudbees.com/blog/how-ruby-string-encoding-benefits-developers</a>
Familiarize Yourself with Ruby String Encoding
written August 14, 2018
Ruby encoding methods
- String#force_encoding is a way of saying that we know the bits for the characters
  are correct and we simply want to properly define how those bits are to be
  interpreted to characters.
- String#encode will transcode the bits themselves that form the characters from
  whatever the string is currently encoded as to our target encoding.
Example of the byte size being different from the character length:
    "≈Ça≈Ç".size
    # =&gt; 3
    "≈Ça≈Ç".bytesize
    # =&gt; 5
Different operating systems have different default character encodings so
programming languages need to support these.
    Encoding.default_external
    # =&gt; #&lt;encoding:utf -8&gt;&lt;/encoding:utf&gt;
Ruby defaults to UTF-8 as its encoding so if it is opening up files from the
operating system and the default is different from UTF-8, it will transcode the
input from that encoding to UTF-8. If this isn't desirable, you may change the
default internal encoding in Ruby with Encoding.default_internal. Otherwise you
can use specific IO encodings in your Ruby code.
    File.open(filename, 'r:UTF-8', &amp;amp;:read)
    # or
    File.open(filename, external_encoding: "ASCII-8BIT", internal_encoding: "ASCII-8BIT") do |f| f.read end
Lately, I've been integrating Ruby's encoding support to Rust with the library Rutie.
Rutie allows you to write Rust that works in Ruby and Ruby that works in Rust.
jlf: see Rutie in Rust lang.


<a target="_blank" rel="noopener noreferrer" href="https://ruby-core.ruby-lang.narkive.com/RDKAvdS7/20483-encoding-of-symbols#post2">https://ruby-core.ruby-lang.narkive.com/RDKAvdS7/20483-encoding-of-symbols#post2</a>
[ruby-core:20483] encoding of symbols
---
jlf: AT LAST! I found a discussion about the comparison of strings.
LONG thread, to carefully read.
---
This message 2008-12-14 is a good summary!
Is it still correct today?
<a target="_blank" rel="noopener noreferrer" href="https://ruby-core.ruby-lang.narkive.com/RDKAvdS7/20483-encoding-of-symbols#post12">https://ruby-core.ruby-lang.narkive.com/RDKAvdS7/20483-encoding-of-symbols#post12</a>
    - String operations are done using the bytes in the strings - they are not
    converted to codepoints internally

    - String equality comparisons seem to be simply done on a byte-by-byte
    basis, without regard to the encoding

    - *However* other operations are not simply byte-by-byte. They are done
    character-by-character, but without converting to codepoints - eg: a 3
    byte character is kept as 3 bytes. For example this means that when
    operating on a variable-length encoding, simple operations like indexing
    can be inefficient, as Ruby may have to scan through the string from the
    start. However Ruby does try to optimize this where possible.

    - There is also a concept of "compatible encodings". Given 2 encodings e1
    &amp; e2, e1 is compatible with e2 if the representation of every character in
    e1 is the same as in e2. This implies that e2 must be a "bigger" encoding
    than e1 - ie: e2 is a superset of e1. Typically we are mainly talking
    about US-ASCII here, which is compatible with most other character sets
    that are either all single-byte (eg: all the ISO-8859 sets) or are
    variable-length multi-byte (eg: UTF-8).

    - When operating on encodings e1 &amp; e2, if e1 is compatible with e2, then
    Ruby treats both strings as being in encoding e2.

    - String#&gt; and String#&lt; are a bit wierd. Normally they are just done on a
    byte-by-byte basis, UNLESS the strings are the same and are incompatible
    encodings, then they always seem to return FALSE. (I have to check this -
    it may be more complicated than this).

    - When operating on incompatible encodings, *normally* non-comparison
    operations (including regexp matches) raise an "Encoding Compatibility
    Error".

    - However there appears to be an exception to this: if operating on 2
    incompatible encodings AND US-ASCII is compatible with both, AND both
    strings are US-ASCII strings, then the operation appears to proceed,
    treating both as US-ASCII. For example "abc" as an ISO-8859-1 and "abc" as
    UTF-8. I guess this is Ruby being "forgiving". (Personally I am not sure
    if this is good or bad). The encoding of the result (for example of a
    string concatenation) seems to be one of the 2 original encodings - I
    haven't figured out the logic to this yet :)
---
jlf: this one seems ugly...
    Actually I just checked this, and this is wrong, sorry. I ended up looking
    at the source code of rb_str_cmp() in string.c, and here is what I think
    it does:

    - it does a byte-by-byte comparison. Assuming the strings are different,
    Ruby returns what you would expect based on this.

    - if the strings are byte for byte identical, but they have incompatible
    encodings and at least one of the strings contains a non-ASCII character,
    then it seems that the result is determined by the ordering of the
    encodings, based on ruby's "encoding index" - an internal ordering of the
    available encodings. Maybe I have got this wrong - it doesn't make a lot
    of sense to me!
---
    I don't mean to shoot you down in flames, but a lot of thought and effort
    has gone into Ruby's encoding support. Ruby could have followed the Python
    route of converting everything to Unicode, but that was rejected for various
    good reasons. Also automatic transcoding to solve issues of incompatible
    encodings was also rejected because it causes a number of problems, in
    particular I believe that transcoding isn't necessarilly accurate, because
    for example there may be multiple or ambiguous representations of the same
    character.
---
    Yukihiro Matsumoto
    UTF-8 + ASCII-8BIT makes ASCII-8BIT. Binary wins.
    jlf: hum... I do the opposite with Executor
    jlf 2023.08.09: I checked today with Ruby 3.2, the result is UTF-8



<a target="_blank" rel="noopener noreferrer" href="http://graysoftinc.com/character-encodings">http://graysoftinc.com/character-encodings</a>
jlf: 12 articles about character encoding in Ruby.
From 2008-10-14 to 2009-06-18
Old, but maybe interesting?
todo: read


<a target="_blank" rel="noopener noreferrer" href="https://docs.ruby-lang.org/en/3.2/case_mapping_rdoc.html">https://docs.ruby-lang.org/en/3.2/case_mapping_rdoc.html</a>
Case Mapping
By default, all of these methods use full Unicode case mapping, which is suitable for most languages.
Non-ASCII case mapping and folding are supported for UTF-8, UTF-16BE/LE, UTF-32BE/LE, and ISO-8859-1~16 Strings/Symbols.
Context-dependent case mapping is currently not supported (Unicode standard: Context Specification for Casing).
In most cases, case conversions of a string have the same number of characters. There are exceptions (see also :fold below):
    s = "\u00DF" # =&gt; "√ü"
    s.upcase     # =&gt; "SS"
    s = "\u0149" # =&gt; "≈â"
    s.upcase     # =&gt; " ºN"
Case mapping may also depend on locale (see also :turkic below)
    s = "\u0049"        # =&gt; "I"
    s.downcase          # =&gt; "i" # Dot above.
    s.downcase(:turkic) # =&gt; "ƒ±" # No dot above.
Case changing methods may not maintain Unicode normalization.
Except for casecmp and casecmp?, each of the case-mapping methods listed above accepts optional arguments, *options.
The arguments may be:
    :ascii only.
    :fold only.
    :turkic or :lithuanian or both.


<a target="_blank" rel="noopener noreferrer" href="https://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/">https://andre.arko.net/2013/12/01/strings-in-ruby-are-utf-8-now/</a>
composition in the form of ligatures isn‚Äôt handled at all
    "baÔ¨Ñe".upcase == "BAFFLE"             # =&gt; false
jlf: Has been fixed in a later version:
    "baÔ¨Ñe".upcase                         # =&gt; "BAFFLE"
BUT
other things are still not good in Ruby 3.2.2 (March 30, 2023):
"noeÃàl".reverse                             # =&gt; "lÃàeon"
"noeÃàl"[0..2]                               # =&gt; "noe"
---
"baÔ¨Ñe"~text~upper=                        -- T'BAÔ¨ÑE'      30/05/2023 Executor not good because utf8proc upper is not good
"baÔ¨Ñe"~text~caselessEquals("baffle")=     -- 1             30/05/2023 Executor is good because utf8proc casefold is good
"noeÃàl"~text~reverse=                       -- T'leÃàon'
"noeÃàl"~text[1,3]=                          -- T'noeÃà'


<a target="_blank" rel="noopener noreferrer" href="https://github.com/jmhodges/rchardet">https://github.com/jmhodges/rchardet</a>
Character encoding auto-detection in Ruby.
jlf: no doc :-(
Returns a confidence rate?
    cd = CharDet.detect(some_data)
    encoding = cd['encoding']
    confidence = cd['confidence'] # 0.0 &lt;= confidence &lt;= 1.0


<a target="_blank" rel="noopener noreferrer" href="https://bugs.ruby-lang.org/issues/18949">https://bugs.ruby-lang.org/issues/18949</a>
Deprecate and remove replicate and dummy encodings
Rejected by Naruse:
    String is a container and an encoding is a label of it. While data whose
    encoding is an encoding categorized in dummy encodings in Ruby, we cannot
    avoid such encodings.
&lt;reopened, lot of discussions&gt;
This is all done now, only <a target="_blank" rel="noopener noreferrer" href="https://github.com/ruby/ruby/pull/7079.">https://github.com/ruby/ruby/pull/7079.</a>
Overall:
    We deprecated and removed Encoding#replicate
    We removed get_actual_encoding()
    We limited to 256 encodings and kept rb_define_dummy_encoding() with that constraint.
    There is a single flat array to lookup encodings, rb_enc_from_index() is fast now.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/ruby/ruby/pull/3803">https://github.com/ruby/ruby/pull/3803</a>
Add string encoding IBM720 alias CP720
The mapping table is generated from the ICU project:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu/blob/master/icu4c/source/data/mappings/ibm-720_P100-1997.ucm">https://github.com/unicode-org/icu/blob/master/icu4c/source/data/mappings/ibm-720_P100-1997.ucm</a>


<a target="_blank" rel="noopener noreferrer" href="https://speakerdeck.com/ima1zumi/dive-into-encoding">https://speakerdeck.com/ima1zumi/dive-into-encoding</a>
slide 23: Code Set Independent (CSI), Treat all encodings fair
slide 24: Each instance of string has encoding information
slide 26: Universal Coded Set (UCS)


<a target="_blank" rel="noopener noreferrer" href="https://shopify.engineering/code-ranges-ruby-strings">https://shopify.engineering/code-ranges-ruby-strings</a>
Code Ranges: A Deeper Look at Ruby Strings
Code ranges are a way for the VM to avoid repeated work and optimize operations
on a per-string basis, guiding away from slow paths when that functionality
isn't needed.
jlf: not sure this article is useful.


<a target="_blank" rel="noopener noreferrer" href="https://idiosyncratic-ruby.com/66-ruby-has-character.html">https://idiosyncratic-ruby.com/66-ruby-has-character.html</a>
Ruby has Character
video: <a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=hlryzsdGtZo">https://www.youtube.com/watch?v=hlryzsdGtZo</a>
(jlf: too small, not very readable, but good for pronuntiation: "Louby")
---
jlf: this page is interesting for the one-liners.
Tools implemented by the author
<a target="_blank" rel="noopener noreferrer" href="https://github.com/janlelis/unibits">https://github.com/janlelis/unibits</a>     Visualize different Unicode encodings in the terminal
<a target="_blank" rel="noopener noreferrer" href="https://github.com/janlelis/uniscribe">https://github.com/janlelis/uniscribe</a>   Know your Unicode ‚úÄ


<a target="_blank" rel="noopener noreferrer" href="https://idiosyncratic-ruby.com/41-proper-unicoding.html">https://idiosyncratic-ruby.com/41-proper-unicoding.html</a>
Proper Unicoding
Ruby's Regexp engine has a powerful feature built in: It can match for Unicode
character properties.


<a target="_blank" rel="noopener noreferrer" href="https://idiosyncratic-ruby.com/26-file-encoding-magic.html">https://idiosyncratic-ruby.com/26-file-encoding-magic.html</a>
default source encoding
    # coding: cp1252
    p "".encoding #=&gt; #&lt;Encoding:Windows-1252&gt;


<a target="_blank" rel="noopener noreferrer" href="https://tomdebruijn.com/posts/rust-string-length-width-calculations/">https://tomdebruijn.com/posts/rust-string-length-width-calculations/</a>
The article is about Rust, but there is an appendix about Ruby.
Seems a good summary, so copy-paste here...
---
    When calling Ruby's String#length, it returns the length of characters like
    Rust's Chars.count. If you want the length in bytes you need to call String#bytesize.
        "abc".length   # =&gt; 3 characters
        "abc".bytesize # =&gt; 3 bytes
        "√©".length     # =&gt; 1 characters
        "√©".bytesize   # =&gt; 2 bytes

    Calling the length on emoji will return the individual characters as the length.
    The üë©‚Äçüî¨ emoji is three characters and eleven bytes in Ruby as well.
        "üë©‚Äçüî¨".length   # =&gt; 3 characters
        "üë©‚Äçüî¨".bytesize # =&gt; 11 bytes

    Do you want grapheme clusters? it's built-in to Ruby with String#grapheme_clusters.
        "üë©‚Äçüî¨".grapheme_clusters.length # =&gt; 1 cluster

    To calculate the display with, we can use the unicode-display_width gem. The same
    multiple counting of emoji in the grapheme cluster still applies here.
        require "unicode/display_width"
        Unicode::DisplayWidth.of("üë©‚Äçüî¨") # =&gt; 4
        Unicode::DisplayWidth.of("‚ù§Ô∏è") # =&gt; 1


<a target="_blank" rel="noopener noreferrer" href="https://ruby-doc.org/3.2.2/File.html">https://ruby-doc.org/3.2.2/File.html</a>
class File
A File object is a representation of a file in the underlying platform.
---
Data mode
To specify whether data is to be treated as text or as binary data, either of
the following may be suffixed to any of the string read/write modes above:
    't': Text data; sets the default external encoding to Encoding::UTF_8;
         on Windows, enables conversion between EOL and CRLF and enables
         interpreting 0x1A as an end-of-file marker.
    'b': Binary data; sets the default external encoding to Encoding::ASCII_8BIT;
         on Windows, suppresses conversion between EOL and CRLF and disables
         interpreting 0x1A as an end-of-file marker.
---
Encodings
Any of the string modes above may specify encodings - either external encoding
only or both external and internal encodings - by appending one or both encoding
names, separated by colons:
    f = File.new('t.dat', 'rb')
    f.external_encoding # =&gt; #&lt;Encoding:ASCII-8BIT&gt;
    f.internal_encoding # =&gt; nil
    f = File.new('t.dat', 'rb:UTF-16')
    f.external_encoding # =&gt; #&lt;Encoding:UTF-16 (dummy)&gt;
    f.internal_encoding # =&gt; nil
    f = File.new('t.dat', 'rb:UTF-16:UTF-16')
    f.external_encoding # =&gt; #&lt;Encoding:UTF-16 (dummy)&gt;
    f.internal_encoding # =&gt; #&lt;Encoding:UTF-16&gt;
    f.close
- When the external encoding is set, strings read are tagged by that encoding
  when reading, and strings written are converted to that encoding when writing.
- When both external and internal encodings are set, strings read are converted
  from external to internal encoding, and strings written are converted from
  internal to external encoding. For further details about transcoding input and
  output, see Encodings.
  <a target="_blank" rel="noopener noreferrer" href="https://ruby-doc.org/3.2.2/encodings_rdoc.html#label-Encodings">https://ruby-doc.org/3.2.2/encodings_rdoc.html#label-Encodings</a>


String comparison
If the encodings are different then the strings are different.
So it's not a comparison of Unicode codepoints.
    irb(main):026:0&gt; s1 = "hello"
    =&gt; "hello"
    irb(main):027:0&gt; s1
    =&gt; "hello"
    irb(main):028:0&gt; s2 = "hello"
    =&gt; "hello"
    irb(main):029:0&gt; s1 == s2
    =&gt; true
    irb(main):030:0&gt; s2.force_encoding("utf-16")
    =&gt; "\x68\x65\x6C\x6C\x6F"
    irb(main):031:0&gt; s2
    =&gt; "\x68\x65\x6C\x6C\x6F"
    irb(main):032:0&gt; s1 == s2
    =&gt; false


<a target="_blank" rel="noopener noreferrer" href="https://bugs.ruby-lang.org/issues/9111">https://bugs.ruby-lang.org/issues/9111</a>
Encoding-free String comparison
14/11/2013
    ---
    Description
    Currently, strings with the same content but with different encodings count
    as different strings. This causes strange behaviour as below (noted in
    StackOverflow question
    <a target="_blank" rel="noopener noreferrer" href="http://stackoverflow.com/questions/19977788/strange-behavior-in-packed-ruby-strings#19978206):">http://stackoverflow.com/questions/19977788/strange-behavior-in-packed-ruby-strings#19978206):</a>

    [128].pack("C")             # =&gt; "\x80"
    [128].pack("C") == "\x80"   # =&gt; false
    Since [128].pack("C") has the encoding ASCII-8BIT and "\x80" (by default)
    has the encoding UTF-8, the two strings are not equal.

    Also, comparison of strings with different encodings may end up with a messy,
    unintended result.
    I suggest that the comparison String#&lt;=&gt; should not be based on the respective
    encoding of the strings, but all the strings should be internally converted
    to UTF-8 for the purpose of comparison.
    ---
    nobu (Nobuyoshi Nakada)
    It's unacceptable to always convert all strings to UTF-8, should restrict to
    comparison with an ASCII-8BIT string.
    ---
    naruse (Yui NARUSE)
    The standard practice is NFD("√¢") == NFD("a" + "^").
    To NFD, you can use some libraries.
    ---
    duerst (Martin D√ºrst)
    Li√© √† Feature #10084: Add Unicode String Normalization to String class ajout√©
    <a target="_blank" rel="noopener noreferrer" href="https://bugs.ruby-lang.org/issues/10084">https://bugs.ruby-lang.org/issues/10084</a>
    ---
    jlf 09/08/2023: ticket still opened...
    The test [128].pack("C") == "\x80" still returns false, so I assume they made
    no change.


<a target="_blank" rel="noopener noreferrer" href="https://bugs.ruby-lang.org/issues/10084">https://bugs.ruby-lang.org/issues/10084</a>
Add Unicode String Normalization to String class
23/07/2014
    ---
    nobu (Nobuyoshi Nakada)
    What will happen for a non-unicode string, raising an exception?
    ---
    duerst (Martin D√ºrst)
    This is a very good question. I'm okay with whatever Matz and the community
    think is best.

    There are many potential approaches. In general, these will be:
        1. Make the operation a no-op.
        2. Convert to UTF-8, normalize, then convert back.
        3. Implement normalization directly in the encoding.
        4. Raise an exception.

    There is also the question of what a "non-unicode" or "unicode" string is.

    UTF-8 is the preferred way to handle Unicode in Ruby, and is where normalization
    is really needed and will be used.

    For the other encodings, unless we go with 1) or 4), the following considerations
    apply.

    UTF8-Mac, UTF8-DoCoMo, UTF8-KDDI and UTF8-Softbank are essentially UTF-8 but
    with slightly different character conversions. For these encodings, the easiest
    thing to do is force_encoding to UTF-8, normalize, and force_encoding back.
    A C-level implementation may not actually need force_encoding, but a Ruby
    implementation does. There are some questions about what normalizing UTF8-Mac
    means, so that may have to be treated separately. The DoCoMo/KDDI/Softbank
    variants are mostly about emoji, which as far as I know are not affected by
    normalization.

    Then there are UTF-16LE/BE and UTF-32LE/BE. For these, it depends on the
    implementation. A Ruby-level implementation (unless very slow) may want to
    convert to UTF-8 and back. A C-level implementation may not need to do this.

    Then there is also GB18030. Conversion to UTF-8 and back seems to be the best
    solution. Doing normalization directly in GB18030 will need too much data.

    For other, truely non-unicode encodings, implementing noramlization directly
    in the encoding would mean the following: Analyze to what extent the normalization
    applies to the encoding in question, and apply this part.
    As an example, '‚ë†'.nfkc produces '1' in UTF-8, it could do the same in Windows-31J.
    The analysis might take some time (but can be automated), and the data needed
    for each encoding would mostly be just very small.
    ---
    matz (Yukihiro Matsumoto)
    First of all, I don't think normalize is the best name.
    I propose unicode_normalize instead, since this normalization is sort of
    unicode specific.

    It should raise an exception for non Unicode strings.
    It shouldn't convert to UTF-8 implicitly inside.


<a target="_blank" rel="noopener noreferrer" href="https://www.honeybadger.io/blog/troubleshooting-encoding-errors-in-ruby/">https://www.honeybadger.io/blog/troubleshooting-encoding-errors-in-ruby/</a>
Troubleshooting Encoding Errors in Ruby
---
jlf: interesting for the one-liners
---
"H".bytes                                                                                   # =&gt; [72]           in decimal
"H".bytes.map {|e| e.to_s 2}                                                                # =&gt; ["1001000"]    convert in base 2
Encoding.name_list                                                                          # =&gt; ["ASCII-8BIT", "UTF-8", "US-ASCII", "UTF-16BE", "UTF-16LE", "UTF-32BE", "UTF-32LE", "UTF-16", "UTF-32", "UTF8-MAC", "EUC-JP", "Windows-31J", "Big5", "Big5-HKSCS", "Big5-UAO", "CP949", "Emacs-Mule", "EUC-KR", ...]
"hell√î!".encode("US-ASCII")                                                                 # in `encode': U+00D4 from UTF-8 to US-ASCII (Encoding::UndefinedConversionError)
"hell√î!".force_encoding("US-ASCII");                                                        # =&gt; "hell\xC3\x94!"
"abc\xCF\x88\xCF\x88"                                                                       # =&gt; "abcœàœà"
"abcœàœà".force_encoding("US-ASCII").valid_encoding?                                          # =&gt; false
"abcœàœà".encode("US-ASCII", "UTF-8", invalid: :replace, undef: :replace, replace: "")        # =&gt; "abc"
"abc\xA1z".encode("US-ASCII")                                                               # in `encode': "\xA1" on UTF-8 (Encoding::InvalidByteSequenceError)
"abc\xA1z".force_encoding("US-ASCII").scrub("*")                                            # =&gt; "abc*z"
"abc\xA1z".force_encoding("US-ASCII").scrub("")                                             # =&gt; "abcz"
"abc\xA1z".force_encoding("US-ASCII").valid_encoding?                                       # =&gt; false


<hr><h2 id="Rust_lang">Rust lang</h2><hr>
Seen in a comment here :  <a target="_blank" rel="noopener noreferrer" href="https://bugs.swift.org/browse/SR-7602">https://bugs.swift.org/browse/SR-7602</a>

    For reference, I think [Rust's model]( <a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/string/struct.String.html">https://doc.rust-lang.org/std/string/struct.String.html</a> ) is pretty good:

    `from_utf8` produces an error explaining why the code units were invalid
    `from_utf8_lossy` replaces encoding errors with U+FFFD
    `from_utf8_unchecked` which takes the bytes, but if there's an encoding error, then memory safety has been violated

    I'm not entirely sure if accepting invalid bytes requires voiding memory safety (assuming bounds checking always happens), but it is totally a security hazard if used improperly.
    We may want to be very cautious about if/how we expose it.

    I think that trying to do read-time validation is dubious for UTF-16, and totally bananas for UTF-8.


17/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://www.generacodice.com/en/articolo/120763/Unicode+Support+in+Various+Programming+Languages">https://www.generacodice.com/en/articolo/120763/Unicode+Support+in+Various+Programming+Languages</a>
jlf: I learned something: OsStr/OsString
    Rust's strings (std::String and &amp;str) are always valid UTF-8, and do not use null
    terminators, and as a result can not be indexed as an array, like they can be in C/C++, etc.
    They can be sliced somewhat like Go using .get since 1.20, with the caveat that
    it will fail if you try slicing the middle of a code point.

    Rust also has OsStr/OsString for interacting with the Host OS.
    It's byte array on Unix (containing any sequence of bytes).
    On windows it's WTF-8 (A super-set of UTF-8 that handles the improperly
    formed Unicode strings that are allowed in Windows and Javascript),
    &amp;str and String can be freely converted to OsStr or OsString, but require
    checks to covert the other way. Either by Failing on invalid unicode, or
    replacing with the Unicode replacement char. (There is also Path/PathBuf,
    which are just wrappers around OsStr/OsString).

    There is also the CStr and CString types, which represent Null terminated C
    strings, like OsStr on Unix they can contain arbitrary bytes.

    Rust doesn't directly support UTF-16. But can convert OsStr to UCS-2 on windows.


22/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://lib.rs/crates/">https://lib.rs/crates/</a>
STFU-8: Sorta Text Format in UTF-8
STFU-8 is a hacky text encoding/decoding protocol for data that might be not
quite UTF-8 but is still mostly UTF-8.
Its primary purpose is to be able to allow a human to visualize and edit "data"
that is mostly (or fully) visible UTF-8 text. It encodes all non visible or non
UTF-8 compliant bytes as longform text (i.e. ESC becomes the full string r"\x1B").
It can also encode/decode ill-formed UTF-16.


28/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://fasterthanli.me/articles/working-with-strings-in-rust">https://fasterthanli.me/articles/working-with-strings-in-rust</a>


07/11/2021
<a target="_blank" rel="noopener noreferrer" href="https://blog.rust-lang.org/2021/11/01/cve-2021-42574.html">https://blog.rust-lang.org/2021/11/01/cve-2021-42574.html</a>
security concern affecting source code containing "bidirectional override" Unicode codepoints


10/03/2022
<a target="_blank" rel="noopener noreferrer" href="https://rust-lang.github.io/rfcs/2457-non-ascii-idents.html">https://rust-lang.github.io/rfcs/2457-non-ascii-idents.html</a>
Allow non-ASCII letters (such as accented characters, Cyrillic, Greek, Kanji, etc.) in Rust identifiers.


10/09/2022
<a target="_blank" rel="noopener noreferrer" href="https://blog.burntsushi.net/bstr/">https://blog.burntsushi.net/bstr/</a>
A byte string library for Rust
Invalid UTF-8 doesn‚Äôt actually prevent one from applying Unicode-aware algorithms on the parts
of the string that are valid UTF-8. The parts that are invalid UTF-8 are simply ignored.


15/10/2022
<a target="_blank" rel="noopener noreferrer" href="https://crates.io/crates/finl_unicode">https://crates.io/crates/finl_unicode</a>
Library for handling Unicode functionality for finl (categories and grapheme segmentation)
There are these comments in <a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=32700315">https://news.ycombinator.com/item?id=32700315</a>
    All with two-step tables instead of range- and binary search?
    Yes. The two-step tables are really not that expensive and they enable features not possible with range and binary search, like identifying the category of a character cheaply.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/open-i18n/rust-unic">https://github.com/open-i18n/rust-unic</a>
UNIC: Unicode and Internationalization Crates for Rust
jlf: seems stale since Oct 21, 2020. Killed by ICU4X?
This fork is still alive: <a target="_blank" rel="noopener noreferrer" href="https://github.com/eyeplum/rust-unic">https://github.com/eyeplum/rust-unic</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/logannc/fuzzywuzzy-rs">https://github.com/logannc/fuzzywuzzy-rs</a>
port of <a target="_blank" rel="noopener noreferrer" href="https://github.com/seatgeek/fuzzywuzzy">https://github.com/seatgeek/fuzzywuzzy</a>
(Fuzzy String Matching in Python
This project has been renamed and moved to <a target="_blank" rel="noopener noreferrer" href="https://github.com/seatgeek/thefuzz)">https://github.com/seatgeek/thefuzz)</a>
Fuzzy string matching like a boss.
It uses Levenshtein Distance to calculate the differences between sequences in a simple-to-use package.
<a target="_blank" rel="noopener noreferrer" href="https://en.wikipedia.org/wiki/Levenshtein_distance">https://en.wikipedia.org/wiki/Levenshtein_distance</a>


<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/encoding_rs/">https://hsivonen.fi/encoding_rs/</a>
encoding_rs: a Web-Compatible Character Encoding Library in Rust
encoding_rs is a high-decode-performance, low-legacy-encode-footprint and high-correctness implementation
of the WHATWG Encoding Standard written in Rust.
---
<a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/modern-cpp-in-rust/">https://hsivonen.fi/modern-cpp-in-rust/</a>
How I Wrote a Modern C++ Library in Rust
Slides: <a target="_blank" rel="noopener noreferrer" href="https://hsivonen.fi/rustfest2018/">https://hsivonen.fi/rustfest2018/</a>
Video: <a target="_blank" rel="noopener noreferrer" href="https://media.ccc.de/v/rustfest18-5-a_rust_crate_that_also_quacks_like_a_modern_c_library">https://media.ccc.de/v/rustfest18-5-a_rust_crate_that_also_quacks_like_a_modern_c_library</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2773r0.pdf">https://www.open-std.org/jtc1/sc22/wg21/docs/papers/2023/p2773r0.pdf</a>
(pdf...)
    Generally speaking, reducing the size of the tables has a direct impact on
    performance, if only because increasing cache locality is the most effective
    way to improve the performance of anything.
    I landed on a set of strategies developed by the rust team
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/rust/tree/master/src/tools/unicode-table-generator/src">https://github.com/rust-lang/rust/tree/master/src/tools/unicode-table-generator/src</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.youtube.com/watch?v=Mcuqzx3rBWc">https://www.youtube.com/watch?v=Mcuqzx3rBWc</a>
Strings in Rust FINALLY EXPLAINED!
jlf: is there something to learn from 15:29 Indexing into a string? no.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/rust-lang/regex/blob/master/UNICODE.md">https://github.com/rust-lang/regex/blob/master/UNICODE.md</a>
regex Unicode conformance
jlf: I found the URL above in this HN comment (related to awk support of Unicode)
<a target="_blank" rel="noopener noreferrer" href="https://news.ycombinator.com/item?id=32538560">https://news.ycombinator.com/item?id=32538560</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/danielpclark/rutie">https://github.com/danielpclark/rutie</a>
Integrate Ruby with your Rust application. Or integrate Rust with your Ruby application.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/danielpclark/rutie/blob/master/src/class/string.rs">https://github.com/danielpclark/rutie/blob/master/src/class/string.rs</a>


<a target="_blank" rel="noopener noreferrer" href="https://tomdebruijn.com/posts/rust-string-length-width-calculations/">https://tomdebruijn.com/posts/rust-string-length-width-calculations/</a>
Calculating String length and width
<a target="_blank" rel="noopener noreferrer" href="https://github.com/lintje/lintje/blob/501aab06e19008e787237438a69ac961f38bb4b7/src/utils.rs#L22-L71">https://github.com/lintje/lintje/blob/501aab06e19008e787237438a69ac961f38bb4b7/src/utils.rs#L22-L71</a>
    // Return String display width as rendered in a monospace font according to the Unicode
    // specification.


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/gpw2ra/how_is_the_rust_compiler_able_to_tell_the_visible/">https://www.reddit.com/r/rust/comments/gpw2ra/how_is_the_rust_compiler_able_to_tell_the_visible/</a>
How is the Rust compiler able to tell the visible width of unicode characters?
---
jlf: some arbitray excerpts
- rustc uses the unicode-width crate (<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-rs/unicode-width)">https://github.com/unicode-rs/unicode-width)</a>
- Now try it with the rainbow flag emoji. Unicode is hard :)
- explanation:
  the rainbow flag emoji is actually just a white flag + zero width joiner + a rainbow, meaning it's technically three characters.
- Sure but why doesn't the unicode-width crate handle that?
- The unicode-width crate operates on scalar values. I don't believe Unicode has
  a way to determine whether a grapheme cluster is halfwidth/fullwidth. The most
  reasonable way to determine this would probably be the maximum width of any scalar
  value within a grapheme cluster, but this isn't part of any standard and probably
  isn't 100% accurate.
- It is also dependent on the display platform. A platform with support for displaying
  emojis but only in older unicode versions would indeed display multiple emojis
  on the screen. I don't believe there's a platform independent way to detect the
  visual length of any given series of unicode codepoints. For Rust this isn't a
  problem as we restrict the unicode identifiers only to things that are fairly
  homogeneous (namely, no emojis in your variable names!).
- At the bottom of things is the unicode-width native Rust implementation, based
  off the Unicode 13.0 data tables. In C/POSIX land, we would use the function
  wcwidth(). Unfortunately, this isn't the whole story. The actual number of
  columns used is dependent upon your font and the font layout engine.
  See section 7.4 of my Free book, Hacking the Planet! with Notcurses, aka "Fixed-width Fonts Ain't So Fixed."
  <a target="_blank" rel="noopener noreferrer" href="https://nick-black.com/htp-notcurses.pdf#page=57">https://nick-black.com/htp-notcurses.pdf#page=57</a>
  you want pages 47--49 (p49 has some good examples).


<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-rs/unicode-width">https://github.com/unicode-rs/unicode-width</a>
Displayed width of Unicode characters and strings according to UAX#11 rules.
NOTE: The computed width values may not match the actual rendered column width.
For example, the woman scientist emoji comprises of a woman emoji, a zero-width
joiner and a microscope emoji.

    extern crate unicode_width;
    use unicode_width::UnicodeWidthStr;

    fn main() {
        assert_eq!(UnicodeWidthStr::width("üë©"), 2); // Woman
        assert_eq!(UnicodeWidthStr::width("üî¨"), 2); // Microscope
        assert_eq!(UnicodeWidthStr::width("üë©‚Äçüî¨"), 4); // Woman scientist
    }


<a target="_blank" rel="noopener noreferrer" href="https://github.com/life4/textdistance.rs">https://github.com/life4/textdistance.rs</a>
<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/13lo6ne/textdistancers_rust_library_to_compare_strings_or/">https://www.reddit.com/r/rust/comments/13lo6ne/textdistancers_rust_library_to_compare_strings_or/</a>
textdistance.rs: Rust library to compare strings (or any sequences).
25+ algorithms, pure Rust, common interface, Unicode support.
Based on popular and battle-tested textdistance Python library <a target="_blank" rel="noopener noreferrer" href="https://github.com/life4/textdistance">https://github.com/life4/textdistance</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/dguo/strsim-rs">https://github.com/dguo/strsim-rs</a>
Rust implementations of string similarity metrics:
    Hamming
    Levenshtein - distance &amp; normalized
    Optimal string alignment
    Damerau-Levenshtein - distance &amp; normalized
    Jaro and Jaro-Winkler - this implementation of Jaro-Winkler does not limit the common prefix length
    S√∏rensen-Dice


<a target="_blank" rel="noopener noreferrer" href="https://docs.rs/xi-unicode/latest/xi_unicode/">https://docs.rs/xi-unicode/latest/xi_unicode/</a>
Unicode utilities useful for text editing, including a line breaking iterator.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/BurntSushi/bstr">https://github.com/BurntSushi/bstr</a>
A string type for Rust that is not required to be valid UTF-8.
---
jlf: this crate is referenced by Stefan Karpinski in the section Filenames
(search this URL).


<a target="_blank" rel="noopener noreferrer" href="https://www.reddit.com/r/rust/comments/qr0rem/how_many_string_types_does_rust_have_maybe_its/">https://www.reddit.com/r/rust/comments/qr0rem/how_many_string_types_does_rust_have_maybe_its/</a>
How many String types does Rust have? Maybe it's just 1
jlf: to read?


<hr><h2 id="Saxon_lang">Saxon lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.saxonica.com/documentation12/#!localization/unicode-collation-algorithm">https://www.saxonica.com/documentation12/#!localization/unicode-collation-algorithm</a>
Unicode Collation Algorithm


<a target="_blank" rel="noopener noreferrer" href="https://www.saxonica.com/documentation12/index.html#!localization/sorting-and-collations">https://www.saxonica.com/documentation12/index.html#!localization/sorting-and-collations</a>
Sorting and collations


<a target="_blank" rel="noopener noreferrer" href="https://www.saxonica.com/documentation12/index.html#!changes/spi/10-11">https://www.saxonica.com/documentation12/index.html#!changes/spi/10-11</a>
Changes from 10 to 11
Strings
    Most uses of CharSequence have been replaced by a new class
    net.sf.saxon.str.UnicodeString (which also replaces the old class
    net.sf.saxon.regex.UnicodeString).
    The UnicodeString class has a number of implementations.
    All of them are designed to be codepoint-addressible: they expose an
    indexable array of 32-bit codepoint values, and never use surrogate pairs.
    The implementations of UnicodeString include:
        - Twine8:
          a string consisting entirely of codepoints in the range 1-255, held in
          an array with one byte per character.
        - Twine16:
          a string consisting entirely of codepoints in the range 1-65535, held
          in an array with two bytes per character.
        - Twine24:
          a string of arbitrary codepoints, held in an array with three bytes
          per character.
        - Slice8:
          a sub-range of an array using one byte per character.
        - Slice16:
          a sub-range of an array using two bytes per character.
        - Slice24:
          a sub-range of an array using two bytes per character.
        - BMPString:
          a wrapper around a Java/C# string known to contain no surrogate pairs.
        - ZenoString:
          a composite string held as a list of segments, each of which is itself
          a UnicodeString. The name derives from the algorithm used to combine
          segments, which results in segments having progressively decreasing
          lengths towards the end of the string.
        - StringView:
          a wrapper around an arbitrary Java/C# string. (This stores the string
          both in its native Java/C# form, and using a "real" codepoint-
          addressible implementation of UnicodeString, which is constructed
          lazily when it is first required.)

    Unicode normalization of strings (for example in the fn:normalize-unicode()
    function) now uses the JDK class java.text.Normalizer rather than code
    derived from the Unicode Consortium's implementation.
    This appears to be substantially faster.


<a target="_blank" rel="noopener noreferrer" href="https://www.balisage.net/Proceedings/vol26/html/Kay01/BalisageVol26-Kay01.html">https://www.balisage.net/Proceedings/vol26/html/Kay01/BalisageVol26-Kay01.html</a>
ZenoString: A Data Structure for Processing XML Strings
August 2 - 6, 2021
Compare with
- Monolithic char arrays
- Strings in Saxon
- Ropes
- Finger Trees
  <a target="_blank" rel="noopener noreferrer" href="https://www.cambridge.org/core/journals/journal-of-functional-programming/article/finger-trees-a-simple-generalpurpose-data-structure/BF419BCA07292DCAAF2A946E6BDF573B#article">https://www.cambridge.org/core/journals/journal-of-functional-programming/article/finger-trees-a-simple-generalpurpose-data-structure/BF419BCA07292DCAAF2A946E6BDF573B#article</a>
  finger-trees-a-simple-general-purpose-data-structure.pdf


<hr><h2 id="SQL_lang">SQL lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://dev.mysql.com/doc/refman/8.0/en/charset-unicode.html">https://dev.mysql.com/doc/refman/8.0/en/charset-unicode.html</a>
Unicode Support
BMP characters
- can be encoded in a variable-length encoding using 8, 16, or 24 bits (1 to 3 bytes)
- can be encoded in a fixed-length encoding using 16 bits (2 bytes).
Supplementary characters take more space than BMP characters (up to 4 bytes per character).
MySQL supports these Unicode character sets:
- utf8mb4: A UTF-8 encoding of the Unicode character set using one to four bytes per character.
- utf8mb3: A UTF-8 encoding of the Unicode character set using one to three bytes per character.
  This character set is deprecated in MySQL 8.0, and you should use utfmb4 instead.
- utf8: An alias for utf8mb3. In MySQL 8.0, this alias is deprecated; use utf8mb4 instead.
  utf8 is expected in a future release to become an alias for utf8mb4.


<a target="_blank" rel="noopener noreferrer" href="https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-utf8mb4.html">https://dev.mysql.com/doc/refman/8.0/en/charset-unicode-utf8mb4.html</a>
jlf: I take note of this URL for this concatenation rule:
utf8mb4 is a superset of utf8mb3, so for an operation such as the following
concatenation, the result has character set utf8mb4 and the collation of
utf8mb4_col:
    SELECT CONCAT(utf8mb3_col, utf8mb4_col);
Similarly, the following comparison in the WHERE clause works according to the
collation of utf8mb4_col:
    SELECT * FROM utf8mb3_tbl, utf8mb4_tbl
    WHERE utf8mb3_tbl.utf8mb3_col = utf8mb4_tbl.utf8mb4_col;


<a target="_blank" rel="noopener noreferrer" href="https://dev.mysql.com/doc/refman/8.0/en/storage-requirements.html#data-types-storage-reqs-strings">https://dev.mysql.com/doc/refman/8.0/en/storage-requirements.html#data-types-storage-reqs-strings</a>
String Type Storage Requirements


<a target="_blank" rel="noopener noreferrer" href="https://dev.mysql.com/doc/refman/8.0/en/charset-introducer.html">https://dev.mysql.com/doc/refman/8.0/en/charset-introducer.html</a>
Character Set Introducers
A character string literal, hexadecimal literal, or bit-value literal may have
an optional character set introducer and COLLATE clause, to designate it as a
string that uses a particular character set and collation:
    [_charset_name] literal [COLLATE collation_name]
The _charset_name expression is formally called an introducer. It tells the
parser, ‚Äúthe string that follows uses character set charset_name.‚Äù An introducer
does not change the string to the introducer character set like CONVERT() would
do. It does not change the string value, although padding may occur. The
introducer is just a signal.
---
Examples:
    SELECT 'abc';
    SELECT _latin1'abc';
    SELECT _binary'abc';
    SELECT _utf8mb4'abc' COLLATE utf8mb4_danish_ci;

    SELECT _latin1 X'4D7953514C';
    SELECT _utf8mb4 0x4D7953514C COLLATE utf8mb4_danish_ci;

    SELECT _latin1 b'1000001';
    SELECT _utf8mb4 0b1000001 COLLATE utf8mb4_danish_ci;
---
Character string literals can be designated as binary strings by using the
_binary introducer.
    mysql&gt; SET @v1 = X'000D' | X'0BC0';
    mysql&gt; SET @v2 = _binary X'000D' | X'0BC0';
    mysql&gt; SELECT HEX(@v1), HEX(@v2);
    +----------+----------+
    | HEX(@v1) | HEX(@v2) |
    +----------+----------+
    | BCD      | 0BCD     |
    +----------+----------+
---
Followed by rules to determines the character set and collation of a character
string literal, hexadecimal literal, or bit-value literal.
See the page for the details.


<a target="_blank" rel="noopener noreferrer" href="https://www.eversql.com/mysql-utf8-vs-utf8mb4-whats-the-difference-between-utf8-and-utf8mb4/">https://www.eversql.com/mysql-utf8-vs-utf8mb4-whats-the-difference-between-utf8-and-utf8mb4/</a>
MySQL utf8 vs utf8mb4 ‚Äì What‚Äôs the difference between utf8 and utf8mb4?
MySQL decided that UTF-8 can only hold 3 bytes per character (as it's defined
as an alias of utf8mb3). Why? no good reason that I can find documented anywhere.
Few years later, when MySQL 5.5.3 was released, they introduced a new encoding
called utf8mb4, which is actually the real 4-byte utf8 encoding that you know and love.


<a target="_blank" rel="noopener noreferrer" href="https://www.percona.com/blog/migrating-to-utf8mb4-things-to-consider/">https://www.percona.com/blog/migrating-to-utf8mb4-things-to-consider/</a>
Migrating to utf8mb4: Things to Consider
The utf8mb4 character set is the new default as of MySQL 8.0, and this change
neither affects existing data nor forces any upgrades.
Migration to utf8mb4 has many advantages including:
- It can store more symbols, including emojis
- It has new collations for Asian languages
- It is faster than utf8mb3


<hr><h2 id="Swift_lang">Swift lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-evolution/blob/main/proposals/0363-unicode-for-string-processing.md">https://github.com/apple/swift-evolution/blob/main/proposals/0363-unicode-for-string-processing.md</a>
Proposal: Unicode for String Processing
This proposal describes Regex's rich Unicode support during regex matching,
along with the character classes and options that define and modify that behavior.
This proposal is one component of a larger regex-powered string processing initiative.


<a target="_blank" rel="noopener noreferrer" href="https://docs.swift.org/swift-book/documentation/the-swift-programming-language/stringsandcharacters/">https://docs.swift.org/swift-book/documentation/the-swift-programming-language/stringsandcharacters/</a>
Strings and Characters
    Every string is composed of encoding-independent Unicode characters, and provides
    support for accessing those characters in various Unicode representations.

    When a Unicode string is written to a text file or some other storage, the Unicode
    scalars in that string are encoded in one of several Unicode-defined encoding forms.
    Each form encodes the string in small chunks known as code units. These include the
    UTF-8 encoding form (which encodes a string as 8-bit code units), the UTF-16 encoding
    form (which encodes a string as 16-bit code units), and the UTF-32 encoding form
    (which encodes a string as 32-bit code units).


03/08/2021
<a target="_blank" rel="noopener noreferrer" href="https://swiftdoc.org/v5.1/type/string/">https://swiftdoc.org/v5.1/type/string/</a>
Auto-generated documentation for Swift.
A Unicode string value that is a collection of characters.


<a target="_blank" rel="noopener noreferrer" href="https://developer.apple.com/documentation/swift/string">https://developer.apple.com/documentation/swift/string</a>


<a target="_blank" rel="noopener noreferrer" href="https://www.simpleswiftguide.com/get-character-from-string-using-its-index-in-swift/">https://www.simpleswiftguide.com/get-character-from-string-using-its-index-in-swift/</a>
jlf: no direct access to a character
Doesn't work:
    let input = "Swift Tutorials"
    let char = input[3]
Work:
    let input = "Swift Tutorials"
    let char = input[input.index(input.startIndex, offsetBy: 3)]
A "workaround" to have direct access
    extension StringProtocol {
        subscript(offset: Int) -&gt; Character {
            self[index(startIndex, offsetBy: offset)]
        }
    }
Which can be used just like that:
    let input = "Swift Tutorials"
    let char = input[3]


<a target="_blank" rel="noopener noreferrer" href="https://gist.github.com/paultopia/6609780e7b53676b7dfc55736221cd23">https://gist.github.com/paultopia/6609780e7b53676b7dfc55736221cd23</a>
paultopia/monkey_patch_slicing_into_string.swift
Another "workaround" to have direct access to the characters like that:
    var s = "here is a boring string"
    print(s.getCharList())
    print(s[1])
    print(s[-1])
    print(s[0, 5])
    print(s[5, 0])
    print(s[3...6])
    print(s[2..&lt;10])
    print(s[...15])
    print(s[2...])
    print(s[..&lt;15])


<a target="_blank" rel="noopener noreferrer" href="https://developer.apple.com/documentation/swift/unicode/canonicalcombiningclass">https://developer.apple.com/documentation/swift/unicode/canonicalcombiningclass</a>
Unicode.CanonicalCombiningClass
The classification of a scalar used in the Canonical Ordering Algorithm defined by the Unicode Standard.
---
Canonical combining classes are used by the ordering algorithm to determine if
two sequences of combining marks should be considered canonically equivalent
(that is, identical in interpretation). Two sequences are canonically equivalent
if they are equal when sorting the scalars in ascending order by their combining class.
---
aboveBeforeBelow = "\u{0041}\u{0301}\u{0316}"~text~unescape
belowBeforeAbove = "\u{0041}\u{0316}\u{0301}"~text~unescape
aboveBeforeBelow~compareTo(belowBeforeAbove)=       -- 0 (good, means equal)
aboveBeforeBelow == belowBeforeAbove=               -- .true


15/07/2017
String Processing For Swift 4
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/master/docs/StringManifesto.md">https://github.com/apple/swift/blob/master/docs/StringManifesto.md</a>


<a target="_blank" rel="noopener noreferrer" href="https://swift.org/blog/utf8-string/">https://swift.org/blog/utf8-string/</a>
Swift 5 switches the preferred encoding of strings from UTF-16 to UTF-8 while preserving efficient Objective-C-interoperability.
jlf: Search "breadcrumb".
Notice that the article is about Swift Objective-C interoperability.
The language Swift itself is not allowing random access to characters.
---
Swift 5, like Rust, performs encoding validation once on creation, when it is far
more efficient to do so. NSStrings, which are lazily bridged (zero-copy) into
Swift and use UTF-16, may contain invalid content (i.e. isolated surrogates).
As in Swift 4.2, these are lazily validated when read from.


<a target="_blank" rel="noopener noreferrer" href="https://bugs.swift.org/browse/SR-7602">https://bugs.swift.org/browse/SR-7602</a> (redirect to next URL)
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/issues/50144">https://github.com/apple/swift/issues/50144</a>
UTF8 should be (one of) the fastest String encoding(s)
---
Requirements:

    being able to copy UTF-8 encoded bytes from a String into a pre-allocated raw buffer
    must be allocation-free and as fast as memcpy can copy them

    creating a String from UTF-8 encoded bytes should just validate the encoding and store the bytes as they are
    (jlf: "and store the bytes as they are" --&gt; YES!)

    slightly softer but still very strong requirement: currently (even with ASCII)
    only the stdlib seems to be able to get a pointer to the contiguous ASCII representation
    (if at all in that form). That works fine if you just want to copy the bytes
    (UnsafeMutableBufferPointer(start: destinationStart, count: destinationLength).initialize(from: string.utf8)
    which will use memcpy if in ASCII representation) but doesn't allow you to implement
    your own algorithms that are only performant on a contiguously stored [UInt8]
---
jlf: this comment in the thread is particularly interesting, because it reminds
me what was said on the ARB mailing list about byte versus string.
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/issues/50144#issuecomment-1108303710">https://github.com/apple/swift/issues/50144#issuecomment-1108303710</a>
    May 9, 2018

    @milseman Virtually all of it comes down to `String(data: myData, encoding: .utf8)`
    and `myString.data(encoding: .utf8)`.

    When parsing protocols such as HTTP, Redis, MySQL, PostgreSQL, etc we will read data from
    the OS into an `UnsafeBufferPointer&lt;UInt8&gt;`. This is almost always via NIO's
    [`ByteBuffer`](<a target="_blank" rel="noopener noreferrer" href="https://apple.github.io/swift-nio/docs/current/NIO/Structs/ByteBuffer.html)">https://apple.github.io/swift-nio/docs/current/NIO/Structs/ByteBuffer.html)</a> type.
    We sometimes grab `String` from that directly or grab `Data` if we want to iterate over the bytes
    for additional parsing.

    In other words, from `UnsafePointer&lt;UInt8&gt;` we commonly read `FixedWidthInteger`,
    `BinaryFloatingPoint`, `Data`, and `String`. All are very performant except String
    which is the concern since the vast majority of bytes ends up being `String`s.
    Considering the DB use case specifically, the data transfer is usually emails,
    names, bios, comments, etc. Very few bytes are actually dedicated to binary
    numbers or data blobs. Strings everywhere.

    To summarize, the faster we can get from `Swift.Unsafe...Pointer&lt;UInt8&gt;` or
    `Foundation.Data` to `String` the better. That will affect (for the better!)
    quite literally our entire framework.
---
jlf: this comment from the same thread shows which questions we should answer for Rexx:
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/issues/50144#issuecomment-1108303720">https://github.com/apple/swift/issues/50144#issuecomment-1108303720</a>

    Along the lines of potentially separable issues, what is your validation story?
    If the stream of bytes contains invalid UTF-8, do you want:

    1) The initializer to fail resulting in nil
    2) The initializer to fail producing an error
    3) The invalid bytes to be replaced with U+FFFD
    4) The bytes verbatim, and experience the emergent behavior / unspecified results / security hazard from those bytes.

    For reference, I think [Rust's model](<a target="_blank" rel="noopener noreferrer" href="https://doc.rust-lang.org/std/string/struct.String.html)">https://doc.rust-lang.org/std/string/struct.String.html)</a> is pretty good:

    `from_utf8` produces an error explaining why the code units were invalid
    `from_utf8_lossy` replaces encoding errors with U+FFFD
    `from_utf8_unchecked` which takes the bytes, but if there's an encoding error, then memory safety has been violated

    I'm not entirely sure if accepting invalid bytes requires voiding memory safety
    (assuming bounds checking always happens), but it is totally a security hazard if used improperly.
    We may want to be very cautious about if/how we expose it.

    I think that trying to do read-time validation is dubious for UTF-16, and totally bananas for UTF-8.
    (jlf: I don't understand this last sentence. By "read-time", does he means "when working with the string?")


milseman Michael Ilseman added a comment - 5 Nov 2018 3:44 PM
    It's now the fastest encoding.
    <a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/string-s-abi-and-utf-8/17676/1">https://forums.swift.org/t/string-s-abi-and-utf-8/17676/1</a>
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/pull/20315">https://github.com/apple/swift/pull/20315</a>


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/7e68e8f4a3cb1173e909dc22a3490c05e43fa592/stdlib/public/core/StringObject.swift">https://github.com/apple/swift/blob/7e68e8f4a3cb1173e909dc22a3490c05e43fa592/stdlib/public/core/StringObject.swift</a>
swift/stdlib/public/core/StringObject.swift
jlf: the link above is a frozen link
To have an up-to-date view, go to
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/tree/main/stdlib/public/core">https://github.com/apple/swift/tree/main/stdlib/public/core</a>
Many code to review!
    String.swift
    StringBreadcrumbs.swift
    StringBridge.swift
    StringCharacterView.swift
    StringComparable.swift
    StringComparison.swift
    StringCreate.swift
    StringGraphemeBreaking.swift
        jlf: Apparently, there are some difficulties when going backwards .
          // When walking backwards, it's impossible to know whether we were in an emoji
          // sequence without walking further backwards. This walks the string backwards
          // enough until we figure out whether or not to break our
          // (.zwj, .extendedPictographic) question.

          // When walking backwards, it's impossible to know whether we break when we
          // see our first (.regionalIndicator, .regionalIndicator) without walking
          // further backwards. This walks the string backwards enough until we figure
          // out whether or not to break these RIs.
    StringGuts.swift
    StringGutsRangeReplaceable.swift
    StringGutsSlice.swift
    StringHashable.swift
    StringIndex.swift
    StringIndexConversions.swift
    StringIndexValidation.swift
    StringInterpolation.swift
    StringLegacy.swift
    StringNormalization.swift
    StringObject.swift
    StringProtocol.swift
    StringRangeReplaceableCollection.swift
    StringStorage.swift
    StringStorageBridge.swift
    StringSwitch.swift
    StringTesting.swift
    StringUTF16View.swift
    StringUTF8Validation.swift
    StringUTF8View.swift
    StringUnicodeScalarView.swift
    StringWordBreaking.swift
    Substring.swift


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/blob/main/stdlib/public/core/StringBreadcrumbs.swift">https://github.com/apple/swift/blob/main/stdlib/public/core/StringBreadcrumbs.swift</a>
Breadcrumb optimization
The distance between successive breadcrumbs, measured in UTF-16 code units is 64.
internal static var breadcrumbStride: Int { 64 }
jlf: nothing sophisticated here...
They scan the whole string by iterating over the UTF-16 indexes and when i % stride == 0 then self.crumbs.append(curIdx)
When searching the offset for a String.Index, they do a binary search.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift/pull/20315/commits/2e368a3f6a25b5e84c0f682861ea0a5c9b3b26af">https://github.com/apple/swift/pull/20315/commits/2e368a3f6a25b5e84c0f682861ea0a5c9b3b26af</a>
[String] Introduce StringBreadcrumbs
Breadcrumbs provide us amortized O(1) access to the UTF-16 view, which
is vital for efficient Cocoa interoperability.
---
jlf: this is the commit where breadcrumbs are added to Swift (Nov 4, 2018).


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/55389444/whats-does-extended-grapheme-clusters-are-canonically-equivalent-means-in-term">https://stackoverflow.com/questions/55389444/whats-does-extended-grapheme-clusters-are-canonically-equivalent-means-in-term</a>
Whats does ‚Äúextended grapheme clusters are canonically equivalent‚Äù means in terms of Swift String?
jlf:
They don't answer to the question :-(
no explanation about "canonically equivalent", just ONE poor example, no general definition.


<a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/pitch-unicode-equivalence-for-swift-source/21576/6">https://forums.swift.org/t/pitch-unicode-equivalence-for-swift-source/21576/6</a>
Pitch: Unicode Equivalence for Swift Source
jlf: interersting
Mar 13,2019
In short, there is a thorough set of rules already laid out in UAX#31 on how to normalize identifiers in programming languages.
Several of us have written several versions of a proposal to adopt it, but each time it has failed because of issues with emoji.
Recent versions of Unicode now have more robust classifications for emoji, so the proposal can be resurrected with better luck now, probably.
No need to start from scratch; feel free to build on the work that we‚Äôve already done.
All of this applies only to identifiers. Literals should never be messed with by the compiler.
That are, after all, supposed to be literals.


13/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-evolution/blob/main/proposals/0211-unicode-scalar-properties.md">https://github.com/apple/swift-evolution/blob/main/proposals/0211-unicode-scalar-properties.md</a>
Add Unicode Properties to Unicode.Scalar
    Issues Linking with ICU
    The Swift standard library uses the system's ICU libraries to implement its Unicode support.
    A third-party developer may expect that they could also link their application directly to the system ICU
    to access the functionality that they need, but this proves problematic on both Apple and Linux platforms.
    Apple
        On Apple operating systems, libicucore.dylib is built with function renaming disabled
        (function names lack the _NN version number suffix). This makes it fairly straightforward to import the C APIs
        and call them from Swift without worrying about which version the operating system is using.
        Unfortunately, libicucore.dylib is considered to be private API for submissions to the App Store,
        so applications doing this will be rejected. Instead, users must built their own copy of ICU from source
        and link that into their applications. This is significant overhead.
    Linux
        On Linux, system ICU libraries are built with function renaming enabled (the default),
        so function names have the _NN version number suffix. Function renaming makes it more difficult
        to use these APIs from Swift; even though the C header files contain #defines that map function names
        like u_foo_59 to u_foo, these #defines are not imported into Swift‚Äîonly the suffixed function names are available.
        This means that Swift bindings would be fixed to a specific version of the library without some other intermediary layer.
        Again, this is significant overhead.
    extension Unicode.Scalar.Properties {
      public var isAlphabetic: Bool { get }    // Alphabetic
      public var isASCIIHexDigit: Bool { get }    // ASCII_Hex_Digit
      public var isBidiControl: Bool { get }    // Bidi_Control
      public var isBidiMirrored: Bool { get }    // Bidi_Mirrored
      public var isDash: Bool { get }    // Dash
      public var isDefaultIgnorableCodePoint: Bool { get }    // Default_Ignorable_Code_Point
      public var isDeprecated: Bool { get }    // Deprecated
      public var isDiacritic: Bool { get }    // Diacritic
      public var isExtender: Bool { get }    // Extender
      public var isFullCompositionExclusion: Bool { get }    // Full_Composition_Exclusion
      public var isGraphemeBase: Bool { get }    // Grapheme_Base
      public var isGraphemeExtend: Bool { get }    // Grapheme_Extend
      public var isHexDigit: Bool { get }    // Hex_Digit
      public var isIDContinue: Bool { get }    // ID_Continue
      public var isIDStart: Bool { get }    // ID_Start
      public var isIdeographic: Bool { get }    // Ideographic
      public var isIDSBinaryOperator: Bool { get }    // IDS_Binary_Operator
      public var isIDSTrinaryOperator: Bool { get }    // IDS_Trinary_Operator
      public var isJoinControl: Bool { get }    // Join_Control
      public var isLogicalOrderException: Bool { get }    // Logical_Order_Exception
      public var isLowercase: Bool { get }    // Lowercase
      public var isMath: Bool { get }    // Math
      public var isNoncharacterCodePoint: Bool { get }    // Noncharacter_Code_Point
      public var isQuotationMark: Bool { get }    // Quotation_Mark
      public var isRadical: Bool { get }    // Radical
      public var isSoftDotted: Bool { get }    // Soft_Dotted
      public var isTerminalPunctuation: Bool { get }    // Terminal_Punctuation
      public var isUnifiedIdeograph: Bool { get }    // Unified_Ideograph
      public var isUppercase: Bool { get }    // Uppercase
      public var isWhitespace: Bool { get }    // Whitespace
      public var isXIDContinue: Bool { get }    // XID_Continue
      public var isXIDStart: Bool { get }    // XID_Start
      public var isCaseSensitive: Bool { get }    // Case_Sensitive
      public var isSentenceTerminal: Bool { get }    // Sentence_Terminal (S_Term)
      public var isVariationSelector: Bool { get }    // Variation_Selector
      public var isNFDInert: Bool { get }    // NFD_Inert
      public var isNFKDInert: Bool { get }    // NFKD_Inert
      public var isNFCInert: Bool { get }    // NFC_Inert
      public var isNFKCInert: Bool { get }    // NFKC_Inert
      public var isSegmentStarter: Bool { get }    // Segment_Starter
      public var isPatternSyntax: Bool { get }    // Pattern_Syntax
      public var isPatternWhitespace: Bool { get }    // Pattern_White_Space
      public var isCased: Bool { get }    // Cased
      public var isCaseIgnorable: Bool { get }    // Case_Ignorable
      public var changesWhenLowercased: Bool { get }    // Changes_When_Lowercased
      public var changesWhenUppercased: Bool { get }    // Changes_When_Uppercased
      public var changesWhenTitlecased: Bool { get }    // Changes_When_Titlecased
      public var changesWhenCaseFolded: Bool { get }    // Changes_When_Casefolded
      public var changesWhenCaseMapped: Bool { get }    // Changes_When_Casemapped
      public var changesWhenNFKCCaseFolded: Bool { get }    // Changes_When_NFKC_Casefolded
      public var isEmoji: Bool { get }    // Emoji
      public var isEmojiPresentation: Bool { get }    // Emoji_Presentation
      public var isEmojiModifier: Bool { get }    // Emoji_Modifier
      public var isEmojiModifierBase: Bool { get }    // Emoji_Modifier_Base
    }
    extension Unicode.Scalar.Properties {

      // Implemented in terms of ICU's `u_isdefined`.
      public var isDefined: Bool { get }
    }
    Case Mappings
    The properties below provide full case mappings for scalars. Since a handful of mappings result in multiple scalars (e.g., "√ü" uppercases to "SS"), these properties are String-valued, not Unicode.Scalar.
    extension Unicode.Scalar.Properties {

      public var lowercaseMapping: String { get }  // u_strToLower
      public var titlecaseMapping: String { get }  // u_strToTitle
      public var uppercaseMapping: String { get }  // u_strToUpper
    }
Identification and Classification
    extension Unicode.Scalar.Properties {

      /// Corresponds to the `Age` Unicode property, when a code point was first
      /// defined.
      public var age: Unicode.Version? { get }

      /// Corresponds to the `Name` Unicode property.
      public var name: String? { get }

      /// Corresponds to the `Name_Alias` Unicode property.
      public var nameAlias: String? { get }

      /// Corresponds to the `General_Category` Unicode property.
      public var generalCategory: Unicode.GeneralCategory { get }

      /// Corresponds to the `Canonical_Combining_Class` Unicode property.
      public var canonicalCombiningClass: Unicode.CanonicalCombiningClass { get }
    }

    extension Unicode {

      /// Represents the version of Unicode in which a scalar was introduced.
      public typealias Version = (major: Int, minor: Int)

      /// General categories returned by
      /// `Unicode.Scalar.Properties.generalCategory`. Listed along with their
      /// two-letter code.
      public enum GeneralCategory {
        case uppercaseLetter  // Lu
        case lowercaseLetter  // Ll
        case titlecaseLetter  // Lt
        case modifierLetter  // Lm
        case otherLetter  // Lo

        case nonspacingMark  // Mn
        case spacingMark  // Mc
        case enclosingMark  // Me

        case decimalNumber  // Nd
        case letterlikeNumber  // Nl
        case otherNumber  // No

        case connectorPunctuation  //Pc
        case dashPunctuation  // Pd
        case openPunctuation  // Ps
        case closePunctuation  // Pe
        case initialPunctuation  // Pi
        case finalPunctuation  // Pf
        case otherPunctuation  // Po

        case mathSymbol  // Sm
        case currencySymbol  // Sc
        case modifierSymbol  // Sk
        case otherSymbol  // So

        case spaceSeparator  // Zs
        case lineSeparator  // Zl
        case paragraphSeparator  // Zp

        case control  // Cc
        case format  // Cf
        case surrogate  // Cs
        case privateUse  // Co
        case unassigned  // Cn
      }

      public struct CanonicalCombiningClass:
        Comparable, Hashable, RawRepresentable
      {
        public static let notReordered = CanonicalCombiningClass(rawValue: 0)
        public static let overlay = CanonicalCombiningClass(rawValue: 1)
        public static let nukta = CanonicalCombiningClass(rawValue: 7)
        public static let kanaVoicing = CanonicalCombiningClass(rawValue: 8)
        public static let virama = CanonicalCombiningClass(rawValue: 9)
        public static let attachedBelowLeft = CanonicalCombiningClass(rawValue: 200)
        public static let attachedBelow = CanonicalCombiningClass(rawValue: 202)
        public static let attachedAbove = CanonicalCombiningClass(rawValue: 214)
        public static let attachedAboveRight = CanonicalCombiningClass(rawValue: 216)
        public static let belowLeft = CanonicalCombiningClass(rawValue: 218)
        public static let below = CanonicalCombiningClass(rawValue: 220)
        public static let belowRight = CanonicalCombiningClass(rawValue: 222)
        public static let left = CanonicalCombiningClass(rawValue: 224)
        public static let right = CanonicalCombiningClass(rawValue: 226)
        public static let aboveLeft = CanonicalCombiningClass(rawValue: 228)
        public static let above = CanonicalCombiningClass(rawValue: 230)
        public static let aboveRight = CanonicalCombiningClass(rawValue: 232)
        public static let doubleBelow = CanonicalCombiningClass(rawValue: 233)
        public static let doubleAbove = CanonicalCombiningClass(rawValue: 234)
        public static let iotaSubscript = CanonicalCombiningClass(rawValue: 240)

        public let rawValue: UInt8

        public init(rawValue: UInt8)
      }
    }
    Numerics
    Many Unicode scalars have associated numeric values.
    These are not only the common digits zero through nine, but also vulgar fractions
    and various other linguistic characters and ideographs that have an innate numeric value.
    These properties are exposed below. They can be useful for determining whether segments
    of text contain numbers or non-numeric data, and can also help in the design of algorithms
    to determine the values of such numbers.
    extension Unicode.Scalar.Properties {

      /// Corresponds to the `Numeric_Type` Unicode property.
      public var numericType: Unicode.NumericType?

      /// Corresponds to the `Numeric_Value` Unicode property.
      public var numericValue: Double?
    }

    extension Unicode {

      public enum NumericType {
        case decimal
        case digit
        case numeric
      }
    }


14/06/2021
<a target="_blank" rel="noopener noreferrer" href="https://lists.isocpp.org/sg16/2018/08/0121.php">https://lists.isocpp.org/sg16/2018/08/0121.php</a>
Feedback from swift team

    Swift strings now sort with NFC (currently UTF-16 code unit order, but likely changed to Unicode scalar value order).
    We didn't find FCC significantly more compelling in practice. Since NFC is far more frequent in the wild
    (why waste space if you don't have to), strings are likely to already be in NFC.
    We have fast-paths to detect on-the-fly normal sections of strings (e.g. all ASCII, all &lt; U+0300, NFC_QC=yes, etc.).
    We lazily normalize portions of string during comparison when needed.
    Q: Swift strings support comparison via normalization. Has use of canonical string equality been a performance issue?
       Or been a source of surprise to programmers?
    A: This was a big performance issue on Linux, where we used to do UCA+DUCET based comparisons.
       We switch to lexicographical order of NFC-normalized UTF-16 code units (future: scalar values),
       and saw a very significant speed up there. The remaining performance work revolves around checking
       and tracking whether a string is known to already be in a normal form, so we can just memcmp.
    Q: I'm curious why this was a larger performance issue for Linux than for (presumably) macOS and/or iOS.
    A: There were two main factors.
       The first is that on Darwin platforms, CFString had an implementation that we used instead of UCA+DUCET which was faster.
       The second is that Darwin platforms are typically up-to-date and have very recent versions of ICU.
       On Linux, we still support Ubuntu LTS 14.04 which has a version of ICU which predates Swift and didn't have any fast-paths for ASCII or mostly-ASCII text.
       Switching to our own implementation based on NFC gave us many X improvement over CFString, which in turn was many X faster than UCA+DUCET (especially on older versions of ICU).
    Q: How firmly is the Swift string implementation tied to ICU?
       If the C++ standard library were to add suitable Unicode support, what would motivate reimplementing Swift strings on top of it?
    A: Swift's tie to ICU is less firm than it used to be
       If the C++ standard library provided these operations, sufficiently up-to-date with Unicode version and comparable or better to ICU in performance,
       we would be willing to switch. A big pain in interacting with ICU is their limited support for UTF-8.
       Some users who would like to use a lighter-weight Swift and are unhappy at having to link against ICU, as it's fairly large, and it can complicate security audits.


<a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/pitch-unicode-for-string-processing/56907/6">https://forums.swift.org/t/pitch-unicode-for-string-processing/56907/6</a>
[Pitch] Unicode for String Processing


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-evolution/blob/main/proposals/0350-regex-type-overview.md">https://github.com/apple/swift-evolution/blob/main/proposals/0350-regex-type-overview.md</a>
jlf: surprising intro!
Swift strings provide an obsessively Unicode-forward model of programming with strings.
String processing with Collection's algorithms is woefully inadequate for many day-to-day
tasks compared to other popular programming and scripting languages.
We propose addressing this basic shortcoming through an effort we are calling regex.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/ProposalOverview.md">https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/ProposalOverview.md</a>
Regex Proposals
todo: read String processing algorithms <a target="_blank" rel="noopener noreferrer" href="https://forums.swift.org/t/pitch-regex-powered-string-processing-algorithms/55969">https://forums.swift.org/t/pitch-regex-powered-string-processing-algorithms/55969</a>
todo: read Unicode for String Processing <a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/UnicodeForStringProcessing.md">https://github.com/apple/swift-experimental-string-processing/blob/main/Documentation/Evolution/UnicodeForStringProcessing.md</a>


<a target="_blank" rel="noopener noreferrer" href="https://stackoverflow.com/questions/41059974/german-character-%C3%9F-uppercased-in-ss">https://stackoverflow.com/questions/41059974/german-character-%C3%9F-uppercased-in-ss</a>
"√ü" is converted to "SS" when using uppercased().
---
Use caseInsensitiveCompare() instead of converting the strings to upper or lowercase:
    let s1 = "gru√ü"
    let s2 = "GRU√ü"
    let eq = s1.caseInsensitiveCompare(s2) == .orderedSame
    print(eq) // true
This compares the strings in a case-insensitive way according to the Unicode standard.
There is also localizedCaseInsensitiveCompare() which does a comparison according to the current locale, and
    s1.compare(s2, options: .caseInsensitive, locale: ...)
for a case-insensitive comparison according to an arbitrary given locale.


<a target="_blank" rel="noopener noreferrer" href="https://www.kodeco.com/3418439-encoding-and-decoding-in-swift">https://www.kodeco.com/3418439-encoding-and-decoding-in-swift</a>
jlf: out of subject, it's not related to strings. It's about serialization of data strctures.


<a target="_blank" rel="noopener noreferrer" href="https://github.com/apple/swift-evolution/blob/main/proposals/0241-string-index-explicit-encoding-offset.md">https://github.com/apple/swift-evolution/blob/main/proposals/0241-string-index-explicit-encoding-offset.md</a>
Deprecate String Index Encoded Offsets
Feb 23, 2019
jlf: I add this URL for this description, not for the topic covered by this proposition:
    String abstracts away details about the underlying encoding used in its storage.
    String.Index is opaque and represents a position within a String or Substring.
    This can make serializing a string alongside its indices difficult, and for that
    reason SE-0180 added a computed variable and initializer encodedOffset in Swift 4.0.

    String was always meant to be capable of handling multiple backing encodings for
    its contents, and this is realized in Swift 5. String now uses UTF-8 for its
    preferred ‚Äúfast‚Äù native encoding, but has a resilient fallback for strings of
    different encodings. Currently, we only use this fall-back for lazily-bridged
    Cocoa strings, which are commonly encoded as UTF-16, though it can be extended
    in the future thanks to resilience.

    Unfortunately, SE-0180‚Äôs approach of a single notion of encodedOffset is flawed.
    A string can be serialized with a choice of encodings, and the offset is therefore
    encoding-dependent and requires access to the contents of the string to calculate.


<a target="_blank" rel="noopener noreferrer" href="https://www.tutorialkart.com/swift-tutorial/swift-read-text-file/#gsc.tab=0">https://www.tutorialkart.com/swift-tutorial/swift-read-text-file/#gsc.tab=0</a>
Read text file

    import Foundation
    let file = "sample.txt"
    var result = ""
    //if you get access to the directory
    if let dir = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first {
        //prepare file url
        let fileURL = dir.appendingPathComponent(file)
        do {
            result = try String(contentsOf: fileURL, encoding: .utf8)
        }
        catch {/* handle if there are any errors */}
    }
    print(result)


<a target="_blank" rel="noopener noreferrer" href="https://www.appsdeveloperblog.com/read-and-write-string-into-a-text-file/">https://www.appsdeveloperblog.com/read-and-write-string-into-a-text-file/</a>
Read and Write String Into a Text File

      let fileName = "myFileName.txt"
      var filePath = ""

      // Fine documents directory on device
      let dirs : [String] = NSSearchPathForDirectoriesInDomains(FileManager.SearchPathDirectory.documentDirectory, FileManager.SearchPathDomainMask.allDomainsMask, true)
      if dirs.count &gt; 0 {
          let dir = dirs[0] //documents directory
          filePath = dir.appending("/" + fileName)
          print("Local path = \(filePath)")
      } else {
          print("Could not find local directory to store file")
          return
      }

      // Set the contents
      let fileContentToWrite = "Text to be recorded into file"
      do {
          // Write contents to file
          try fileContentToWrite.write(toFile: filePath, atomically: false, encoding: String.Encoding.utf8)
      }
      catch let error as NSError {
          print("An error took place: \(error)")
      }

      // Read file content. Example in Swift
      do {
          // Read file content
          let contentFromFile = try NSString(contentsOfFile: filePath, encoding: String.Encoding.utf8.rawValue)
          print(contentFromFile)
      }
      catch let error as NSError {
          print("An error took place: \(error)")
      }


Testing the JMB's example
"œÇ".uppercased()                  // "Œ£"
"œÉ".uc                            // "Œ£"
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£".lowercased()         // "·ΩÄŒ¥œÖœÉœÉŒµœçœÉ"     NOT SUPPORTED last Œ£ becomes œÇ
"·ΩàŒîŒ•Œ£Œ£ŒïŒéŒ£A".lowercased()        // "·ΩÄŒ¥œÖœÉœÉŒµœçœÉa"    last Œ£ becomes œÉ


<a target="_blank" rel="noopener noreferrer" href="https://developer.apple.com/documentation/swift/character/isnewline">https://developer.apple.com/documentation/swift/character/isnewline</a>
isNewline
A Boolean value indicating whether this character represents a newline.
For example, the following characters all represent newlines:
    ‚Äú\n‚Äù (U+000A): LINE FEED (LF)
    U+000B: LINE TABULATION (VT)
    U+000C: FORM FEED (FF)
    ‚Äú\r‚Äù (U+000D): CARRIAGE RETURN (CR)
    ‚Äú\r\n‚Äù (U+000D U+000A): CR-LF
    U+0085: NEXT LINE (NEL)
    U+2028: LINE SEPARATOR
    U+2029: PARAGRAPH SEPARATOR
---
jlf: this is related to Unicode properties of a character.
But what is the impacts on file I/O?


<hr><h2 id="Typst_lang">Typst lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://github.com/typst/typst">https://github.com/typst/typst</a>
A new markup-based typesetting system that is powerful and easy to learn.
---
jlf: uses ICU4X
<a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/3811">https://github.com/unicode-org/icu4x/issues/3811</a>


<hr><h2 id="XPath_lang">XPath lang</h2><hr>
<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/xpath-functions-31/#string-functions">https://www.w3.org/TR/xpath-functions-31/#string-functions</a>
Functions on strings
jlf:
to read
no "grapheme" in this document.
written by Michael Kay (XSLT WG), Saxonica &lt;<a target="_blank" rel="noopener noreferrer" href="http://www.saxonica.com/>">http://www.saxonica.com/></a>


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/xpath-functions-31/#string.match">https://www.w3.org/TR/xpath-functions-31/#string.match</a>
String functions that use regular expressions
jlf: part of the doc "Functions on strings" above, explicitely referenced for
direct access.


<a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/xpath-functions-31/#func-collation-key">https://www.w3.org/TR/xpath-functions-31/#func-collation-key</a>
Referenced in <a target="_blank" rel="noopener noreferrer" href="https://github.com/unicode-org/icu4x/issues/2689#issuecomment-1743127855">https://github.com/unicode-org/icu4x/issues/2689#issuecomment-1743127855</a>
    hsivonen:
        I'm quite skeptical of processes that use XPath having the kind of lifetimes
        and numbers of comparisons that computing a sort key is justified, but whether
        or not exposing sort keys in XPath is a good idea, it's good to know that XPath
        has this dependency.
    faassen:
        I think the XPath spec (the library portion) has been influenced by the capabilities of ICU4J.
        The motivation for this facility is described in the "notes" section:
        <a target="_blank" rel="noopener noreferrer" href="https://www.w3.org/TR/xpath-functions-31/#func-collation-key">https://www.w3.org/TR/xpath-functions-31/#func-collation-key</a>
        and is basically to use this as a collation-dependent hashmap key.
        I can't judge myself how useful that is, so I'll defer to your skepticism.
        I'll note however that this same specification also provides the function
        library available to XQuery, and with XQuery the lifetimes and numbers of
        comparisons are likely to be much bigger.


<hr><h2 id="Zig_lang__Ziglyph">Zig lang, Ziglyph</h2><hr>
04/07/2021
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jecolon/ziglyph">https://github.com/jecolon/ziglyph</a>
Unicode text processing for the Zig programming language.

<a target="_blank" rel="noopener noreferrer" href="https://devlog.hexops.com/2021/unicode-data-file-compression/">https://devlog.hexops.com/2021/unicode-data-file-compression/</a>
achieving 40-70% reduction over gzip alone

<a target="_blank" rel="noopener noreferrer" href="https://github.com/jecolon/ziglyph/issues/3">https://github.com/jecolon/ziglyph/issues/3</a>
More size-optimal grapheme cluster sorting


08/02/2023
<a target="_blank" rel="noopener noreferrer" href="https://github.com/natecraddock/zf">https://github.com/natecraddock/zf</a>
a commandline fuzzy finder that prioritizes matches on filenames
To review: uses zygliph
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jecolon/ziglyph/issues/20">https://github.com/jecolon/ziglyph/issues/20</a>
Grapheme segmentation with ZWJ sequences


10/02/2023
<a target="_blank" rel="noopener noreferrer" href="https://github.com/jecolon/ziglyph/issues/20">https://github.com/jecolon/ziglyph/issues/20</a>
Grapheme segmentation with ZWJ sequences
---
jlf: Executor is ok with utf8proc
t = "üêª‚Äç‚ùÑÔ∏èüêª‚Äç‚ùÑÔ∏è"~text
t~description=              -- 'UTF-8 not-ASCII (2 graphemes, 8 codepoints, 26 bytes, 0 error)'
t~characters==
    an Array (shape [8], 8 items)
     1 : ( "üêª"  U+1F43B So 2 "BEAR FACE" )
     2 : ( "‚Äç"    U+200D Cf 0 "ZERO WIDTH JOINER", "ZWJ" )
     3 : ( "‚ùÑ"   U+2744 So 1 "SNOWFLAKE" )
     4 : ( "Ô∏è"    U+FE0F Mn 0 "VARIATION SELECTOR-16", "VS16" )
     5 : ( "üêª"  U+1F43B So 2 "BEAR FACE" )
     6 : ( "‚Äç"    U+200D Cf 0 "ZERO WIDTH JOINER", "ZWJ" )
     7 : ( "‚ùÑ"   U+2744 So 1 "SNOWFLAKE" )
     8 : ( "Ô∏è"    U+FE0F Mn 0 "VARIATION SELECTOR-16", "VS16" )


<a target="_blank" rel="noopener noreferrer" href="https://devlog.hexops.com/2021/unicode-sorting-why-browsers-added-special-emoji-matching/">https://devlog.hexops.com/2021/unicode-sorting-why-browsers-added-special-emoji-matching/</a>
    Whether your application is in Go and has it‚Äôs own Unicode Collation Algorithm
    (UCA) implementation, or Rust and uses bindings to the popular ICU4C library -
    one thing is going to remain true: it requires large data files to work.

    The UCA algorithm depends on two quite large data table files to work:
    - UnicodeData.txt for normalization, a step required before sorting can take place.
    - allkeys.txt for weighting certain text above others.
    - And more, if you want truly locale-aware sorting and not just ‚Äúthe default‚Äù
      the UCA algorithm gives you.
    Together, these files can add up to over a half a megabyte.

    While WASM languages could shell out to JavaScript browser APIs for collation,
    I suspect they won‚Äôt due to the lack of guarantees around those APIs.

    A more likely scenario is languages continuing to leave locale-aware sorting
    as an optional, opt-in feature - that also makes your application larger.

    I think this a worthwhile problem to solve, so I am working on compression a
    lgorithms for these files specifically in Zig to reduce them to only a few
    tens of kilobytes.
    <a target="_blank" rel="noopener noreferrer" href="https://github.com/jecolon/ziglyph/issues/3">https://github.com/jecolon/ziglyph/issues/3</a>


<hr><h2 id="Knock__knock_">Knock, knock.</h2><hr>
Knock, knock.
    Who‚Äôs there?
You.
    You who?
Yoo-hoo! It's You Nicode.


Knock, knock.
    Who‚Äôs there?
Sue.
    Sue who?
It's Sue Nicode.
</pre>
</body>
</html>
